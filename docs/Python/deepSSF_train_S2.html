<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Scott Forrest">
<meta name="dcterms.date" content="2025-02-13">

<title>deepSSF Training - S2 â€“ deepSSF</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../Python/deepSSF_simulations.html" rel="next">
<link href="../Python/deepSSF_train.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-fd053a38988e21bb3d3b0b617a2084cf.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">deepSSF</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../study_system.html"> 
<span class="menu-text">Study System</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../step_selection_intuition.html"> 
<span class="menu-text">Step Selection Intuition</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../deepSSF_model_overview.html"> 
<span class="menu-text">deepSSF Model Overview</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/swforrest/deepSSF"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../Python/deepSSF_train.html">Model Training</a></li><li class="breadcrumb-item"><a href="../Python/deepSSF_train_S2.html">deepSSF Training - S2</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Data prep</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Model Training</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Python/deepSSF_train.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">deepSSF Training</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Python/deepSSF_train_S2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">deepSSF Training - S2</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Generating simulations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Python/deepSSF_simulations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">deepSSF Simulations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Python/deepSSF_simulations_S2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">deepSSF Simulations - S2</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Landscape predictions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Python/deepSSF_landscape_preds.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">deepSSF Landscape Predictions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Python/deepSSF_landscape_preds_S2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">deepSSF Landscape Predictions - S2</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">SSF Model Fitting</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Next-Step Validation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Python/deepSSF_next_step_validation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">deepSSF Validation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Python/deepSSF_next_step_validation_S2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">deepSSF Validation - S2</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#import-packages" id="toc-import-packages" class="nav-link active" data-scroll-target="#import-packages">Import packages</a>
  <ul class="collapse">
  <li><a href="#if-using-google-colab-uncomment-the-following-lines" id="toc-if-using-google-colab-uncomment-the-following-lines" class="nav-link" data-scroll-target="#if-using-google-colab-uncomment-the-following-lines">If using Google Colab, uncomment the following lines</a></li>
  </ul></li>
  <li><a href="#import-data" id="toc-import-data" class="nav-link" data-scroll-target="#import-data">Import data</a>
  <ul class="collapse">
  <li><a href="#set-paths-to-data" id="toc-set-paths-to-data" class="nav-link" data-scroll-target="#set-paths-to-data">Set paths to data</a></li>
  <li><a href="#read-buffalo-data" id="toc-read-buffalo-data" class="nav-link" data-scroll-target="#read-buffalo-data">Read buffalo data</a></li>
  <li><a href="#importing-spatial-data" id="toc-importing-spatial-data" class="nav-link" data-scroll-target="#importing-spatial-data">Importing spatial data</a></li>
  <li><a href="#slope" id="toc-slope" class="nav-link" data-scroll-target="#slope">Slope</a></li>
  <li><a href="#sentinel-2-bands" id="toc-sentinel-2-bands" class="nav-link" data-scroll-target="#sentinel-2-bands">Sentinel-2 bands</a>
  <ul class="collapse">
  <li><a href="#band-1" id="toc-band-1" class="nav-link" data-scroll-target="#band-1">Band 1</a></li>
  </ul></li>
  <li><a href="#band-2" id="toc-band-2" class="nav-link" data-scroll-target="#band-2">Band 2</a></li>
  <li><a href="#band-3" id="toc-band-3" class="nav-link" data-scroll-target="#band-3">Band 3</a></li>
  <li><a href="#band-4" id="toc-band-4" class="nav-link" data-scroll-target="#band-4">Band 4</a></li>
  <li><a href="#band-5" id="toc-band-5" class="nav-link" data-scroll-target="#band-5">Band 5</a></li>
  <li><a href="#band-6" id="toc-band-6" class="nav-link" data-scroll-target="#band-6">Band 6</a></li>
  <li><a href="#band-7" id="toc-band-7" class="nav-link" data-scroll-target="#band-7">Band 7</a></li>
  <li><a href="#band-8" id="toc-band-8" class="nav-link" data-scroll-target="#band-8">Band 8</a></li>
  <li><a href="#band-8a" id="toc-band-8a" class="nav-link" data-scroll-target="#band-8a">Band 8a</a></li>
  <li><a href="#band-9" id="toc-band-9" class="nav-link" data-scroll-target="#band-9">Band 9</a></li>
  <li><a href="#band-11" id="toc-band-11" class="nav-link" data-scroll-target="#band-11">Band 11</a></li>
  <li><a href="#band-12" id="toc-band-12" class="nav-link" data-scroll-target="#band-12">Band 12</a></li>
  <li><a href="#view-as-rgb" id="toc-view-as-rgb" class="nav-link" data-scroll-target="#view-as-rgb">View as RGB</a></li>
  <li><a href="#presence-records---target-of-model" id="toc-presence-records---target-of-model" class="nav-link" data-scroll-target="#presence-records---target-of-model">Presence records - target of model</a>
  <ul class="collapse">
  <li><a href="#combine-the-spatial-layers-into-channels" id="toc-combine-the-spatial-layers-into-channels" class="nav-link" data-scroll-target="#combine-the-spatial-layers-into-channels">Combine the spatial layers into channels</a></li>
  </ul></li>
  <li><a href="#defining-data-sets-and-data-loaders" id="toc-defining-data-sets-and-data-loaders" class="nav-link" data-scroll-target="#defining-data-sets-and-data-loaders">Defining data sets and data loaders</a>
  <ul class="collapse">
  <li><a href="#creating-a-dataset-class" id="toc-creating-a-dataset-class" class="nav-link" data-scroll-target="#creating-a-dataset-class">Creating a dataset class</a></li>
  <li><a href="#split-into-training-validation-and-test-sets" id="toc-split-into-training-validation-and-test-sets" class="nav-link" data-scroll-target="#split-into-training-validation-and-test-sets">Split into training, validation and test sets</a></li>
  <li><a href="#create-dataloaders" id="toc-create-dataloaders" class="nav-link" data-scroll-target="#create-dataloaders">Create dataloaders</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#load-the-model" id="toc-load-the-model" class="nav-link" data-scroll-target="#load-the-model">Load the model</a>
  <ul class="collapse">
  <li><a href="#define-the-parameters-for-the-model" id="toc-define-the-parameters-for-the-model" class="nav-link" data-scroll-target="#define-the-parameters-for-the-model">Define the parameters for the model</a></li>
  <li><a href="#instantiate-the-model" id="toc-instantiate-the-model" class="nav-link" data-scroll-target="#instantiate-the-model">Instantiate the model</a></li>
  <li><a href="#set-model-hyperparameters" id="toc-set-model-hyperparameters" class="nav-link" data-scroll-target="#set-model-hyperparameters">Set model hyperparameters</a></li>
  <li><a href="#training-loop" id="toc-training-loop" class="nav-link" data-scroll-target="#training-loop">Training loop</a></li>
  <li><a href="#test-loop" id="toc-test-loop" class="nav-link" data-scroll-target="#test-loop">Test loop</a></li>
  <li><a href="#train-the-model" id="toc-train-the-model" class="nav-link" data-scroll-target="#train-the-model">Train the model</a>
  <ul class="collapse">
  <li><a href="#create-an-output-directory-to-save-the-outputs-and-plots." id="toc-create-an-output-directory-to-save-the-outputs-and-plots." class="nav-link" data-scroll-target="#create-an-output-directory-to-save-the-outputs-and-plots.">Create an output directory to save the outputs and plots.</a></li>
  <li><a href="#save-the-validation-loss-as-a-dataframe" id="toc-save-the-validation-loss-as-a-dataframe" class="nav-link" data-scroll-target="#save-the-validation-loss-as-a-dataframe">Save the validation loss as a dataframe</a></li>
  <li><a href="#plot-the-validation-loss" id="toc-plot-the-validation-loss" class="nav-link" data-scroll-target="#plot-the-validation-loss">Plot the validation loss</a></li>
  <li><a href="#check-model-parameters" id="toc-check-model-parameters" class="nav-link" data-scroll-target="#check-model-parameters">Check model parameters</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#loading-in-previous-models" id="toc-loading-in-previous-models" class="nav-link" data-scroll-target="#loading-in-previous-models">Loading in previous models</a>
  <ul class="collapse">
  <li><a href="#if-loading-a-previously-trained-model" id="toc-if-loading-a-previously-trained-model" class="nav-link" data-scroll-target="#if-loading-a-previously-trained-model">If loading a previously trained model</a></li>
  </ul></li>
  <li><a href="#test-model" id="toc-test-model" class="nav-link" data-scroll-target="#test-model">Test model</a>
  <ul class="collapse">
  <li><a href="#helper-functions" id="toc-helper-functions" class="nav-link" data-scroll-target="#helper-functions">Helper functions</a></li>
  </ul></li>
  <li><a href="#extracting-convolution-layer-outputs" id="toc-extracting-convolution-layer-outputs" class="nav-link" data-scroll-target="#extracting-convolution-layer-outputs">Extracting convolution layer outputs</a>
  <ul class="collapse">
  <li><a href="#create-scalar-grids-for-plotting" id="toc-create-scalar-grids-for-plotting" class="nav-link" data-scroll-target="#create-scalar-grids-for-plotting">Create scalar grids for plotting</a></li>
  <li><a href="#convolutional-layer-1" id="toc-convolutional-layer-1" class="nav-link" data-scroll-target="#convolutional-layer-1">Convolutional layer 1</a>
  <ul class="collapse">
  <li><a href="#activation-hook" id="toc-activation-hook" class="nav-link" data-scroll-target="#activation-hook">Activation hook</a></li>
  <li><a href="#stack-spatial-and-scalar-as-grid-covariates" id="toc-stack-spatial-and-scalar-as-grid-covariates" class="nav-link" data-scroll-target="#stack-spatial-and-scalar-as-grid-covariates">Stack spatial and scalar (as grid) covariates</a></li>
  <li><a href="#extract-filters-and-plot" id="toc-extract-filters-and-plot" class="nav-link" data-scroll-target="#extract-filters-and-plot">Extract filters and plot</a></li>
  </ul></li>
  <li><a href="#convolutional-layer-2" id="toc-convolutional-layer-2" class="nav-link" data-scroll-target="#convolutional-layer-2">Convolutional layer 2</a>
  <ul class="collapse">
  <li><a href="#activation-hook-1" id="toc-activation-hook-1" class="nav-link" data-scroll-target="#activation-hook-1">Activation hook</a></li>
  <li><a href="#extract-filters-and-plot-1" id="toc-extract-filters-and-plot-1" class="nav-link" data-scroll-target="#extract-filters-and-plot-1">Extract filters and plot</a></li>
  </ul></li>
  <li><a href="#convolutional-layer-3" id="toc-convolutional-layer-3" class="nav-link" data-scroll-target="#convolutional-layer-3">Convolutional layer 3</a>
  <ul class="collapse">
  <li><a href="#activation-hook-2" id="toc-activation-hook-2" class="nav-link" data-scroll-target="#activation-hook-2">Activation hook</a></li>
  <li><a href="#extract-filters-and-plot-2" id="toc-extract-filters-and-plot-2" class="nav-link" data-scroll-target="#extract-filters-and-plot-2">Extract filters and plot</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#checking-estimated-movement-parameters" id="toc-checking-estimated-movement-parameters" class="nav-link" data-scroll-target="#checking-estimated-movement-parameters">Checking estimated movement parameters</a>
  <ul class="collapse">
  <li><a href="#plot-the-movement-distributions" id="toc-plot-the-movement-distributions" class="nav-link" data-scroll-target="#plot-the-movement-distributions">Plot the movement distributions</a></li>
  <li><a href="#generate-a-distribution-of-movement-parameters" id="toc-generate-a-distribution-of-movement-parameters" class="nav-link" data-scroll-target="#generate-a-distribution-of-movement-parameters">Generate a distribution of movement parameters</a>
  <ul class="collapse">
  <li><a href="#plot-the-distribution-of-movement-parameters" id="toc-plot-the-distribution-of-movement-parameters" class="nav-link" data-scroll-target="#plot-the-distribution-of-movement-parameters">Plot the distribution of movement parameters</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../Python/deepSSF_train.html">Model Training</a></li><li class="breadcrumb-item"><a href="../Python/deepSSF_train_S2.html">deepSSF Training - S2</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">deepSSF Training - S2</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Scott Forrest <a href="mailto:scottwforrest@gmail.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 13, 2025</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>In this script, we will train a deepSSF model on Sentinel-2 data directly (instead of derived covariates such as NDVI). The training data was generated using the <code>deepSSF_data_prep_S2_id.qmd</code> script, which crops out local images for each step of the observed telemetry data.</p>
    <p>There arenâ€™t as many comments or information to help understand the model, so please refer to the <code>deepSSF_train</code> script for more detail.</p>
  </div>
</div>


</header>


<section id="import-packages" class="level2">
<h2 class="anchored" data-anchor-id="import-packages">Import packages</h2>
<div id="cell-2" class="cell" data-executioninfo="{&quot;elapsed&quot;:9129,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1738212452755,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="d26486c3-17f1-4d1f-8ace-09bf6da9eebf">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># If using Google Colab, uncomment the following line</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install rasterio</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sys.version)  <span class="co"># Print Python version in use</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np                                      <span class="co"># Array operations</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt                         <span class="co"># Plotting library</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch                                            <span class="co"># Main PyTorch library</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim                             <span class="co"># Optimization algorithms</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn                                    <span class="co"># Neural network modules</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader        <span class="co"># Dataset and batch data loading</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> datetime                           <span class="co"># Date/time utilities</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os                                               <span class="co"># Operating system utilities</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd                                     <span class="co"># Data manipulation</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> rasterio                                         <span class="co"># Geospatial raster data</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> deepSSF_model                                    <span class="co"># Import the .py file containing the deepSSF model     </span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> deepSSF_loss                                     <span class="co"># Import the .py file containing the deepSSF loss function</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> deepSSF_early_stopping                           <span class="co"># Import the .py file containing the early stopping function                                     </span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Get today's date</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>today_date <span class="op">=</span> datetime.today().strftime(<span class="st">'%Y-%m-</span><span class="sc">%d</span><span class="st">'</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">42</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:18:29) [MSC v.1929 64 bit (AMD64)]</code></pre>
</div>
</div>
<section id="if-using-google-colab-uncomment-the-following-lines" class="level3">
<h3 class="anchored" data-anchor-id="if-using-google-colab-uncomment-the-following-lines">If using Google Colab, uncomment the following lines</h3>
<p>The file directories will also need to be changed to match the location of the files in your Google Drive.</p>
<div id="cell-4" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># from google.colab import drive</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># drive.mount('/content/drive')</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="import-data" class="level1">
<h1>Import data</h1>
<section id="set-paths-to-data" class="level2">
<h2 class="anchored" data-anchor-id="set-paths-to-data">Set paths to data</h2>
<div id="cell-7" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>buffalo_id <span class="op">=</span> <span class="dv">2005</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">10297</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Specify the path to CSV file</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>csv_file_path <span class="op">=</span> <span class="ss">f'../buffalo_local_data_id/buffalo_</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_data_df_lag_1hr_n</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">.csv'</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Path to your TIF file (slope)</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>slope_path <span class="op">=</span> <span class="ss">f'../buffalo_local_layers_id/buffalo_</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_slope_cent101x101_lag_1hr_n</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">.tif'</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths to the Sentinel-2 bands</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>b1_path <span class="op">=</span> <span class="ss">f'../buffalo_local_layers_id/buffalo_</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_s2_b1_cent101x101_lag_1hr_n</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">.tif'</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>b2_path <span class="op">=</span> <span class="ss">f'../buffalo_local_layers_id/buffalo_</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_s2_b2_cent101x101_lag_1hr_n</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">.tif'</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>b3_path <span class="op">=</span> <span class="ss">f'../buffalo_local_layers_id/buffalo_</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_s2_b3_cent101x101_lag_1hr_n</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">.tif'</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>b4_path <span class="op">=</span> <span class="ss">f'../buffalo_local_layers_id/buffalo_</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_s2_b4_cent101x101_lag_1hr_n</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">.tif'</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>b5_path <span class="op">=</span> <span class="ss">f'../buffalo_local_layers_id/buffalo_</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_s2_b5_cent101x101_lag_1hr_n</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">.tif'</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>b6_path <span class="op">=</span> <span class="ss">f'../buffalo_local_layers_id/buffalo_</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_s2_b6_cent101x101_lag_1hr_n</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">.tif'</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>b7_path <span class="op">=</span> <span class="ss">f'../buffalo_local_layers_id/buffalo_</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_s2_b7_cent101x101_lag_1hr_n</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">.tif'</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>b8_path <span class="op">=</span> <span class="ss">f'../buffalo_local_layers_id/buffalo_</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_s2_b8_cent101x101_lag_1hr_n</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">.tif'</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>b8a_path <span class="op">=</span> <span class="ss">f'../buffalo_local_layers_id/buffalo_</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_s2_b8a_cent101x101_lag_1hr_n</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">.tif'</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>b9_path <span class="op">=</span> <span class="ss">f'../buffalo_local_layers_id/buffalo_</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_s2_b9_cent101x101_lag_1hr_n</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">.tif'</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>b11_path <span class="op">=</span> <span class="ss">f'../buffalo_local_layers_id/buffalo_</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_s2_b11_cent101x101_lag_1hr_n</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">.tif'</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>b12_path <span class="op">=</span> <span class="ss">f'../buffalo_local_layers_id/buffalo_</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_s2_b12_cent101x101_lag_1hr_n</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">.tif'</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Path to your TIF file (target variable)</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>pres_path <span class="op">=</span> <span class="ss">f'../buffalo_local_layers_id/buffalo_</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_pres_cent101x101_lag_1hr_n</span><span class="sc">{</span>n_samples<span class="sc">}</span><span class="ss">.tif'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="read-buffalo-data" class="level2">
<h2 class="anchored" data-anchor-id="read-buffalo-data">Read buffalo data</h2>
<div id="cell-9" class="cell" data-executioninfo="{&quot;elapsed&quot;:3573,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731284942464,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="ba5c7819-16b2-4457-929d-717c74d9e759" data-execution_count="4">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Read the CSV file into a DataFrame</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>buffalo_df <span class="op">=</span> pd.read_csv(csv_file_path)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(buffalo_df.shape)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Lag the values in column 'A' by one index to get the bearing of the previous step</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>buffalo_df[<span class="st">'bearing_tm1'</span>] <span class="op">=</span> buffalo_df[<span class="st">'bearing'</span>].shift(<span class="dv">1</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Pad the missing value with a specified value, e.g., 0</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>buffalo_df[<span class="st">'bearing_tm1'</span>] <span class="op">=</span> buffalo_df[<span class="st">'bearing_tm1'</span>].fillna(<span class="dv">0</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first few rows of the DataFrame</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(buffalo_df.head())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(10103, 35)
             x_            y_                    t_    id           x1_  \
0  41969.310875 -1.435671e+06  2018-07-25T01:04:23Z  2005  41969.310875   
1  41921.521939 -1.435654e+06  2018-07-25T02:04:39Z  2005  41921.521939   
2  41779.439594 -1.435601e+06  2018-07-25T03:04:17Z  2005  41779.439594   
3  41841.203272 -1.435635e+06  2018-07-25T04:04:39Z  2005  41841.203272   
4  41655.463332 -1.435604e+06  2018-07-25T05:04:27Z  2005  41655.463332   

            y1_           x2_           y2_     x2_cent    y2_cent  ...  \
0 -1.435671e+06  41921.521939 -1.435654e+06  -47.788936  16.857110  ...   
1 -1.435654e+06  41779.439594 -1.435601e+06 -142.082345  53.568427  ...   
2 -1.435601e+06  41841.203272 -1.435635e+06   61.763677 -34.322938  ...   
3 -1.435635e+06  41655.463332 -1.435604e+06 -185.739939  31.003534  ...   
4 -1.435604e+06  41618.651923 -1.435608e+06  -36.811409  -4.438037  ...   

     cos_ta         x_min         x_max         y_min         y_max  s2_index  \
0  0.201466  40706.810875  43231.810875 -1.436934e+06 -1.434409e+06         7   
1  0.999770  40659.021939  43184.021939 -1.436917e+06 -1.434392e+06         7   
2 -0.989262  40516.939594  43041.939594 -1.436863e+06 -1.434338e+06         7   
3 -0.942144  40578.703272  43103.703272 -1.436898e+06 -1.434373e+06         7   
4  0.959556  40392.963332  42917.963332 -1.436867e+06 -1.434342e+06         7   

   points_vect_cent  year_t2  yday_t2_2018_base  bearing_tm1  
0               NaN     2018                206     0.000000  
1               NaN     2018                206     2.802478  
2               NaN     2018                206     2.781049  
3               NaN     2018                206    -0.507220  
4               NaN     2018                206     2.976198  

[5 rows x 36 columns]</code></pre>
</div>
</div>
</section>
<section id="importing-spatial-data" class="level2">
<h2 class="anchored" data-anchor-id="importing-spatial-data">Importing spatial data</h2>
</section>
<section id="slope" class="level2">
<h2 class="anchored" data-anchor-id="slope">Slope</h2>
<div id="cell-12" class="cell" data-executioninfo="{&quot;elapsed&quot;:9312,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731284951772,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="5dc08301-9d9f-49d0-b47a-b575021f524d" data-execution_count="5">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using rasterio</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rasterio.<span class="bu">open</span>(slope_path) <span class="im">as</span> slope:</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read all layers/channels into a single numpy array</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rasterio indexes channels starting from 1, hence the range is 1 to src.count + 1</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    slope_stack <span class="op">=</span> slope.read([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, slope.count <span class="op">+</span> <span class="dv">1</span>)])</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(slope_stack.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(10103, 101, 101)</code></pre>
</div>
</div>
<div id="cell-13" class="cell" data-executioninfo="{&quot;elapsed&quot;:7,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731284951772,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="ebbf6788-39be-4483-cc0e-f1156de2d7bb" data-execution_count="6">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the numpy array to a PyTorch tensor, which is the format required for training the model</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>slope_tens <span class="op">=</span> torch.from_numpy(slope_stack)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(slope_tens.shape)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the mean, max, and min values of the slope tensor</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean = "</span>, torch.mean(slope_tens))</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>slope_max <span class="op">=</span> torch.<span class="bu">max</span>(slope_tens)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>slope_min <span class="op">=</span> torch.<span class="bu">min</span>(slope_tens)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Max = "</span>, slope_max)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Min = "</span>, slope_min)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalizing the data</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>slope_tens <span class="op">=</span> (slope_tens <span class="op">-</span> slope_min) <span class="op">/</span> (slope_max <span class="op">-</span> slope_min)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean = "</span>, torch.mean(slope_tens))</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Max = "</span>, torch.<span class="bu">max</span>(slope_tens))</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Min = "</span>, torch.<span class="bu">min</span>(slope_tens))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([10103, 101, 101])
Mean =  tensor(0.7779)
Max =  tensor(12.2981)
Min =  tensor(0.0006)
Mean =  tensor(0.0632)
Max =  tensor(1.)
Min =  tensor(0.)</code></pre>
</div>
</div>
<div id="cell-14" class="cell" data-executioninfo="{&quot;elapsed&quot;:5,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731284951772,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="712d4cc8-daa5-44d5-c1c4-a696d482550f" data-execution_count="7">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">1</span>):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(slope_tens[i])</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sentinel-2-bands" class="level2">
<h2 class="anchored" data-anchor-id="sentinel-2-bands">Sentinel-2 bands</h2>
<p>During the data preparation (in the <code>deepSSF_data_prep_id_S2</code> script) for the Sentinel-2 bands, we scaled them by 10,000, so we do not need to scale them again here (as we did for the other covariates).</p>
<section id="band-1" class="level3">
<h3 class="anchored" data-anchor-id="band-1">Band 1</h3>
<div id="cell-16" class="cell" data-execution_count="8">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using rasterio</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rasterio.<span class="bu">open</span>(b1_path) <span class="im">as</span> b1:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read all layers/channels into a single numpy array</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rasterio indexes channels starting from 1, hence the range is 1 to src.count + 1</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    b1_stack <span class="op">=</span> b1.read([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, b1.count <span class="op">+</span> <span class="dv">1</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-17" class="cell" data-executioninfo="{&quot;elapsed&quot;:3,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731284959974,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="f4173c28-483e-4d82-e044-9c6aba1c8f70" data-execution_count="9">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of the original b1_stack array</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b1_stack.shape)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace NaNs with -1</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>b1_stack <span class="op">=</span> np.nan_to_num(b1_stack, nan<span class="op">=-</span><span class="fl">1.0</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the numpy array to a PyTorch tensor</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>b1_tens <span class="op">=</span> torch.from_numpy(b1_stack)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b1_tens.shape)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the mean, max, and min values of the b1 tensor</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Min =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">min</span>(b1_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Mean = </span><span class="sc">{</span>torch<span class="sc">.</span>mean(b1_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Max =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">max</span>(b1_tens)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(10103, 101, 101)
torch.Size([10103, 101, 101])
Min =  9.999999747378752e-05
Mean = 0.04444880783557892
Max =  0.1517084836959839</code></pre>
</div>
</div>
<div id="cell-18" class="cell" data-executioninfo="{&quot;elapsed&quot;:947,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731284960919,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="12128cb3-8bc6-422a-b011-008078280ca4" data-execution_count="10">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">1</span>):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(b1_tens[i].numpy())</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="band-2" class="level2">
<h2 class="anchored" data-anchor-id="band-2">Band 2</h2>
<div id="cell-20" class="cell" data-execution_count="11">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using rasterio</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rasterio.<span class="bu">open</span>(b2_path) <span class="im">as</span> b2:</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read all layers/channels into a single numpy array</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rasterio indexes channels starting from 1, hence the range is 1 to src.count + 1</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    b2_stack <span class="op">=</span> b2.read([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, b2.count <span class="op">+</span> <span class="dv">1</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-21" class="cell" data-executioninfo="{&quot;elapsed&quot;:3,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731284972091,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="b0e57602-09dc-4258-8d4b-3b091cb19dbe" data-execution_count="12">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of the original b2_stack array</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b2_stack.shape)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace NaNs with -1</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>b2_stack <span class="op">=</span> np.nan_to_num(b2_stack, nan<span class="op">=-</span><span class="fl">1.0</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the numpy array to a PyTorch tensor</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>b2_tens <span class="op">=</span> torch.from_numpy(b2_stack)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b2_tens.shape)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the mean, max, and min values of the b2 tensor</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Min =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">min</span>(b2_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Mean = </span><span class="sc">{</span>torch<span class="sc">.</span>mean(b2_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Max =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">max</span>(b2_tens)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(10103, 101, 101)
torch.Size([10103, 101, 101])
Min =  0.002810720121487975
Mean = 0.05629923567175865
Max =  0.1931755244731903</code></pre>
</div>
</div>
<div id="cell-22" class="cell" data-executioninfo="{&quot;elapsed&quot;:1003,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731284973092,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="774d2f8b-5be6-4f92-b68c-71f4563459bf" data-execution_count="13">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">1</span>):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(b2_tens[i].numpy())</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="band-3" class="level2">
<h2 class="anchored" data-anchor-id="band-3">Band 3</h2>
<div id="cell-24" class="cell" data-execution_count="14">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using rasterio</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rasterio.<span class="bu">open</span>(b3_path) <span class="im">as</span> b3:</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read all layers/channels into a single numpy array</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rasterio indexes channels starting from 1, hence the range is 1 to src.count + 1</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    b3_stack <span class="op">=</span> b3.read([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, b3.count <span class="op">+</span> <span class="dv">1</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-25" class="cell" data-executioninfo="{&quot;elapsed&quot;:3,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731284978263,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="530cbeb2-3193-49c3-c68b-e58869b241bf" data-execution_count="15">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of the original b3_stack array</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b3_stack.shape)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace NaNs with -1</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>b3_stack <span class="op">=</span> np.nan_to_num(b3_stack, nan<span class="op">=-</span><span class="fl">1.0</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the numpy array to a PyTorch tensor</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>b3_tens <span class="op">=</span> torch.from_numpy(b3_stack)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b3_tens.shape)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the mean, max, and min values of the b3 tensor</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Min =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">min</span>(b3_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Mean = </span><span class="sc">{</span>torch<span class="sc">.</span>mean(b3_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Max =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">max</span>(b3_tens)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(10103, 101, 101)
torch.Size([10103, 101, 101])
Min =  0.02109863981604576
Mean = 0.08027872443199158
Max =  0.2795756757259369</code></pre>
</div>
</div>
<div id="cell-26" class="cell" data-executioninfo="{&quot;elapsed&quot;:1299,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731284979560,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="b9951abd-0ff0-42f9-9564-6148c9c4a3fa" data-execution_count="16">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">1</span>):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(b3_tens[i].numpy())</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="band-4" class="level2">
<h2 class="anchored" data-anchor-id="band-4">Band 4</h2>
<div id="cell-28" class="cell" data-execution_count="17">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using rasterio</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rasterio.<span class="bu">open</span>(b4_path) <span class="im">as</span> b4:</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read all layers/channels into a single numpy array</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rasterio indexes channels starting from 1, hence the range is 1 to src.count + 1</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    b4_stack <span class="op">=</span> b4.read([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, b4.count <span class="op">+</span> <span class="dv">1</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-29" class="cell" data-executioninfo="{&quot;elapsed&quot;:4,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731284986219,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="4703df25-eaea-481b-e6eb-9e86fcd165a0" data-execution_count="18">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of the original b4_stack array</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b4_stack.shape)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace NaNs with -1</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>b4_stack <span class="op">=</span> np.nan_to_num(b4_stack, nan<span class="op">=-</span><span class="fl">1.0</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the numpy array to a PyTorch tensor</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>b4_tens <span class="op">=</span> torch.from_numpy(b4_stack)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b4_tens.shape)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the mean, max, and min values of the b4 tensor</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Min =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">min</span>(b4_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Mean = </span><span class="sc">{</span>torch<span class="sc">.</span>mean(b4_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Max =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">max</span>(b4_tens)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(10103, 101, 101)
torch.Size([10103, 101, 101])
Min =  0.006578320171684027
Mean = 0.09937984496355057
Max =  0.43867969512939453</code></pre>
</div>
</div>
<div id="cell-30" class="cell" data-executioninfo="{&quot;elapsed&quot;:980,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731284987196,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="3e5599c0-9399-4d7b-a9cf-15e648d902bf" data-execution_count="19">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">1</span>):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(b4_tens[i].numpy())</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="band-5" class="level2">
<h2 class="anchored" data-anchor-id="band-5">Band 5</h2>
<div id="cell-32" class="cell" data-execution_count="20">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using rasterio</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rasterio.<span class="bu">open</span>(b5_path) <span class="im">as</span> b5:</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read all layers/channels into a single numpy array</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rasterio indexes channels starting from 1, hence the range is 1 to src.count + 1</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    b5_stack <span class="op">=</span> b5.read([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, b5.count <span class="op">+</span> <span class="dv">1</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-33" class="cell" data-executioninfo="{&quot;elapsed&quot;:3,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731284993811,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="5396a62d-018c-41cb-9a9c-a3941690ac4a" data-execution_count="21">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of the original b5_stack array</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b5_stack.shape)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace NaNs with -1</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>b5_stack <span class="op">=</span> np.nan_to_num(b5_stack, nan<span class="op">=-</span><span class="fl">1.0</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the numpy array to a PyTorch tensor</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>b5_tens <span class="op">=</span> torch.from_numpy(b5_stack)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b5_tens.shape)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the mean, max, and min values of the b5 tensor</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Min =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">min</span>(b5_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Mean = </span><span class="sc">{</span>torch<span class="sc">.</span>mean(b5_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Max =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">max</span>(b5_tens)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(10103, 101, 101)
torch.Size([10103, 101, 101])
Min =  0.03587600216269493
Mean = 0.136901393532753
Max =  0.4592735767364502</code></pre>
</div>
</div>
<div id="cell-34" class="cell" data-executioninfo="{&quot;elapsed&quot;:783,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731284994592,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="975fea3a-ee48-4620-eebf-80e85b5a0021" data-execution_count="22">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">1</span>):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(b5_tens[i].numpy())</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="band-6" class="level2">
<h2 class="anchored" data-anchor-id="band-6">Band 6</h2>
<div id="cell-36" class="cell" data-execution_count="23">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using rasterio</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rasterio.<span class="bu">open</span>(b6_path) <span class="im">as</span> b6:</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read all layers/channels into a single numpy array</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rasterio indexes channels starting from 1, hence the range is 1 to src.count + 1</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    b6_stack <span class="op">=</span> b6.read([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, b6.count <span class="op">+</span> <span class="dv">1</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-37" class="cell" data-executioninfo="{&quot;elapsed&quot;:3,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285001144,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="9185b4c0-4e39-4da5-f64e-71ec02798014" data-execution_count="24">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of the original b6_stack array</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b6_stack.shape)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace NaNs with -1</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>b6_stack <span class="op">=</span> np.nan_to_num(b6_stack, nan<span class="op">=-</span><span class="fl">1.0</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the numpy array to a PyTorch tensor</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>b6_tens <span class="op">=</span> torch.from_numpy(b6_stack)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b6_tens.shape)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the mean, max, and min values of the b6 tensor</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Min =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">min</span>(b6_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Mean = </span><span class="sc">{</span>torch<span class="sc">.</span>mean(b6_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Max =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">max</span>(b6_tens)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(10103, 101, 101)
torch.Size([10103, 101, 101])
Min =  0.038534000515937805
Mean = 0.17727355659008026
Max =  0.5120914578437805</code></pre>
</div>
</div>
<div id="cell-38" class="cell" data-executioninfo="{&quot;elapsed&quot;:1075,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285002217,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="c33e2dcd-cb4f-4c9a-afa4-f37aa53f17ff" data-execution_count="25">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">1</span>):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(b6_tens[i].numpy())</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="band-7" class="level2">
<h2 class="anchored" data-anchor-id="band-7">Band 7</h2>
<div id="cell-40" class="cell" data-execution_count="26">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using rasterio</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rasterio.<span class="bu">open</span>(b7_path) <span class="im">as</span> b7:</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read all layers/channels into a single numpy array</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rasterio indexes channels starting from 1, hence the range is 1 to src.count + 1</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    b7_stack <span class="op">=</span> b7.read([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, b7.count <span class="op">+</span> <span class="dv">1</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-41" class="cell" data-executioninfo="{&quot;elapsed&quot;:3,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285008434,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="8a568fca-40f8-41fb-a3ca-f35db0e6f34a" data-execution_count="27">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of the original b7_stack array</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b7_stack.shape)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace NaNs with -1</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>b7_stack <span class="op">=</span> np.nan_to_num(b7_stack, nan<span class="op">=-</span><span class="fl">1.0</span>)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the numpy array to a PyTorch tensor</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>b7_tens <span class="op">=</span> torch.from_numpy(b7_stack)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b7_tens.shape)</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the mean, max, and min values of the b7 tensor</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Min =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">min</span>(b7_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Mean = </span><span class="sc">{</span>torch<span class="sc">.</span>mean(b7_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Max =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">max</span>(b7_tens)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(10103, 101, 101)
torch.Size([10103, 101, 101])
Min =  0.04165744036436081
Mean = 0.19983430206775665
Max =  0.6045699119567871</code></pre>
</div>
</div>
<div id="cell-42" class="cell" data-executioninfo="{&quot;elapsed&quot;:1999,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285010431,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="6622c38e-2b47-4528-aed3-61f10afd671f" data-execution_count="28">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">1</span>):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(b7_tens[i].numpy())</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-29-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="band-8" class="level2">
<h2 class="anchored" data-anchor-id="band-8">Band 8</h2>
<div id="cell-44" class="cell" data-execution_count="29">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using rasterio</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rasterio.<span class="bu">open</span>(b8_path) <span class="im">as</span> b8:</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read all layers/channels into a single numpy array</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rasterio indexes channels starting from 1, hence the range is 1 to src.count + 1</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    b8_stack <span class="op">=</span> b8.read([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, b8.count <span class="op">+</span> <span class="dv">1</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-45" class="cell" data-executioninfo="{&quot;elapsed&quot;:3,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285018703,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="7e056498-32db-4fd8-81e4-0d6aede97536" data-execution_count="30">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of the original b8_stack array</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b8_stack.shape)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace NaNs with -1</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>b8_stack <span class="op">=</span> np.nan_to_num(b8_stack, nan<span class="op">=-</span><span class="fl">1.0</span>)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the numpy array to a PyTorch tensor</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>b8_tens <span class="op">=</span> torch.from_numpy(b8_stack)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b8_tens.shape)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the mean, max, and min values of the b8 tensor</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Min =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">min</span>(b8_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Mean = </span><span class="sc">{</span>torch<span class="sc">.</span>mean(b8_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Max =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">max</span>(b8_tens)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(10103, 101, 101)
torch.Size([10103, 101, 101])
Min =  0.03680320084095001
Mean = 0.2095790058374405
Max =  0.6004582643508911</code></pre>
</div>
</div>
<div id="cell-46" class="cell" data-executioninfo="{&quot;elapsed&quot;:1860,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285020561,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="88719bee-93e2-446c-c1f9-52915586e254" data-execution_count="31">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">1</span>):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(b8_tens[i].numpy())</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-32-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="band-8a" class="level2">
<h2 class="anchored" data-anchor-id="band-8a">Band 8a</h2>
<div id="cell-48" class="cell" data-execution_count="32">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using rasterio</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rasterio.<span class="bu">open</span>(b8a_path) <span class="im">as</span> b8a:</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read all layers/channels into a single numpy array</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rasterio indexes channels starting from 1, hence the range is 1 to src.count + 1</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    b8a_stack <span class="op">=</span> b8a.read([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, b8a.count <span class="op">+</span> <span class="dv">1</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-49" class="cell" data-executioninfo="{&quot;elapsed&quot;:3,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285027717,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="8caf77dc-2dfe-44ad-f8d5-012e50e931fb" data-execution_count="33">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of the original b8a_stack array</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b8a_stack.shape)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace NaNs with -1</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>b8a_stack <span class="op">=</span> np.nan_to_num(b8a_stack, nan<span class="op">=-</span><span class="fl">1.0</span>)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the numpy array to a PyTorch tensor</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>b8a_tens <span class="op">=</span> torch.from_numpy(b8a_stack)</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b8a_tens.shape)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the mean, max, and min values of the b8a tensor</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Min =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">min</span>(b8a_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Mean = </span><span class="sc">{</span>torch<span class="sc">.</span>mean(b8a_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Max =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">max</span>(b8a_tens)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(10103, 101, 101)
torch.Size([10103, 101, 101])
Min =  0.03570704162120819
Mean = 0.22782425582408905
Max =  0.6218413710594177</code></pre>
</div>
</div>
<div id="cell-50" class="cell" data-executioninfo="{&quot;elapsed&quot;:780,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285028495,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="a548eaed-b14c-4e5d-e739-5172fc971d20" data-execution_count="34">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">1</span>):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(b8a_tens[i].numpy())</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-35-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="band-9" class="level2">
<h2 class="anchored" data-anchor-id="band-9">Band 9</h2>
<div id="cell-52" class="cell" data-execution_count="35">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using rasterio</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rasterio.<span class="bu">open</span>(b9_path) <span class="im">as</span> b9:</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read all layers/channels into a single numpy array</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rasterio indexes channels starting from 1, hence the range is 1 to src.count + 1</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    b9_stack <span class="op">=</span> b9.read([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, b9.count <span class="op">+</span> <span class="dv">1</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-53" class="cell" data-executioninfo="{&quot;elapsed&quot;:3,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285035794,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="c63f69bd-79c5-4fe5-a5a3-4b2870d6e420" data-execution_count="36">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of the original b9_stack array</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b9_stack.shape)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace NaNs with -1</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>b9_stack <span class="op">=</span> np.nan_to_num(b9_stack, nan<span class="op">=-</span><span class="fl">1.0</span>)</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the numpy array to a PyTorch tensor</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>b9_tens <span class="op">=</span> torch.from_numpy(b9_stack)</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b9_tens.shape)</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the mean, max, and min values of the b9 tensor</span></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Min =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">min</span>(b9_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Mean = </span><span class="sc">{</span>torch<span class="sc">.</span>mean(b9_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Max =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">max</span>(b9_tens)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(10103, 101, 101)
torch.Size([10103, 101, 101])
Min =  0.012299999594688416
Mean = 0.22701694071292877
Max =  0.5680500268936157</code></pre>
</div>
</div>
<div id="cell-54" class="cell" data-executioninfo="{&quot;elapsed&quot;:1945,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285037737,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="241972c2-95f7-4a16-bd8a-4f264c22a544" data-execution_count="37">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">1</span>):</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(b9_tens[i].numpy())</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-38-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="band-11" class="level2">
<h2 class="anchored" data-anchor-id="band-11">Band 11</h2>
<div id="cell-56" class="cell" data-execution_count="38">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using rasterio</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rasterio.<span class="bu">open</span>(b11_path) <span class="im">as</span> b11:</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read all layers/channels into a single numpy array</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rasterio indexes channels starting from 1, hence the range is 1 to src.count + 1</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    b11_stack <span class="op">=</span> b11.read([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, b11.count <span class="op">+</span> <span class="dv">1</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-57" class="cell" data-executioninfo="{&quot;elapsed&quot;:3,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285042561,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="39d4de44-547b-42d1-d597-25120bc290ef" data-execution_count="39">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of the original b11_stack array</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b11_stack.shape)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace NaNs with -1</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>b11_stack <span class="op">=</span> np.nan_to_num(b11_stack, nan<span class="op">=-</span><span class="fl">1.0</span>)</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the numpy array to a PyTorch tensor</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>b11_tens <span class="op">=</span> torch.from_numpy(b11_stack)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b11_tens.shape)</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the mean, max, and min values of the b11 tensor</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Min =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">min</span>(b11_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Mean = </span><span class="sc">{</span>torch<span class="sc">.</span>mean(b11_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Max =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">max</span>(b11_tens)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(10103, 101, 101)
torch.Size([10103, 101, 101])
Min =  0.01741199940443039
Mean = 0.27866700291633606
Max =  0.657039225101471</code></pre>
</div>
</div>
<div id="cell-58" class="cell" data-executioninfo="{&quot;elapsed&quot;:1955,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285044513,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="9af05149-cdb4-412e-f645-b4592b0886e9" data-execution_count="40">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">1</span>):</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(b11_tens[i].numpy())</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-41-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="band-12" class="level2">
<h2 class="anchored" data-anchor-id="band-12">Band 12</h2>
<div id="cell-60" class="cell" data-execution_count="41">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using rasterio</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rasterio.<span class="bu">open</span>(b12_path) <span class="im">as</span> b12:</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read all layers/channels into a single numpy array</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rasterio indexes channels starting from 1, hence the range is 1 to src.count + 1</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    b12_stack <span class="op">=</span> b12.read([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, b1.count <span class="op">+</span> <span class="dv">1</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-61" class="cell" data-executioninfo="{&quot;elapsed&quot;:4,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285048644,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="7d93f6f3-44f8-4910-83ff-131946a8a9d4" data-execution_count="42">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of the original b12_stack array</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b12_stack.shape)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace NaNs with -1</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>b12_stack <span class="op">=</span> np.nan_to_num(b12_stack, nan<span class="op">=-</span><span class="fl">1.0</span>)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the numpy array to a PyTorch tensor</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>b12_tens <span class="op">=</span> torch.from_numpy(b12_stack)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b12_tens.shape)</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the mean, max, and min values of the b12 tensor</span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Min =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">min</span>(b12_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Mean = </span><span class="sc">{</span>torch<span class="sc">.</span>mean(b12_tens)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Max =  </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">max</span>(b12_tens)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(10103, 101, 101)
torch.Size([10103, 101, 101])
Min =  0.012337599880993366
Mean = 0.19245100021362305
Max =  0.5119996666908264</code></pre>
</div>
</div>
<div id="cell-62" class="cell" data-executioninfo="{&quot;elapsed&quot;:1526,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285050167,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="28769a09-1b76-476a-b73b-a77ea76abb0b" data-execution_count="43">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">1</span>):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(b12_tens[i].numpy())</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-44-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="view-as-rgb" class="level2">
<h2 class="anchored" data-anchor-id="view-as-rgb">View as RGB</h2>
<p>Given the Red (B4), Green (B3), and Blue (B2) bands, we can create an RGB image.</p>
<div id="cell-64" class="cell" data-executioninfo="{&quot;elapsed&quot;:10,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285050168,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="62af61d3-ebfb-4deb-92be-fca4f6a17163" data-execution_count="44">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming b4_tens, b3_tens, and b2_tens are your tensors</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>rgb_image <span class="op">=</span> torch.stack([b4_tens, b3_tens, b2_tens], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to NumPy</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>rgb_image_np <span class="op">=</span> rgb_image[<span class="dv">0</span>].cpu().numpy()</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize to the range [0, 1] for display</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>rgb_image_np <span class="op">=</span> (rgb_image_np <span class="op">-</span> rgb_image_np.<span class="bu">min</span>()) <span class="op">/</span> (rgb_image_np.<span class="bu">max</span>() <span class="op">-</span> rgb_image_np.<span class="bu">min</span>())</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>plt.imshow(rgb_image_np)</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Sentinel-2 RGB Image'</span>)</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-45-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="presence-records---target-of-model" class="level2">
<h2 class="anchored" data-anchor-id="presence-records---target-of-model">Presence records - target of model</h2>
<p>The target is what we are trying to predict with the deepSSF model, with is the location of the observed next step.</p>
<div id="cell-66" class="cell" data-executioninfo="{&quot;elapsed&quot;:2939,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285053099,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="09179be1-b4e3-43eb-a9fb-aabb78f4b1f0" data-execution_count="45">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using rasterio</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> rasterio.<span class="bu">open</span>(pres_path) <span class="im">as</span> pres:</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read all layers/channels into a single numpy array</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># rasterio indexes channels starting from 1, hence the range is 1 to src.count + 1</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    pres_stack <span class="op">=</span> pres.read([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, pres.count <span class="op">+</span> <span class="dv">1</span>)])</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pres_stack.shape)</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(pres_stack))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(10103, 101, 101)
&lt;class 'numpy.ndarray'&gt;</code></pre>
</div>
</div>
<div id="cell-67" class="cell" data-executioninfo="{&quot;elapsed&quot;:7,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285053100,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="d1af4707-551f-4d58-abd7-51076215ff9d" data-execution_count="46">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">1</span>):</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(pres_stack[i])</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-47-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-68" class="cell" data-executioninfo="{&quot;elapsed&quot;:6,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285053100,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="27e3520a-6ee4-4353-8f71-829db3cbef74" data-execution_count="47">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train on the GPU or on the CPU, if a GPU is not available</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span>) <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> torch.device(<span class="st">'cpu'</span>)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss"> device"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Using cpu device</code></pre>
</div>
</div>
<section id="combine-the-spatial-layers-into-channels" class="level3">
<h3 class="anchored" data-anchor-id="combine-the-spatial-layers-into-channels">Combine the spatial layers into channels</h3>
<div id="cell-70" class="cell" data-executioninfo="{&quot;elapsed&quot;:504,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285053599,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="806e950e-4ffc-4f83-ddf3-0aa7ee748931" data-execution_count="48">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use sentinel-2 bands</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>combined_stack <span class="op">=</span> torch.stack([b1_tens, </span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>                              b2_tens, </span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>                              b3_tens, </span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>                              b4_tens,</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>                              b5_tens, </span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>                              b6_tens, </span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>                              b7_tens, </span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>                              b8_tens,</span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>                              b8a_tens, </span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>                              b9_tens, </span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a>                              b11_tens, </span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a>                              b12_tens,</span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a>                              slope_tens], </span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a>                              dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(combined_stack.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([10103, 13, 101, 101])</code></pre>
</div>
</div>
</section>
</section>
<section id="defining-data-sets-and-data-loaders" class="level2">
<h2 class="anchored" data-anchor-id="defining-data-sets-and-data-loaders">Defining data sets and data loaders</h2>
<section id="creating-a-dataset-class" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-dataset-class">Creating a dataset class</h3>
<p>This custom PyTorch Dataset organizes all your input (spatial data, scalar covariates, bearing, and target) in a single object, allowing you to neatly manage how samples are accessed. The <code>__init__</code> method prepares and stores all the data, <code>__len__</code> returns the total number of samples, and <code>__getitem__</code> retrieves a single sample by indexâ€”enabling straightforward batching and iteration when used with a DataLoader.</p>
<div id="cell-73" class="cell" data-execution_count="49">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> buffalo_data(Dataset):</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># data loading</span></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.spatial_data_x <span class="op">=</span> combined_stack</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the scalar data that will be converted to grid data and added to the spatial covariates for CNN components</span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scalar_to_grid_data <span class="op">=</span> torch.from_numpy(buffalo_df[[<span class="st">'hour_t2_sin'</span>, </span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>                                                                <span class="st">'hour_t2_cos'</span>, </span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>                                                                <span class="st">'yday_t2_sin'</span>, </span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>                                                                <span class="st">'yday_t2_cos'</span>]].values).<span class="bu">float</span>()</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>       <span class="co"># the bearing data that will be added as a channel to the spatial covariates</span></span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bearing_x <span class="op">=</span> torch.from_numpy(buffalo_df[[<span class="st">'bearing_tm1'</span>]].values).<span class="bu">float</span>()</span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the target data</span></span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target <span class="op">=</span> torch.tensor(pres_stack)</span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-19"><a href="#cb68-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># number of samples</span></span>
<span id="cb68-20"><a href="#cb68-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_samples <span class="op">=</span> <span class="va">self</span>.spatial_data_x.shape[<span class="dv">0</span>]</span>
<span id="cb68-21"><a href="#cb68-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-22"><a href="#cb68-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb68-23"><a href="#cb68-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># allows for the use of len() function</span></span>
<span id="cb68-24"><a href="#cb68-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.n_samples</span>
<span id="cb68-25"><a href="#cb68-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-26"><a href="#cb68-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb68-27"><a href="#cb68-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># allows for indexing of the dataset</span></span>
<span id="cb68-28"><a href="#cb68-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.spatial_data_x[index], <span class="va">self</span>.scalar_to_grid_data[index], <span class="va">self</span>.bearing_x[index], <span class="va">self</span>.target[index]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now we can create an instance of the dataset class and check that is working as expected.</p>
<div id="cell-75" class="cell" data-executioninfo="{&quot;elapsed&quot;:6,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285053599,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="14597b5f-5af2-41bf-d5ef-60170a55ca02" data-execution_count="50">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an instance of our custom buffalo_data Dataset:</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> buffalo_data()</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the total number of samples loaded (determined by n_samples in the dataset):</span></span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dataset.n_samples)</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve *all* samples (using the slice dataset[:] invokes __getitem__ on all indices).</span></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a><span class="co"># This returns a tuple of (spatial data, scalar-to-grid data, bearing data, target labels).</span></span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>features1, features2, features3, labels <span class="op">=</span> dataset[:]</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Examine the dimensions of each returned tensor for verification:</span></span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Spatial data</span></span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(features1.shape)</span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Scalar-to-grid data</span></span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(features2.shape)</span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-19"><a href="#cb69-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Bearing data</span></span>
<span id="cb69-20"><a href="#cb69-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(features3.shape)</span>
<span id="cb69-21"><a href="#cb69-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-22"><a href="#cb69-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Target labels</span></span>
<span id="cb69-23"><a href="#cb69-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(labels.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>10103
torch.Size([10103, 13, 101, 101])
torch.Size([10103, 4])
torch.Size([10103, 1])
torch.Size([10103, 101, 101])</code></pre>
</div>
</div>
</section>
<section id="split-into-training-validation-and-test-sets" class="level3">
<h3 class="anchored" data-anchor-id="split-into-training-validation-and-test-sets">Split into training, validation and test sets</h3>
<div id="cell-77" class="cell" data-executioninfo="{&quot;elapsed&quot;:5,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285053599,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="de83b0c4-1901-4c3c-f83e-449b317144c8" data-execution_count="51">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>training_split <span class="op">=</span> <span class="fl">0.8</span> <span class="co"># 80% of the data will be used for training</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>validation_split <span class="op">=</span> <span class="fl">0.1</span> <span class="co"># 10% of the data will be used for validation (deciding when to stop training)</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>test_split <span class="op">=</span> <span class="fl">0.1</span> <span class="co"># 10% of the data will be used for testing (model evaluation)</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>dataset_train, dataset_val, dataset_test <span class="op">=</span> torch.utils.data.random_split(dataset, </span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>                                                                         [training_split, </span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>                                                                          validation_split, </span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>                                                                          test_split])</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of training samples: "</span>, <span class="bu">len</span>(dataset_train))</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of validation samples: "</span>, <span class="bu">len</span>(dataset_val))</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of testing samples: "</span>, <span class="bu">len</span>(dataset_test))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Number of training samples:  8083
Number of validation samples:  1010
Number of testing samples:  1010</code></pre>
</div>
</div>
</section>
<section id="create-dataloaders" class="level3">
<h3 class="anchored" data-anchor-id="create-dataloaders">Create dataloaders</h3>
<p>The DataLoader in PyTorch wraps an iterable around the Dataset to enable easy access to the samples.</p>
<div id="cell-79" class="cell" data-execution_count="52">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the batch size for how many samples to process at once in each step:</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>bs <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataLoader for the training dataset with a batch size of bs, and shuffle samples </span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a><span class="co"># so that the model doesn't see data in the same order each epoch.</span></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>dataloader_train <span class="op">=</span> DataLoader(dataset<span class="op">=</span>dataset_train, </span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>                              batch_size<span class="op">=</span>bs, </span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>                              shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataLoader for the validation dataset, also with a batch size of bs and shuffling.</span></span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Even though it's not always mandatory to shuffle validation data, some users keep the same setting.</span></span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>dataloader_val <span class="op">=</span> DataLoader(dataset<span class="op">=</span>dataset_val, </span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>                            batch_size<span class="op">=</span>bs, </span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>                            shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataLoader for the test dataset, likewise with a batch size of bs and shuffling.</span></span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a><span class="co"># As we want to index the testing data for plotting, we will not shuffle the test data.</span></span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>dataloader_test <span class="op">=</span> DataLoader(dataset<span class="op">=</span>dataset_test, </span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a>                             batch_size<span class="op">=</span>bs, </span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a>                             shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Check that the data loader is working as expected.</p>
<div id="cell-81" class="cell" data-executioninfo="{&quot;elapsed&quot;:4,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285053599,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="0bec5797-774b-4f1c-81b5-63a86ca1e1e4" data-execution_count="53">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display image and label.</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="co"># next(iter(dataloader_train)) returns the next batch of the training data</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>features1, features2, features3, labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataloader_train))</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Feature 1 batch shape: </span><span class="sc">{</span>features1<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Feature 2 batch shape: </span><span class="sc">{</span>features2<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Feature 3 batch shape: </span><span class="sc">{</span>features3<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Labels batch shape: </span><span class="sc">{</span>labels<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Feature 1 batch shape: torch.Size([32, 13, 101, 101])
Feature 2 batch shape: torch.Size([32, 4])
Feature 3 batch shape: torch.Size([32, 1])
Labels batch shape: torch.Size([32, 101, 101])</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="load-the-model" class="level1">
<h1>Load the model</h1>
<p>As we have already described the model in detail in the <code>deepSSF_model</code> script, we can simply import the model here.</p>
<p>We will use the same model architecture as in the previous script, except that we will need to use a slightly edited dictionary to account for the additional input channels.</p>
<div id="cell-83" class="cell" data-execution_count="54">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run on the GPU or on the CPU, if a GPU is not available</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span>) <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> torch.device(<span class="st">'cpu'</span>)</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss"> device"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Using cpu device</code></pre>
</div>
</div>
<section id="define-the-parameters-for-the-model" class="level2">
<h2 class="anchored" data-anchor-id="define-the-parameters-for-the-model">Define the parameters for the model</h2>
<p>Here we enter the specific parameter values and hyperparameters for the model. These are the values that will be used to instantiate the model.</p>
<div id="cell-85" class="cell" data-execution_count="55">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In our case the 12 Sentinel-2 layers + slope</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>num_spatial_covs <span class="op">=</span> <span class="dv">13</span> </span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>params_dict <span class="op">=</span> {<span class="st">"batch_size"</span>: <span class="dv">32</span>,</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>               <span class="st">"image_dim"</span>: <span class="dv">101</span>, <span class="co">#number of pixels along the edge of each local patch/image</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>               <span class="st">"pixel_size"</span>: <span class="dv">25</span>, <span class="co">#number of metres along the edge of a pixel</span></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>               <span class="st">"dim_in_nonspatial_to_grid"</span>: <span class="dv">4</span>, <span class="co">#the number of scalar predictors that are converted to a grid and appended to the spatial features</span></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>               <span class="st">"dense_dim_in_nonspatial"</span>: <span class="dv">4</span>, <span class="co">#change this to however many other scalar predictors you have (bearing, velocity etc)</span></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>               <span class="st">"dense_dim_hidden"</span>: <span class="dv">128</span>, <span class="co">#number of nodes in the hidden layers</span></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>               <span class="st">"dense_dim_out"</span>: <span class="dv">128</span>, <span class="co">#number of nodes in the output of the fully connected block (FCN)</span></span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>               <span class="st">"dense_dim_in_all"</span>: <span class="dv">2500</span>,<span class="co"># + 128, #number of inputs entering the fully connected block once the nonspatial features have been concatenated to the spatial features</span></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>               <span class="st">"input_channels"</span>: num_spatial_covs <span class="op">+</span> <span class="dv">4</span>, <span class="co">#number of spatial layers in each image + number of scalar layers that are converted to a grid</span></span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>               <span class="st">"output_channels"</span>: <span class="dv">4</span>, <span class="co">#number of filters to learn</span></span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>               <span class="st">"kernel_size"</span>: <span class="dv">3</span>, <span class="co">#the size of the 2D moving windows / kernels that are being learned</span></span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>               <span class="st">"stride"</span>: <span class="dv">1</span>, <span class="co">#the stride used when applying the kernel.  This reduces the dimension of the output if set to greater than 1</span></span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>               <span class="st">"kernel_size_mp"</span>: <span class="dv">2</span>, <span class="co">#the size of the kernel that is used in max pooling operations</span></span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>               <span class="st">"stride_mp"</span>: <span class="dv">2</span>, <span class="co">#the stride that is used in max pooling operations</span></span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>               <span class="st">"padding"</span>: <span class="dv">1</span>, <span class="co">#the amount of padding to apply to images prior to applying the 2D convolution</span></span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a>               <span class="st">"num_movement_params"</span>: <span class="dv">12</span>, <span class="co">#number of parameters used to parameterise the movement kernel</span></span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a>               <span class="st">"dropout"</span>: <span class="fl">0.1</span>,</span>
<span id="cb78-21"><a href="#cb78-21" aria-hidden="true" tabindex="-1"></a>               <span class="st">"device"</span>: device</span>
<span id="cb78-22"><a href="#cb78-22" aria-hidden="true" tabindex="-1"></a>               }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="instantiate-the-model" class="level2">
<h2 class="anchored" data-anchor-id="instantiate-the-model">Instantiate the model</h2>
<p>As described in the <code>deepSSF_train.ipynb</code> script, we saved the model definition into a file named <code>deepSSF_model.py</code>. We can instantiate the model by importing the file (which was done when importing other packages) and calling the classes parameter dictionary from that script.</p>
<div id="cell-87" class="cell" data-execution_count="56">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> deepSSF_model.ModelParams(params_dict)</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> deepSSF_model.ConvJointModel(params).to(device)</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>ConvJointModel(
  (scalar_grid_output): Scalar_to_Grid_Block()
  (conv_habitat): Conv2d_block_spatial(
    (conv2d): Sequential(
      (0): Conv2d(17, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU()
      (4): Conv2d(4, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (conv_movement): Conv2d_block_toFC(
    (conv2d): Sequential(
      (0): Conv2d(17, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): ReLU()
      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (6): Flatten(start_dim=1, end_dim=-1)
    )
  )
  (fcn_movement_all): FCN_block_all_movement(
    (ffn): Sequential(
      (0): Linear(in_features=2500, out_features=128, bias=True)
      (1): Dropout(p=0.1, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): Dropout(p=0.1, inplace=False)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=12, bias=True)
    )
  )
  (movement_grid_output): Params_to_Grid_Block()
)</code></pre>
</div>
</div>
</section>
<section id="set-model-hyperparameters" class="level2">
<h2 class="anchored" data-anchor-id="set-model-hyperparameters">Set model hyperparameters</h2>
<p>Set the learning rate, loss function, optimizer, scheduler and early stopping.</p>
<div id="cell-89" class="cell" data-execution_count="57">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the negative log-likelihood loss function with mean reduction</span></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> deepSSF_loss.negativeLogLikeLoss(reduction<span class="op">=</span><span class="st">'mean'</span>)</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="co"># path to save the model weights</span></span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>path_save_weights <span class="op">=</span> <span class="ss">f'model_checkpoints/deepSSF_S2_slope_buffalo</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>today_date<span class="sc">}</span><span class="ss">.pt'</span></span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up the Adam optimizer for updating the model's parameters</span></span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>optimiser <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a learning rate scheduler that reduces the LR by a factor of 0.1 </span></span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a><span class="co">#    if validation loss has not improved for 'patience=5' epochs</span></span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.ReduceLROnPlateau(</span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a>    optimiser,  <span class="co"># The optimizer whose learning rate will be adjusted</span></span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span><span class="st">'min'</span>, <span class="co"># The metric to be minimized (e.g., validation loss)</span></span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a>    factor<span class="op">=</span><span class="fl">0.1</span>, <span class="co"># Factor by which the learning rate will be reduced</span></span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a>    patience<span class="op">=</span><span class="dv">5</span>  <span class="co"># Number of epochs with no improvement before learning rate reduces</span></span>
<span id="cb81-19"><a href="#cb81-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb81-20"><a href="#cb81-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-21"><a href="#cb81-21" aria-hidden="true" tabindex="-1"></a><span class="co"># EarlyStopping stops training after 'patience=10' epochs with no improvement, </span></span>
<span id="cb81-22"><a href="#cb81-22" aria-hidden="true" tabindex="-1"></a><span class="co">#    optionally saving the best model weights</span></span>
<span id="cb81-23"><a href="#cb81-23" aria-hidden="true" tabindex="-1"></a>early_stopping <span class="op">=</span> deepSSF_early_stopping.EarlyStopping(patience<span class="op">=</span><span class="dv">20</span>, verbose<span class="op">=</span><span class="va">True</span>, path<span class="op">=</span>path_save_weights)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="training-loop" class="level2">
<h2 class="anchored" data-anchor-id="training-loop">Training loop</h2>
<p>This code defines the main training loop for a single epoch. It iterates over batches from the training dataloader, moves the data to the correct device (e.g., CPU or GPU), calculates the loss, and performs backpropagation to update the model parameters. It also prints periodic updates of the current loss.</p>
<div id="cell-91" class="cell" data-execution_count="58">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_loop(dataloader_train, model, loss_fn, optimiser):</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Runs the training process for one epoch using the given dataloader, model, </span></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a><span class="co">    loss function, and optimizer. Prints progress updates every few batches.</span></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Total number of training examples</span></span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(dataloader_train.dataset)</span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Put model in training mode (affects layers like dropout, batchnorm)</span></span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Variable to accumulate the total loss over the epoch</span></span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Loop over batches in the training dataloader</span></span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch, (x1, x2, x3, y) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader_train):</span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Move the batch of data to the specified device (CPU/GPU)</span></span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a>        x1 <span class="op">=</span> x1.to(device)</span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a>        x2 <span class="op">=</span> x2.to(device)</span>
<span id="cb82-22"><a href="#cb82-22" aria-hidden="true" tabindex="-1"></a>        x3 <span class="op">=</span> x3.to(device)</span>
<span id="cb82-23"><a href="#cb82-23" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.to(device)</span>
<span id="cb82-24"><a href="#cb82-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-25"><a href="#cb82-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass: compute the model output and loss</span></span>
<span id="cb82-26"><a href="#cb82-26" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(model((x1, x2, x3)), y)</span>
<span id="cb82-27"><a href="#cb82-27" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">+=</span> loss</span>
<span id="cb82-28"><a href="#cb82-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-29"><a href="#cb82-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backpropagation: compute gradients and update parameters</span></span>
<span id="cb82-30"><a href="#cb82-30" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb82-31"><a href="#cb82-31" aria-hidden="true" tabindex="-1"></a>        optimiser.step()</span>
<span id="cb82-32"><a href="#cb82-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-33"><a href="#cb82-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reset gradients before the next iteration</span></span>
<span id="cb82-34"><a href="#cb82-34" aria-hidden="true" tabindex="-1"></a>        optimiser.zero_grad()</span>
<span id="cb82-35"><a href="#cb82-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-36"><a href="#cb82-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Print an update every 5 batches to keep track of training progress</span></span>
<span id="cb82-37"><a href="#cb82-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> batch <span class="op">%</span> <span class="dv">5</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb82-38"><a href="#cb82-38" aria-hidden="true" tabindex="-1"></a>            loss_val <span class="op">=</span> loss.item()</span>
<span id="cb82-39"><a href="#cb82-39" aria-hidden="true" tabindex="-1"></a>            current <span class="op">=</span> batch <span class="op">*</span> bs <span class="op">+</span> <span class="bu">len</span>(x1)</span>
<span id="cb82-40"><a href="#cb82-40" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"loss: </span><span class="sc">{</span>loss_val<span class="sc">:&gt;15f}</span><span class="ss">  [</span><span class="sc">{</span>current<span class="sc">:&gt;5d}</span><span class="ss">/</span><span class="sc">{</span>size<span class="sc">:&gt;5d}</span><span class="ss">]"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="test-loop" class="level2">
<h2 class="anchored" data-anchor-id="test-loop">Test loop</h2>
<p>The test loop is similar to the training loop, but it does not perform backpropagation. It calculates the loss on the test set and returns the average loss.</p>
<div id="cell-93" class="cell" data-execution_count="59">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_loop(dataloader_test, model, loss_fn):</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Evaluates the model on the provided test dataset by computing </span></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="co">    the average loss over all batches. </span></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="co">    No gradients are computed during this process (torch.no_grad()).</span></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Set the model to evaluation mode (affects layers like dropout, batchnorm).</span></span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(dataloader_test.dataset)</span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(dataloader_test)</span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-16"><a href="#cb83-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Disable gradient computation to speed up evaluation and reduce memory usage</span></span>
<span id="cb83-17"><a href="#cb83-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb83-18"><a href="#cb83-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Loop through each batch in the test dataloader</span></span>
<span id="cb83-19"><a href="#cb83-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x1, x2, x3, y <span class="kw">in</span> dataloader_test:</span>
<span id="cb83-20"><a href="#cb83-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-21"><a href="#cb83-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Move the batch of data to the appropriate device (CPU/GPU)</span></span>
<span id="cb83-22"><a href="#cb83-22" aria-hidden="true" tabindex="-1"></a>            x1 <span class="op">=</span> x1.to(device)</span>
<span id="cb83-23"><a href="#cb83-23" aria-hidden="true" tabindex="-1"></a>            x2 <span class="op">=</span> x2.to(device)</span>
<span id="cb83-24"><a href="#cb83-24" aria-hidden="true" tabindex="-1"></a>            x3 <span class="op">=</span> x3.to(device)</span>
<span id="cb83-25"><a href="#cb83-25" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y.to(device)</span>
<span id="cb83-26"><a href="#cb83-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-27"><a href="#cb83-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute the loss on the test set (no backward pass needed)</span></span>
<span id="cb83-28"><a href="#cb83-28" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss_fn(model((x1, x2, x3)), y)</span>
<span id="cb83-29"><a href="#cb83-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-30"><a href="#cb83-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Compute average test loss over all batches</span></span>
<span id="cb83-31"><a href="#cb83-31" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">/=</span> num_batches</span>
<span id="cb83-32"><a href="#cb83-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-33"><a href="#cb83-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print the average test loss</span></span>
<span id="cb83-34"><a href="#cb83-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Avg test loss: </span><span class="sc">{</span>test_loss<span class="sc">:&gt;15f}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="train-the-model" class="level2">
<h2 class="anchored" data-anchor-id="train-the-model">Train the model</h2>
<div id="cell-95" class="cell" data-executioninfo="{&quot;elapsed&quot;:191634,&quot;status&quot;:&quot;ok&quot;,&quot;timestamp&quot;:1731285253046,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="a19b7537-4624-44c2-b9f8-1f2df074686d" data-execution_count="60">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>val_losses <span class="op">=</span> []   <span class="co"># Track validation losses across epochs</span></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>    val_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(dataloader_test)</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>t<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">-------------------------------"</span>)</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Run the training loop for one epoch using the training dataloader</span></span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a>    train_loop(dataloader_train, model, loss_fn, optimiser)</span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Evaluate model performance on the validation dataset</span></span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()  <span class="co"># Switch to evaluation mode for proper layer behavior</span></span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb84-17"><a href="#cb84-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x1, x2, x3, y <span class="kw">in</span> dataloader_val:</span>
<span id="cb84-18"><a href="#cb84-18" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Move data to the chosen device (CPU/GPU)</span></span>
<span id="cb84-19"><a href="#cb84-19" aria-hidden="true" tabindex="-1"></a>            x1 <span class="op">=</span> x1.to(device)</span>
<span id="cb84-20"><a href="#cb84-20" aria-hidden="true" tabindex="-1"></a>            x2 <span class="op">=</span> x2.to(device)</span>
<span id="cb84-21"><a href="#cb84-21" aria-hidden="true" tabindex="-1"></a>            x3 <span class="op">=</span> x3.to(device)</span>
<span id="cb84-22"><a href="#cb84-22" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y.to(device)</span>
<span id="cb84-23"><a href="#cb84-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-24"><a href="#cb84-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Accumulate validation loss</span></span>
<span id="cb84-25"><a href="#cb84-25" aria-hidden="true" tabindex="-1"></a>            val_loss <span class="op">+=</span> loss_fn(model((x1, x2, x3)), y)</span>
<span id="cb84-26"><a href="#cb84-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-27"><a href="#cb84-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Step the scheduler based on the validation loss (adjusts learning rate if needed)</span></span>
<span id="cb84-28"><a href="#cb84-28" aria-hidden="true" tabindex="-1"></a>    scheduler.step(val_loss)</span>
<span id="cb84-29"><a href="#cb84-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-30"><a href="#cb84-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Compute the average validation loss and print it, along with the current learning rate</span></span>
<span id="cb84-31"><a href="#cb84-31" aria-hidden="true" tabindex="-1"></a>    val_loss <span class="op">/=</span> num_batches</span>
<span id="cb84-32"><a href="#cb84-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Avg validation loss: </span><span class="sc">{</span>val_loss<span class="sc">:&gt;15f}</span><span class="ss">"</span>)</span>
<span id="cb84-33"><a href="#cb84-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Learning rate: </span><span class="sc">{</span>scheduler<span class="sc">.</span>get_last_lr()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb84-34"><a href="#cb84-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-35"><a href="#cb84-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5. Track the validation loss for plotting or monitoring</span></span>
<span id="cb84-36"><a href="#cb84-36" aria-hidden="true" tabindex="-1"></a>    val_losses.append(val_loss)</span>
<span id="cb84-37"><a href="#cb84-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-38"><a href="#cb84-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6. Early stopping: if no improvement in validation loss for a set patience, stop training</span></span>
<span id="cb84-39"><a href="#cb84-39" aria-hidden="true" tabindex="-1"></a>    early_stopping(val_loss, model)</span>
<span id="cb84-40"><a href="#cb84-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> early_stopping.early_stop:</span>
<span id="cb84-41"><a href="#cb84-41" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Early stopping"</span>)</span>
<span id="cb84-42"><a href="#cb84-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Restore the best model weights saved by EarlyStopping</span></span>
<span id="cb84-43"><a href="#cb84-43" aria-hidden="true" tabindex="-1"></a>        model.load_state_dict(torch.load(path_save_weights, weights_only<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb84-44"><a href="#cb84-44" aria-hidden="true" tabindex="-1"></a>        test_loop(dataloader_test, model, loss_fn)  <span class="co"># Evaluate on test set once training stops</span></span>
<span id="cb84-45"><a href="#cb84-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb84-46"><a href="#cb84-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb84-47"><a href="#cb84-47" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb84-48"><a href="#cb84-48" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb84-49"><a href="#cb84-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-50"><a href="#cb84-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Done!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1
-------------------------------
loss:        0.000820  [   32/ 8083]
loss:        0.000711  [  192/ 8083]
loss:        0.000606  [  352/ 8083]
loss:        0.000546  [  512/ 8083]
loss:        0.000617  [  672/ 8083]
loss:        0.000587  [  832/ 8083]
loss:        0.000603  [  992/ 8083]
loss:        0.000676  [ 1152/ 8083]
loss:        0.000518  [ 1312/ 8083]
loss:        0.000564  [ 1472/ 8083]
loss:        0.000654  [ 1632/ 8083]
loss:        0.000506  [ 1792/ 8083]
loss:        0.000603  [ 1952/ 8083]
loss:        0.000650  [ 2112/ 8083]
loss:        0.000649  [ 2272/ 8083]
loss:        0.000556  [ 2432/ 8083]
loss:        0.000539  [ 2592/ 8083]
loss:        0.000555  [ 2752/ 8083]
loss:        0.000558  [ 2912/ 8083]
loss:        0.000582  [ 3072/ 8083]
loss:        0.000570  [ 3232/ 8083]
loss:        0.000524  [ 3392/ 8083]
loss:        0.000517  [ 3552/ 8083]
loss:        0.000587  [ 3712/ 8083]
loss:        0.000572  [ 3872/ 8083]
loss:        0.000633  [ 4032/ 8083]
loss:        0.000585  [ 4192/ 8083]
loss:        0.000583  [ 4352/ 8083]
loss:        0.000514  [ 4512/ 8083]
loss:        0.000503  [ 4672/ 8083]
loss:        0.000611  [ 4832/ 8083]
loss:        0.000568  [ 4992/ 8083]
loss:        0.000574  [ 5152/ 8083]
loss:        0.000620  [ 5312/ 8083]
loss:        0.000561  [ 5472/ 8083]
loss:        0.000537  [ 5632/ 8083]
loss:        0.000562  [ 5792/ 8083]
loss:        0.000643  [ 5952/ 8083]
loss:        0.000585  [ 6112/ 8083]
loss:        0.000611  [ 6272/ 8083]
loss:        0.000564  [ 6432/ 8083]
loss:        0.000629  [ 6592/ 8083]
loss:        0.000493  [ 6752/ 8083]
loss:        0.000522  [ 6912/ 8083]
loss:        0.000711  [ 7072/ 8083]
loss:        0.000575  [ 7232/ 8083]
loss:        0.000603  [ 7392/ 8083]
loss:        0.000582  [ 7552/ 8083]
loss:        0.000562  [ 7712/ 8083]
loss:        0.000586  [ 7872/ 8083]
loss:        0.000644  [ 8032/ 8083]

Avg validation loss:        0.000532
Learning rate: [0.001]
Validation loss decreased (inf --&gt; 0.000532).  Saving model ...


Epoch 2
-------------------------------
loss:        0.000521  [   32/ 8083]
loss:        0.000573  [  192/ 8083]
loss:        0.000459  [  352/ 8083]
loss:        0.000565  [  512/ 8083]
loss:        0.000557  [  672/ 8083]
loss:        0.000533  [  832/ 8083]
loss:        0.000599  [  992/ 8083]
loss:        0.000614  [ 1152/ 8083]
loss:        0.000453  [ 1312/ 8083]
loss:        0.000692  [ 1472/ 8083]
loss:        0.000511  [ 1632/ 8083]
loss:        0.000563  [ 1792/ 8083]
loss:        0.000493  [ 1952/ 8083]
loss:        0.000593  [ 2112/ 8083]
loss:        0.000544  [ 2272/ 8083]
loss:        0.000613  [ 2432/ 8083]
loss:        0.000584  [ 2592/ 8083]
loss:        0.000631  [ 2752/ 8083]
loss:        0.000551  [ 2912/ 8083]
loss:        0.000578  [ 3072/ 8083]
loss:        0.000587  [ 3232/ 8083]
loss:        0.000622  [ 3392/ 8083]
loss:        0.000550  [ 3552/ 8083]
loss:        0.000534  [ 3712/ 8083]
loss:        0.000548  [ 3872/ 8083]
loss:        0.000567  [ 4032/ 8083]
loss:        0.000624  [ 4192/ 8083]
loss:        0.000559  [ 4352/ 8083]
loss:        0.000455  [ 4512/ 8083]
loss:        0.000612  [ 4672/ 8083]
loss:        0.000512  [ 4832/ 8083]
loss:        0.000585  [ 4992/ 8083]
loss:        0.000578  [ 5152/ 8083]
loss:        0.000428  [ 5312/ 8083]
loss:        0.000475  [ 5472/ 8083]
loss:        0.000504  [ 5632/ 8083]
loss:        0.000505  [ 5792/ 8083]
loss:        0.000521  [ 5952/ 8083]
loss:        0.000558  [ 6112/ 8083]
loss:        0.000407  [ 6272/ 8083]
loss:        0.000507  [ 6432/ 8083]
loss:        0.000511  [ 6592/ 8083]
loss:        0.000455  [ 6752/ 8083]
loss:        0.000535  [ 6912/ 8083]
loss:        0.000610  [ 7072/ 8083]
loss:        0.000635  [ 7232/ 8083]
loss:        0.000546  [ 7392/ 8083]
loss:        0.000557  [ 7552/ 8083]
loss:        0.000608  [ 7712/ 8083]
loss:        0.000525  [ 7872/ 8083]
loss:        0.000421  [ 8032/ 8083]

Avg validation loss:        0.000518
Learning rate: [0.001]
Validation loss decreased (0.000532 --&gt; 0.000518).  Saving model ...


Epoch 3
-------------------------------
loss:        0.000410  [   32/ 8083]
loss:        0.000544  [  192/ 8083]
loss:        0.000506  [  352/ 8083]
loss:        0.000443  [  512/ 8083]
loss:        0.000638  [  672/ 8083]
loss:        0.000502  [  832/ 8083]
loss:        0.000523  [  992/ 8083]
loss:        0.000594  [ 1152/ 8083]
loss:        0.000546  [ 1312/ 8083]
loss:        0.000494  [ 1472/ 8083]
loss:        0.000503  [ 1632/ 8083]
loss:        0.000637  [ 1792/ 8083]
loss:        0.000501  [ 1952/ 8083]
loss:        0.000593  [ 2112/ 8083]
loss:        0.000488  [ 2272/ 8083]
loss:        0.000481  [ 2432/ 8083]
loss:        0.000490  [ 2592/ 8083]
loss:        0.000448  [ 2752/ 8083]
loss:        0.000625  [ 2912/ 8083]
loss:        0.000573  [ 3072/ 8083]
loss:        0.000558  [ 3232/ 8083]
loss:        0.000521  [ 3392/ 8083]
loss:        0.000545  [ 3552/ 8083]
loss:        0.000495  [ 3712/ 8083]
loss:        0.000457  [ 3872/ 8083]
loss:        0.000540  [ 4032/ 8083]
loss:        0.000563  [ 4192/ 8083]
loss:        0.000546  [ 4352/ 8083]
loss:        0.000575  [ 4512/ 8083]
loss:        0.000636  [ 4672/ 8083]
loss:        0.000487  [ 4832/ 8083]
loss:        0.000565  [ 4992/ 8083]
loss:        0.000503  [ 5152/ 8083]
loss:        0.000584  [ 5312/ 8083]
loss:        0.000559  [ 5472/ 8083]
loss:        0.000600  [ 5632/ 8083]
loss:        0.000544  [ 5792/ 8083]
loss:        0.000440  [ 5952/ 8083]
loss:        0.000484  [ 6112/ 8083]
loss:        0.000510  [ 6272/ 8083]
loss:        0.000542  [ 6432/ 8083]
loss:        0.000605  [ 6592/ 8083]
loss:        0.000567  [ 6752/ 8083]
loss:        0.000632  [ 6912/ 8083]
loss:        0.000458  [ 7072/ 8083]
loss:        0.000479  [ 7232/ 8083]
loss:        0.000486  [ 7392/ 8083]
loss:        0.000505  [ 7552/ 8083]
loss:        0.000493  [ 7712/ 8083]
loss:        0.000622  [ 7872/ 8083]
loss:        0.000546  [ 8032/ 8083]

Avg validation loss:        0.000513
Learning rate: [0.001]
Validation loss decreased (0.000518 --&gt; 0.000513).  Saving model ...


Epoch 4
-------------------------------
loss:        0.000541  [   32/ 8083]
loss:        0.000519  [  192/ 8083]
loss:        0.000566  [  352/ 8083]
loss:        0.000441  [  512/ 8083]
loss:        0.000581  [  672/ 8083]
loss:        0.000480  [  832/ 8083]
loss:        0.000427  [  992/ 8083]
loss:        0.000628  [ 1152/ 8083]
loss:        0.000560  [ 1312/ 8083]
loss:        0.000594  [ 1472/ 8083]
loss:        0.000682  [ 1632/ 8083]
loss:        0.000565  [ 1792/ 8083]
loss:        0.000538  [ 1952/ 8083]
loss:        0.000448  [ 2112/ 8083]
loss:        0.000546  [ 2272/ 8083]
loss:        0.000532  [ 2432/ 8083]
loss:        0.000628  [ 2592/ 8083]
loss:        0.000463  [ 2752/ 8083]
loss:        0.000512  [ 2912/ 8083]
loss:        0.000582  [ 3072/ 8083]
loss:        0.000668  [ 3232/ 8083]
loss:        0.000533  [ 3392/ 8083]
loss:        0.000573  [ 3552/ 8083]
loss:        0.000443  [ 3712/ 8083]
loss:        0.000578  [ 3872/ 8083]
loss:        0.000444  [ 4032/ 8083]
loss:        0.000583  [ 4192/ 8083]
loss:        0.000570  [ 4352/ 8083]
loss:        0.000586  [ 4512/ 8083]
loss:        0.000544  [ 4672/ 8083]
loss:        0.000491  [ 4832/ 8083]
loss:        0.000487  [ 4992/ 8083]
loss:        0.000468  [ 5152/ 8083]
loss:        0.000574  [ 5312/ 8083]
loss:        0.000534  [ 5472/ 8083]
loss:        0.000675  [ 5632/ 8083]
loss:        0.000531  [ 5792/ 8083]
loss:        0.000568  [ 5952/ 8083]
loss:        0.000504  [ 6112/ 8083]
loss:        0.000632  [ 6272/ 8083]
loss:        0.000543  [ 6432/ 8083]
loss:        0.000546  [ 6592/ 8083]
loss:        0.000558  [ 6752/ 8083]
loss:        0.000598  [ 6912/ 8083]
loss:        0.000499  [ 7072/ 8083]
loss:        0.000459  [ 7232/ 8083]
loss:        0.000577  [ 7392/ 8083]
loss:        0.000560  [ 7552/ 8083]
loss:        0.000508  [ 7712/ 8083]
loss:        0.000427  [ 7872/ 8083]
loss:        0.000488  [ 8032/ 8083]

Avg validation loss:        0.000513
Learning rate: [0.001]
EarlyStopping counter: 1 out of 20


Epoch 5
-------------------------------
loss:        0.000481  [   32/ 8083]
loss:        0.000549  [  192/ 8083]
loss:        0.000586  [  352/ 8083]
loss:        0.000581  [  512/ 8083]
loss:        0.000515  [  672/ 8083]
loss:        0.000596  [  832/ 8083]
loss:        0.000516  [  992/ 8083]
loss:        0.000563  [ 1152/ 8083]
loss:        0.000478  [ 1312/ 8083]
loss:        0.000605  [ 1472/ 8083]
loss:        0.000457  [ 1632/ 8083]
loss:        0.000446  [ 1792/ 8083]
loss:        0.000484  [ 1952/ 8083]
loss:        0.000551  [ 2112/ 8083]
loss:        0.000584  [ 2272/ 8083]
loss:        0.000656  [ 2432/ 8083]
loss:        0.000468  [ 2592/ 8083]
loss:        0.000508  [ 2752/ 8083]
loss:        0.000586  [ 2912/ 8083]
loss:        0.000547  [ 3072/ 8083]
loss:        0.000507  [ 3232/ 8083]
loss:        0.000485  [ 3392/ 8083]
loss:        0.000563  [ 3552/ 8083]
loss:        0.000600  [ 3712/ 8083]
loss:        0.000576  [ 3872/ 8083]
loss:        0.000523  [ 4032/ 8083]
loss:        0.000450  [ 4192/ 8083]
loss:        0.000614  [ 4352/ 8083]
loss:        0.000490  [ 4512/ 8083]
loss:        0.000519  [ 4672/ 8083]
loss:        0.000545  [ 4832/ 8083]
loss:        0.000634  [ 4992/ 8083]
loss:        0.000494  [ 5152/ 8083]
loss:        0.000606  [ 5312/ 8083]
loss:        0.000474  [ 5472/ 8083]
loss:        0.000438  [ 5632/ 8083]
loss:        0.000545  [ 5792/ 8083]
loss:        0.000537  [ 5952/ 8083]
loss:        0.000499  [ 6112/ 8083]
loss:        0.000483  [ 6272/ 8083]
loss:        0.000597  [ 6432/ 8083]
loss:        0.000519  [ 6592/ 8083]
loss:        0.000511  [ 6752/ 8083]
loss:        0.000542  [ 6912/ 8083]
loss:        0.000586  [ 7072/ 8083]
loss:        0.000462  [ 7232/ 8083]
loss:        0.000389  [ 7392/ 8083]
loss:        0.000559  [ 7552/ 8083]
loss:        0.000524  [ 7712/ 8083]
loss:        0.000610  [ 7872/ 8083]
loss:        0.000511  [ 8032/ 8083]

Avg validation loss:        0.000513
Learning rate: [0.001]
Validation loss decreased (0.000513 --&gt; 0.000513).  Saving model ...


Epoch 6
-------------------------------
loss:        0.000497  [   32/ 8083]
loss:        0.000540  [  192/ 8083]
loss:        0.000621  [  352/ 8083]
loss:        0.000466  [  512/ 8083]
loss:        0.000472  [  672/ 8083]
loss:        0.000605  [  832/ 8083]
loss:        0.000526  [  992/ 8083]
loss:        0.000560  [ 1152/ 8083]
loss:        0.000524  [ 1312/ 8083]
loss:        0.000526  [ 1472/ 8083]
loss:        0.000532  [ 1632/ 8083]
loss:        0.000481  [ 1792/ 8083]
loss:        0.000587  [ 1952/ 8083]
loss:        0.000498  [ 2112/ 8083]
loss:        0.000565  [ 2272/ 8083]
loss:        0.000623  [ 2432/ 8083]
loss:        0.000633  [ 2592/ 8083]
loss:        0.000510  [ 2752/ 8083]
loss:        0.000431  [ 2912/ 8083]
loss:        0.000477  [ 3072/ 8083]
loss:        0.000480  [ 3232/ 8083]
loss:        0.000536  [ 3392/ 8083]
loss:        0.000466  [ 3552/ 8083]
loss:        0.000479  [ 3712/ 8083]
loss:        0.000618  [ 3872/ 8083]
loss:        0.000492  [ 4032/ 8083]
loss:        0.000547  [ 4192/ 8083]
loss:        0.000486  [ 4352/ 8083]
loss:        0.000506  [ 4512/ 8083]
loss:        0.000531  [ 4672/ 8083]
loss:        0.000588  [ 4832/ 8083]
loss:        0.000484  [ 4992/ 8083]
loss:        0.000494  [ 5152/ 8083]
loss:        0.000476  [ 5312/ 8083]
loss:        0.000587  [ 5472/ 8083]
loss:        0.000522  [ 5632/ 8083]
loss:        0.000487  [ 5792/ 8083]
loss:        0.000470  [ 5952/ 8083]
loss:        0.000558  [ 6112/ 8083]
loss:        0.000648  [ 6272/ 8083]
loss:        0.000480  [ 6432/ 8083]
loss:        0.000437  [ 6592/ 8083]
loss:        0.000498  [ 6752/ 8083]
loss:        0.000387  [ 6912/ 8083]
loss:        0.000553  [ 7072/ 8083]
loss:        0.000658  [ 7232/ 8083]
loss:        0.000516  [ 7392/ 8083]
loss:        0.000544  [ 7552/ 8083]
loss:        0.000566  [ 7712/ 8083]
loss:        0.000450  [ 7872/ 8083]
loss:        0.000638  [ 8032/ 8083]

Avg validation loss:        0.000516
Learning rate: [0.001]
EarlyStopping counter: 1 out of 20


Epoch 7
-------------------------------
loss:        0.000603  [   32/ 8083]
loss:        0.000587  [  192/ 8083]
loss:        0.000605  [  352/ 8083]
loss:        0.000608  [  512/ 8083]
loss:        0.000543  [  672/ 8083]
loss:        0.000513  [  832/ 8083]
loss:        0.000457  [  992/ 8083]
loss:        0.000534  [ 1152/ 8083]
loss:        0.000447  [ 1312/ 8083]
loss:        0.000555  [ 1472/ 8083]
loss:        0.000515  [ 1632/ 8083]
loss:        0.000543  [ 1792/ 8083]
loss:        0.000447  [ 1952/ 8083]
loss:        0.000639  [ 2112/ 8083]
loss:        0.000488  [ 2272/ 8083]
loss:        0.000585  [ 2432/ 8083]
loss:        0.000459  [ 2592/ 8083]
loss:        0.000611  [ 2752/ 8083]
loss:        0.000465  [ 2912/ 8083]
loss:        0.000480  [ 3072/ 8083]
loss:        0.000427  [ 3232/ 8083]
loss:        0.000563  [ 3392/ 8083]
loss:        0.000447  [ 3552/ 8083]
loss:        0.000591  [ 3712/ 8083]
loss:        0.000496  [ 3872/ 8083]
loss:        0.000528  [ 4032/ 8083]
loss:        0.000624  [ 4192/ 8083]
loss:        0.000562  [ 4352/ 8083]
loss:        0.000590  [ 4512/ 8083]
loss:        0.000409  [ 4672/ 8083]
loss:        0.000545  [ 4832/ 8083]
loss:        0.000394  [ 4992/ 8083]
loss:        0.000626  [ 5152/ 8083]
loss:        0.000420  [ 5312/ 8083]
loss:        0.000488  [ 5472/ 8083]
loss:        0.000456  [ 5632/ 8083]
loss:        0.000575  [ 5792/ 8083]
loss:        0.000553  [ 5952/ 8083]
loss:        0.000417  [ 6112/ 8083]
loss:        0.000498  [ 6272/ 8083]
loss:        0.000477  [ 6432/ 8083]
loss:        0.000541  [ 6592/ 8083]
loss:        0.000361  [ 6752/ 8083]
loss:        0.000537  [ 6912/ 8083]
loss:        0.000578  [ 7072/ 8083]
loss:        0.000566  [ 7232/ 8083]
loss:        0.000552  [ 7392/ 8083]
loss:        0.000520  [ 7552/ 8083]
loss:        0.000585  [ 7712/ 8083]
loss:        0.000537  [ 7872/ 8083]
loss:        0.000520  [ 8032/ 8083]

Avg validation loss:        0.000511
Learning rate: [0.001]
Validation loss decreased (0.000513 --&gt; 0.000511).  Saving model ...


Epoch 8
-------------------------------
loss:        0.000494  [   32/ 8083]
loss:        0.000475  [  192/ 8083]
loss:        0.000557  [  352/ 8083]
loss:        0.000510  [  512/ 8083]
loss:        0.000542  [  672/ 8083]
loss:        0.000571  [  832/ 8083]
loss:        0.000579  [  992/ 8083]
loss:        0.000464  [ 1152/ 8083]
loss:        0.000581  [ 1312/ 8083]
loss:        0.000581  [ 1472/ 8083]
loss:        0.000533  [ 1632/ 8083]
loss:        0.000574  [ 1792/ 8083]
loss:        0.000607  [ 1952/ 8083]
loss:        0.000514  [ 2112/ 8083]
loss:        0.000561  [ 2272/ 8083]
loss:        0.000520  [ 2432/ 8083]
loss:        0.000538  [ 2592/ 8083]
loss:        0.000585  [ 2752/ 8083]
loss:        0.000621  [ 2912/ 8083]
loss:        0.000546  [ 3072/ 8083]
loss:        0.000530  [ 3232/ 8083]
loss:        0.000583  [ 3392/ 8083]
loss:        0.000590  [ 3552/ 8083]
loss:        0.000543  [ 3712/ 8083]
loss:        0.000638  [ 3872/ 8083]
loss:        0.000524  [ 4032/ 8083]
loss:        0.000443  [ 4192/ 8083]
loss:        0.000557  [ 4352/ 8083]
loss:        0.000617  [ 4512/ 8083]
loss:        0.000559  [ 4672/ 8083]
loss:        0.000445  [ 4832/ 8083]
loss:        0.000444  [ 4992/ 8083]
loss:        0.000514  [ 5152/ 8083]
loss:        0.000441  [ 5312/ 8083]
loss:        0.000433  [ 5472/ 8083]
loss:        0.000417  [ 5632/ 8083]
loss:        0.000612  [ 5792/ 8083]
loss:        0.000599  [ 5952/ 8083]
loss:        0.000502  [ 6112/ 8083]
loss:        0.000530  [ 6272/ 8083]
loss:        0.000624  [ 6432/ 8083]
loss:        0.000525  [ 6592/ 8083]
loss:        0.000522  [ 6752/ 8083]
loss:        0.000548  [ 6912/ 8083]
loss:        0.000589  [ 7072/ 8083]
loss:        0.000532  [ 7232/ 8083]
loss:        0.000524  [ 7392/ 8083]
loss:        0.000567  [ 7552/ 8083]
loss:        0.000536  [ 7712/ 8083]
loss:        0.000613  [ 7872/ 8083]
loss:        0.000595  [ 8032/ 8083]

Avg validation loss:        0.000511
Learning rate: [0.001]
EarlyStopping counter: 1 out of 20


Epoch 9
-------------------------------
loss:        0.000512  [   32/ 8083]
loss:        0.000407  [  192/ 8083]
loss:        0.000467  [  352/ 8083]
loss:        0.000537  [  512/ 8083]
loss:        0.000488  [  672/ 8083]
loss:        0.000467  [  832/ 8083]
loss:        0.000544  [  992/ 8083]
loss:        0.000509  [ 1152/ 8083]
loss:        0.000425  [ 1312/ 8083]
loss:        0.000544  [ 1472/ 8083]
loss:        0.000522  [ 1632/ 8083]
loss:        0.000516  [ 1792/ 8083]
loss:        0.000534  [ 1952/ 8083]
loss:        0.000489  [ 2112/ 8083]
loss:        0.000465  [ 2272/ 8083]
loss:        0.000518  [ 2432/ 8083]
loss:        0.000504  [ 2592/ 8083]
loss:        0.000551  [ 2752/ 8083]
loss:        0.000533  [ 2912/ 8083]
loss:        0.000479  [ 3072/ 8083]
loss:        0.000501  [ 3232/ 8083]
loss:        0.000612  [ 3392/ 8083]
loss:        0.000535  [ 3552/ 8083]
loss:        0.000447  [ 3712/ 8083]
loss:        0.000580  [ 3872/ 8083]
loss:        0.000469  [ 4032/ 8083]
loss:        0.000452  [ 4192/ 8083]
loss:        0.000493  [ 4352/ 8083]
loss:        0.000527  [ 4512/ 8083]
loss:        0.000541  [ 4672/ 8083]
loss:        0.000452  [ 4832/ 8083]
loss:        0.000513  [ 4992/ 8083]
loss:        0.000536  [ 5152/ 8083]
loss:        0.000546  [ 5312/ 8083]
loss:        0.000588  [ 5472/ 8083]
loss:        0.000553  [ 5632/ 8083]
loss:        0.000379  [ 5792/ 8083]
loss:        0.000518  [ 5952/ 8083]
loss:        0.000525  [ 6112/ 8083]
loss:        0.000568  [ 6272/ 8083]
loss:        0.000565  [ 6432/ 8083]
loss:        0.000653  [ 6592/ 8083]
loss:        0.000532  [ 6752/ 8083]
loss:        0.000476  [ 6912/ 8083]
loss:        0.000424  [ 7072/ 8083]
loss:        0.000546  [ 7232/ 8083]
loss:        0.000517  [ 7392/ 8083]
loss:        0.000508  [ 7552/ 8083]
loss:        0.000545  [ 7712/ 8083]
loss:        0.000438  [ 7872/ 8083]
loss:        0.000501  [ 8032/ 8083]

Avg validation loss:        0.000514
Learning rate: [0.001]
EarlyStopping counter: 2 out of 20


Epoch 10
-------------------------------
loss:        0.000574  [   32/ 8083]
loss:        0.000545  [  192/ 8083]
loss:        0.000439  [  352/ 8083]
loss:        0.000500  [  512/ 8083]
loss:        0.000551  [  672/ 8083]
loss:        0.000587  [  832/ 8083]
loss:        0.000484  [  992/ 8083]
loss:        0.000514  [ 1152/ 8083]
loss:        0.000511  [ 1312/ 8083]
loss:        0.000587  [ 1472/ 8083]
loss:        0.000654  [ 1632/ 8083]
loss:        0.000620  [ 1792/ 8083]
loss:        0.000495  [ 1952/ 8083]
loss:        0.000459  [ 2112/ 8083]
loss:        0.000582  [ 2272/ 8083]
loss:        0.000532  [ 2432/ 8083]
loss:        0.000384  [ 2592/ 8083]
loss:        0.000532  [ 2752/ 8083]
loss:        0.000508  [ 2912/ 8083]
loss:        0.000437  [ 3072/ 8083]
loss:        0.000556  [ 3232/ 8083]
loss:        0.000491  [ 3392/ 8083]
loss:        0.000585  [ 3552/ 8083]
loss:        0.000422  [ 3712/ 8083]
loss:        0.000475  [ 3872/ 8083]
loss:        0.000569  [ 4032/ 8083]
loss:        0.000499  [ 4192/ 8083]
loss:        0.000599  [ 4352/ 8083]
loss:        0.000556  [ 4512/ 8083]
loss:        0.000548  [ 4672/ 8083]
loss:        0.000606  [ 4832/ 8083]
loss:        0.000553  [ 4992/ 8083]
loss:        0.000555  [ 5152/ 8083]
loss:        0.000606  [ 5312/ 8083]
loss:        0.000492  [ 5472/ 8083]
loss:        0.000509  [ 5632/ 8083]
loss:        0.000529  [ 5792/ 8083]
loss:        0.000516  [ 5952/ 8083]
loss:        0.000504  [ 6112/ 8083]
loss:        0.000658  [ 6272/ 8083]
loss:        0.000539  [ 6432/ 8083]
loss:        0.000470  [ 6592/ 8083]
loss:        0.000551  [ 6752/ 8083]
loss:        0.000517  [ 6912/ 8083]
loss:        0.000472  [ 7072/ 8083]
loss:        0.000584  [ 7232/ 8083]
loss:        0.000522  [ 7392/ 8083]
loss:        0.000564  [ 7552/ 8083]
loss:        0.000506  [ 7712/ 8083]
loss:        0.000511  [ 7872/ 8083]
loss:        0.000527  [ 8032/ 8083]

Avg validation loss:        0.000509
Learning rate: [0.001]
Validation loss decreased (0.000511 --&gt; 0.000509).  Saving model ...


Epoch 11
-------------------------------
loss:        0.000648  [   32/ 8083]
loss:        0.000522  [  192/ 8083]
loss:        0.000590  [  352/ 8083]
loss:        0.000542  [  512/ 8083]
loss:        0.000434  [  672/ 8083]
loss:        0.000528  [  832/ 8083]
loss:        0.000498  [  992/ 8083]
loss:        0.000487  [ 1152/ 8083]
loss:        0.000523  [ 1312/ 8083]
loss:        0.000510  [ 1472/ 8083]
loss:        0.000521  [ 1632/ 8083]
loss:        0.000497  [ 1792/ 8083]
loss:        0.000521  [ 1952/ 8083]
loss:        0.000439  [ 2112/ 8083]
loss:        0.000532  [ 2272/ 8083]
loss:        0.000461  [ 2432/ 8083]
loss:        0.000429  [ 2592/ 8083]
loss:        0.000550  [ 2752/ 8083]
loss:        0.000539  [ 2912/ 8083]
loss:        0.000449  [ 3072/ 8083]
loss:        0.000472  [ 3232/ 8083]
loss:        0.000699  [ 3392/ 8083]
loss:        0.000511  [ 3552/ 8083]
loss:        0.000438  [ 3712/ 8083]
loss:        0.000521  [ 3872/ 8083]
loss:        0.000493  [ 4032/ 8083]
loss:        0.000475  [ 4192/ 8083]
loss:        0.000600  [ 4352/ 8083]
loss:        0.000538  [ 4512/ 8083]
loss:        0.000475  [ 4672/ 8083]
loss:        0.000623  [ 4832/ 8083]
loss:        0.000588  [ 4992/ 8083]
loss:        0.000519  [ 5152/ 8083]
loss:        0.000487  [ 5312/ 8083]
loss:        0.000480  [ 5472/ 8083]
loss:        0.000515  [ 5632/ 8083]
loss:        0.000586  [ 5792/ 8083]
loss:        0.000569  [ 5952/ 8083]
loss:        0.000546  [ 6112/ 8083]
loss:        0.000532  [ 6272/ 8083]
loss:        0.000510  [ 6432/ 8083]
loss:        0.000602  [ 6592/ 8083]
loss:        0.000480  [ 6752/ 8083]
loss:        0.000463  [ 6912/ 8083]
loss:        0.000481  [ 7072/ 8083]
loss:        0.000447  [ 7232/ 8083]
loss:        0.000563  [ 7392/ 8083]
loss:        0.000533  [ 7552/ 8083]
loss:        0.000599  [ 7712/ 8083]
loss:        0.000517  [ 7872/ 8083]
loss:        0.000482  [ 8032/ 8083]

Avg validation loss:        0.000510
Learning rate: [0.001]
EarlyStopping counter: 1 out of 20


Epoch 12
-------------------------------
loss:        0.000494  [   32/ 8083]
loss:        0.000537  [  192/ 8083]
loss:        0.000512  [  352/ 8083]
loss:        0.000491  [  512/ 8083]
loss:        0.000516  [  672/ 8083]
loss:        0.000496  [  832/ 8083]
loss:        0.000544  [  992/ 8083]
loss:        0.000490  [ 1152/ 8083]
loss:        0.000669  [ 1312/ 8083]
loss:        0.000526  [ 1472/ 8083]
loss:        0.000511  [ 1632/ 8083]
loss:        0.000515  [ 1792/ 8083]
loss:        0.000494  [ 1952/ 8083]
loss:        0.000532  [ 2112/ 8083]
loss:        0.000472  [ 2272/ 8083]
loss:        0.000483  [ 2432/ 8083]
loss:        0.000521  [ 2592/ 8083]
loss:        0.000467  [ 2752/ 8083]
loss:        0.000485  [ 2912/ 8083]
loss:        0.000686  [ 3072/ 8083]
loss:        0.000510  [ 3232/ 8083]
loss:        0.000546  [ 3392/ 8083]
loss:        0.000605  [ 3552/ 8083]
loss:        0.000646  [ 3712/ 8083]
loss:        0.000414  [ 3872/ 8083]
loss:        0.000494  [ 4032/ 8083]
loss:        0.000463  [ 4192/ 8083]
loss:        0.000570  [ 4352/ 8083]
loss:        0.000511  [ 4512/ 8083]
loss:        0.000439  [ 4672/ 8083]
loss:        0.000476  [ 4832/ 8083]
loss:        0.000533  [ 4992/ 8083]
loss:        0.000489  [ 5152/ 8083]
loss:        0.000494  [ 5312/ 8083]
loss:        0.000486  [ 5472/ 8083]
loss:        0.000528  [ 5632/ 8083]
loss:        0.000490  [ 5792/ 8083]
loss:        0.000577  [ 5952/ 8083]
loss:        0.000509  [ 6112/ 8083]
loss:        0.000520  [ 6272/ 8083]
loss:        0.000476  [ 6432/ 8083]
loss:        0.000594  [ 6592/ 8083]
loss:        0.000518  [ 6752/ 8083]
loss:        0.000654  [ 6912/ 8083]
loss:        0.000574  [ 7072/ 8083]
loss:        0.000430  [ 7232/ 8083]
loss:        0.000490  [ 7392/ 8083]
loss:        0.000458  [ 7552/ 8083]
loss:        0.000541  [ 7712/ 8083]
loss:        0.000589  [ 7872/ 8083]
loss:        0.000630  [ 8032/ 8083]

Avg validation loss:        0.000514
Learning rate: [0.001]
EarlyStopping counter: 2 out of 20


Epoch 13
-------------------------------
loss:        0.000546  [   32/ 8083]
loss:        0.000496  [  192/ 8083]
loss:        0.000472  [  352/ 8083]
loss:        0.000683  [  512/ 8083]
loss:        0.000511  [  672/ 8083]
loss:        0.000470  [  832/ 8083]
loss:        0.000506  [  992/ 8083]
loss:        0.000540  [ 1152/ 8083]
loss:        0.000574  [ 1312/ 8083]
loss:        0.000365  [ 1472/ 8083]
loss:        0.000746  [ 1632/ 8083]
loss:        0.000537  [ 1792/ 8083]
loss:        0.000554  [ 1952/ 8083]
loss:        0.000547  [ 2112/ 8083]
loss:        0.000517  [ 2272/ 8083]
loss:        0.000546  [ 2432/ 8083]
loss:        0.000504  [ 2592/ 8083]
loss:        0.000570  [ 2752/ 8083]
loss:        0.000545  [ 2912/ 8083]
loss:        0.000503  [ 3072/ 8083]
loss:        0.000451  [ 3232/ 8083]
loss:        0.000615  [ 3392/ 8083]
loss:        0.000474  [ 3552/ 8083]
loss:        0.000531  [ 3712/ 8083]
loss:        0.000594  [ 3872/ 8083]
loss:        0.000532  [ 4032/ 8083]
loss:        0.000452  [ 4192/ 8083]
loss:        0.000475  [ 4352/ 8083]
loss:        0.000596  [ 4512/ 8083]
loss:        0.000474  [ 4672/ 8083]
loss:        0.000473  [ 4832/ 8083]
loss:        0.000610  [ 4992/ 8083]
loss:        0.000492  [ 5152/ 8083]
loss:        0.000508  [ 5312/ 8083]
loss:        0.000506  [ 5472/ 8083]
loss:        0.000640  [ 5632/ 8083]
loss:        0.000517  [ 5792/ 8083]
loss:        0.000532  [ 5952/ 8083]
loss:        0.000488  [ 6112/ 8083]
loss:        0.000500  [ 6272/ 8083]
loss:        0.000483  [ 6432/ 8083]
loss:        0.000554  [ 6592/ 8083]
loss:        0.000498  [ 6752/ 8083]
loss:        0.000474  [ 6912/ 8083]
loss:        0.000539  [ 7072/ 8083]
loss:        0.000576  [ 7232/ 8083]
loss:        0.000453  [ 7392/ 8083]
loss:        0.000446  [ 7552/ 8083]
loss:        0.000575  [ 7712/ 8083]
loss:        0.000562  [ 7872/ 8083]
loss:        0.000511  [ 8032/ 8083]

Avg validation loss:        0.000507
Learning rate: [0.001]
Validation loss decreased (0.000509 --&gt; 0.000507).  Saving model ...


Epoch 14
-------------------------------
loss:        0.000599  [   32/ 8083]
loss:        0.000500  [  192/ 8083]
loss:        0.000543  [  352/ 8083]
loss:        0.000592  [  512/ 8083]
loss:        0.000488  [  672/ 8083]
loss:        0.000563  [  832/ 8083]
loss:        0.000545  [  992/ 8083]
loss:        0.000608  [ 1152/ 8083]
loss:        0.000519  [ 1312/ 8083]
loss:        0.000566  [ 1472/ 8083]
loss:        0.000593  [ 1632/ 8083]
loss:        0.000477  [ 1792/ 8083]
loss:        0.000649  [ 1952/ 8083]
loss:        0.000548  [ 2112/ 8083]
loss:        0.000544  [ 2272/ 8083]
loss:        0.000616  [ 2432/ 8083]
loss:        0.000492  [ 2592/ 8083]
loss:        0.000396  [ 2752/ 8083]
loss:        0.000499  [ 2912/ 8083]
loss:        0.000544  [ 3072/ 8083]
loss:        0.000512  [ 3232/ 8083]
loss:        0.000504  [ 3392/ 8083]
loss:        0.000435  [ 3552/ 8083]
loss:        0.000526  [ 3712/ 8083]
loss:        0.000483  [ 3872/ 8083]
loss:        0.000487  [ 4032/ 8083]
loss:        0.000485  [ 4192/ 8083]
loss:        0.000511  [ 4352/ 8083]
loss:        0.000547  [ 4512/ 8083]
loss:        0.000493  [ 4672/ 8083]
loss:        0.000534  [ 4832/ 8083]
loss:        0.000474  [ 4992/ 8083]
loss:        0.000593  [ 5152/ 8083]
loss:        0.000512  [ 5312/ 8083]
loss:        0.000451  [ 5472/ 8083]
loss:        0.000588  [ 5632/ 8083]
loss:        0.000524  [ 5792/ 8083]
loss:        0.000547  [ 5952/ 8083]
loss:        0.000517  [ 6112/ 8083]
loss:        0.000538  [ 6272/ 8083]
loss:        0.000568  [ 6432/ 8083]
loss:        0.000550  [ 6592/ 8083]
loss:        0.000403  [ 6752/ 8083]
loss:        0.000469  [ 6912/ 8083]
loss:        0.000473  [ 7072/ 8083]
loss:        0.000435  [ 7232/ 8083]
loss:        0.000528  [ 7392/ 8083]
loss:        0.000567  [ 7552/ 8083]
loss:        0.000538  [ 7712/ 8083]
loss:        0.000520  [ 7872/ 8083]
loss:        0.000504  [ 8032/ 8083]

Avg validation loss:        0.000511
Learning rate: [0.001]
EarlyStopping counter: 1 out of 20


Epoch 15
-------------------------------
loss:        0.000469  [   32/ 8083]
loss:        0.000538  [  192/ 8083]
loss:        0.000497  [  352/ 8083]
loss:        0.000440  [  512/ 8083]
loss:        0.000480  [  672/ 8083]
loss:        0.000505  [  832/ 8083]
loss:        0.000497  [  992/ 8083]
loss:        0.000581  [ 1152/ 8083]
loss:        0.000532  [ 1312/ 8083]
loss:        0.000509  [ 1472/ 8083]
loss:        0.000420  [ 1632/ 8083]
loss:        0.000519  [ 1792/ 8083]
loss:        0.000459  [ 1952/ 8083]
loss:        0.000415  [ 2112/ 8083]
loss:        0.000509  [ 2272/ 8083]
loss:        0.000494  [ 2432/ 8083]
loss:        0.000460  [ 2592/ 8083]
loss:        0.000511  [ 2752/ 8083]
loss:        0.000574  [ 2912/ 8083]
loss:        0.000602  [ 3072/ 8083]
loss:        0.000476  [ 3232/ 8083]
loss:        0.000456  [ 3392/ 8083]
loss:        0.000489  [ 3552/ 8083]
loss:        0.000559  [ 3712/ 8083]
loss:        0.000576  [ 3872/ 8083]
loss:        0.000551  [ 4032/ 8083]
loss:        0.000600  [ 4192/ 8083]
loss:        0.000545  [ 4352/ 8083]
loss:        0.000520  [ 4512/ 8083]
loss:        0.000459  [ 4672/ 8083]
loss:        0.000521  [ 4832/ 8083]
loss:        0.000589  [ 4992/ 8083]
loss:        0.000478  [ 5152/ 8083]
loss:        0.000583  [ 5312/ 8083]
loss:        0.000478  [ 5472/ 8083]
loss:        0.000589  [ 5632/ 8083]
loss:        0.000576  [ 5792/ 8083]
loss:        0.000543  [ 5952/ 8083]
loss:        0.000503  [ 6112/ 8083]
loss:        0.000525  [ 6272/ 8083]
loss:        0.000462  [ 6432/ 8083]
loss:        0.000501  [ 6592/ 8083]
loss:        0.000566  [ 6752/ 8083]
loss:        0.000551  [ 6912/ 8083]
loss:        0.000451  [ 7072/ 8083]
loss:        0.000628  [ 7232/ 8083]
loss:        0.000594  [ 7392/ 8083]
loss:        0.000588  [ 7552/ 8083]
loss:        0.000531  [ 7712/ 8083]
loss:        0.000567  [ 7872/ 8083]
loss:        0.000508  [ 8032/ 8083]

Avg validation loss:        0.000512
Learning rate: [0.001]
EarlyStopping counter: 2 out of 20


Epoch 16
-------------------------------
loss:        0.000602  [   32/ 8083]
loss:        0.000605  [  192/ 8083]
loss:        0.000420  [  352/ 8083]
loss:        0.000489  [  512/ 8083]
loss:        0.000578  [  672/ 8083]
loss:        0.000484  [  832/ 8083]
loss:        0.000564  [  992/ 8083]
loss:        0.000489  [ 1152/ 8083]
loss:        0.000612  [ 1312/ 8083]
loss:        0.000402  [ 1472/ 8083]
loss:        0.000464  [ 1632/ 8083]
loss:        0.000518  [ 1792/ 8083]
loss:        0.000516  [ 1952/ 8083]
loss:        0.000465  [ 2112/ 8083]
loss:        0.000591  [ 2272/ 8083]
loss:        0.000547  [ 2432/ 8083]
loss:        0.000494  [ 2592/ 8083]
loss:        0.000546  [ 2752/ 8083]
loss:        0.000540  [ 2912/ 8083]
loss:        0.000514  [ 3072/ 8083]
loss:        0.000554  [ 3232/ 8083]
loss:        0.000530  [ 3392/ 8083]
loss:        0.000596  [ 3552/ 8083]
loss:        0.000509  [ 3712/ 8083]
loss:        0.000534  [ 3872/ 8083]
loss:        0.000532  [ 4032/ 8083]
loss:        0.000627  [ 4192/ 8083]
loss:        0.000669  [ 4352/ 8083]
loss:        0.000464  [ 4512/ 8083]
loss:        0.000484  [ 4672/ 8083]
loss:        0.000623  [ 4832/ 8083]
loss:        0.000503  [ 4992/ 8083]
loss:        0.000450  [ 5152/ 8083]
loss:        0.000438  [ 5312/ 8083]
loss:        0.000470  [ 5472/ 8083]
loss:        0.000523  [ 5632/ 8083]
loss:        0.000623  [ 5792/ 8083]
loss:        0.000488  [ 5952/ 8083]
loss:        0.000439  [ 6112/ 8083]
loss:        0.000441  [ 6272/ 8083]
loss:        0.000547  [ 6432/ 8083]
loss:        0.000549  [ 6592/ 8083]
loss:        0.000546  [ 6752/ 8083]
loss:        0.000495  [ 6912/ 8083]
loss:        0.000520  [ 7072/ 8083]
loss:        0.000571  [ 7232/ 8083]
loss:        0.000510  [ 7392/ 8083]
loss:        0.000566  [ 7552/ 8083]
loss:        0.000487  [ 7712/ 8083]
loss:        0.000553  [ 7872/ 8083]
loss:        0.000437  [ 8032/ 8083]

Avg validation loss:        0.000505
Learning rate: [0.001]
Validation loss decreased (0.000507 --&gt; 0.000505).  Saving model ...


Epoch 17
-------------------------------
loss:        0.000482  [   32/ 8083]
loss:        0.000443  [  192/ 8083]
loss:        0.000438  [  352/ 8083]
loss:        0.000439  [  512/ 8083]
loss:        0.000478  [  672/ 8083]
loss:        0.000484  [  832/ 8083]
loss:        0.000525  [  992/ 8083]
loss:        0.000458  [ 1152/ 8083]
loss:        0.000540  [ 1312/ 8083]
loss:        0.000511  [ 1472/ 8083]
loss:        0.000523  [ 1632/ 8083]
loss:        0.000497  [ 1792/ 8083]
loss:        0.000498  [ 1952/ 8083]
loss:        0.000564  [ 2112/ 8083]
loss:        0.000441  [ 2272/ 8083]
loss:        0.000519  [ 2432/ 8083]
loss:        0.000598  [ 2592/ 8083]
loss:        0.000492  [ 2752/ 8083]
loss:        0.000543  [ 2912/ 8083]
loss:        0.000574  [ 3072/ 8083]
loss:        0.000642  [ 3232/ 8083]
loss:        0.000598  [ 3392/ 8083]
loss:        0.000422  [ 3552/ 8083]
loss:        0.000417  [ 3712/ 8083]
loss:        0.000496  [ 3872/ 8083]
loss:        0.000545  [ 4032/ 8083]
loss:        0.000519  [ 4192/ 8083]
loss:        0.000509  [ 4352/ 8083]
loss:        0.000487  [ 4512/ 8083]
loss:        0.000559  [ 4672/ 8083]
loss:        0.000523  [ 4832/ 8083]
loss:        0.000526  [ 4992/ 8083]
loss:        0.000426  [ 5152/ 8083]
loss:        0.000441  [ 5312/ 8083]
loss:        0.000584  [ 5472/ 8083]
loss:        0.000533  [ 5632/ 8083]
loss:        0.000553  [ 5792/ 8083]
loss:        0.000441  [ 5952/ 8083]
loss:        0.000542  [ 6112/ 8083]
loss:        0.000434  [ 6272/ 8083]
loss:        0.000593  [ 6432/ 8083]
loss:        0.000556  [ 6592/ 8083]
loss:        0.000570  [ 6752/ 8083]
loss:        0.000526  [ 6912/ 8083]
loss:        0.000500  [ 7072/ 8083]
loss:        0.000547  [ 7232/ 8083]
loss:        0.000525  [ 7392/ 8083]
loss:        0.000601  [ 7552/ 8083]
loss:        0.000567  [ 7712/ 8083]
loss:        0.000525  [ 7872/ 8083]
loss:        0.000524  [ 8032/ 8083]

Avg validation loss:        0.000508
Learning rate: [0.001]
EarlyStopping counter: 1 out of 20


Epoch 18
-------------------------------
loss:        0.000484  [   32/ 8083]
loss:        0.000500  [  192/ 8083]
loss:        0.000450  [  352/ 8083]
loss:        0.000672  [  512/ 8083]
loss:        0.000542  [  672/ 8083]
loss:        0.000528  [  832/ 8083]
loss:        0.000618  [  992/ 8083]
loss:        0.000514  [ 1152/ 8083]
loss:        0.000533  [ 1312/ 8083]
loss:        0.000502  [ 1472/ 8083]
loss:        0.000472  [ 1632/ 8083]
loss:        0.000540  [ 1792/ 8083]
loss:        0.000618  [ 1952/ 8083]
loss:        0.000478  [ 2112/ 8083]
loss:        0.000552  [ 2272/ 8083]
loss:        0.000533  [ 2432/ 8083]
loss:        0.000554  [ 2592/ 8083]
loss:        0.000498  [ 2752/ 8083]
loss:        0.000488  [ 2912/ 8083]
loss:        0.000514  [ 3072/ 8083]
loss:        0.000684  [ 3232/ 8083]
loss:        0.000533  [ 3392/ 8083]
loss:        0.000496  [ 3552/ 8083]
loss:        0.000521  [ 3712/ 8083]
loss:        0.000528  [ 3872/ 8083]
loss:        0.000480  [ 4032/ 8083]
loss:        0.000550  [ 4192/ 8083]
loss:        0.000414  [ 4352/ 8083]
loss:        0.000625  [ 4512/ 8083]
loss:        0.000562  [ 4672/ 8083]
loss:        0.000539  [ 4832/ 8083]
loss:        0.000547  [ 4992/ 8083]
loss:        0.000557  [ 5152/ 8083]
loss:        0.000508  [ 5312/ 8083]
loss:        0.000586  [ 5472/ 8083]
loss:        0.000683  [ 5632/ 8083]
loss:        0.000481  [ 5792/ 8083]
loss:        0.000561  [ 5952/ 8083]
loss:        0.000506  [ 6112/ 8083]
loss:        0.000496  [ 6272/ 8083]
loss:        0.000499  [ 6432/ 8083]
loss:        0.000478  [ 6592/ 8083]
loss:        0.000547  [ 6752/ 8083]
loss:        0.000485  [ 6912/ 8083]
loss:        0.000570  [ 7072/ 8083]
loss:        0.000500  [ 7232/ 8083]
loss:        0.000551  [ 7392/ 8083]
loss:        0.000477  [ 7552/ 8083]
loss:        0.000475  [ 7712/ 8083]
loss:        0.000579  [ 7872/ 8083]
loss:        0.000499  [ 8032/ 8083]

Avg validation loss:        0.000503
Learning rate: [0.001]
Validation loss decreased (0.000505 --&gt; 0.000503).  Saving model ...


Epoch 19
-------------------------------
loss:        0.000466  [   32/ 8083]
loss:        0.000475  [  192/ 8083]
loss:        0.000488  [  352/ 8083]
loss:        0.000459  [  512/ 8083]
loss:        0.000461  [  672/ 8083]
loss:        0.000470  [  832/ 8083]
loss:        0.000540  [  992/ 8083]
loss:        0.000567  [ 1152/ 8083]
loss:        0.000481  [ 1312/ 8083]
loss:        0.000474  [ 1472/ 8083]
loss:        0.000503  [ 1632/ 8083]
loss:        0.000540  [ 1792/ 8083]
loss:        0.000510  [ 1952/ 8083]
loss:        0.000513  [ 2112/ 8083]
loss:        0.000483  [ 2272/ 8083]
loss:        0.000508  [ 2432/ 8083]
loss:        0.000456  [ 2592/ 8083]
loss:        0.000618  [ 2752/ 8083]
loss:        0.000496  [ 2912/ 8083]
loss:        0.000515  [ 3072/ 8083]
loss:        0.000567  [ 3232/ 8083]
loss:        0.000444  [ 3392/ 8083]
loss:        0.000320  [ 3552/ 8083]
loss:        0.000583  [ 3712/ 8083]
loss:        0.000577  [ 3872/ 8083]
loss:        0.000492  [ 4032/ 8083]
loss:        0.000606  [ 4192/ 8083]
loss:        0.000474  [ 4352/ 8083]
loss:        0.000532  [ 4512/ 8083]
loss:        0.000513  [ 4672/ 8083]
loss:        0.000428  [ 4832/ 8083]
loss:        0.000496  [ 4992/ 8083]
loss:        0.000488  [ 5152/ 8083]
loss:        0.000549  [ 5312/ 8083]
loss:        0.000558  [ 5472/ 8083]
loss:        0.000581  [ 5632/ 8083]
loss:        0.000568  [ 5792/ 8083]
loss:        0.000567  [ 5952/ 8083]
loss:        0.000471  [ 6112/ 8083]
loss:        0.000572  [ 6272/ 8083]
loss:        0.000562  [ 6432/ 8083]
loss:        0.000480  [ 6592/ 8083]
loss:        0.000599  [ 6752/ 8083]
loss:        0.000480  [ 6912/ 8083]
loss:        0.000527  [ 7072/ 8083]
loss:        0.000472  [ 7232/ 8083]
loss:        0.000522  [ 7392/ 8083]
loss:        0.000602  [ 7552/ 8083]
loss:        0.000425  [ 7712/ 8083]
loss:        0.000492  [ 7872/ 8083]
loss:        0.000517  [ 8032/ 8083]

Avg validation loss:        0.000507
Learning rate: [0.001]
EarlyStopping counter: 1 out of 20


Epoch 20
-------------------------------
loss:        0.000468  [   32/ 8083]
loss:        0.000545  [  192/ 8083]
loss:        0.000559  [  352/ 8083]
loss:        0.000477  [  512/ 8083]
loss:        0.000522  [  672/ 8083]
loss:        0.000507  [  832/ 8083]
loss:        0.000535  [  992/ 8083]
loss:        0.000557  [ 1152/ 8083]
loss:        0.000500  [ 1312/ 8083]
loss:        0.000597  [ 1472/ 8083]
loss:        0.000538  [ 1632/ 8083]
loss:        0.000611  [ 1792/ 8083]
loss:        0.000491  [ 1952/ 8083]
loss:        0.000449  [ 2112/ 8083]
loss:        0.000498  [ 2272/ 8083]
loss:        0.000507  [ 2432/ 8083]
loss:        0.000558  [ 2592/ 8083]
loss:        0.000434  [ 2752/ 8083]
loss:        0.000499  [ 2912/ 8083]
loss:        0.000461  [ 3072/ 8083]
loss:        0.000539  [ 3232/ 8083]
loss:        0.000468  [ 3392/ 8083]
loss:        0.000481  [ 3552/ 8083]
loss:        0.000585  [ 3712/ 8083]
loss:        0.000500  [ 3872/ 8083]
loss:        0.000581  [ 4032/ 8083]
loss:        0.000434  [ 4192/ 8083]
loss:        0.000455  [ 4352/ 8083]
loss:        0.000537  [ 4512/ 8083]
loss:        0.000525  [ 4672/ 8083]
loss:        0.000542  [ 4832/ 8083]
loss:        0.000461  [ 4992/ 8083]
loss:        0.000468  [ 5152/ 8083]
loss:        0.000444  [ 5312/ 8083]
loss:        0.000526  [ 5472/ 8083]
loss:        0.000585  [ 5632/ 8083]
loss:        0.000586  [ 5792/ 8083]
loss:        0.000597  [ 5952/ 8083]
loss:        0.000351  [ 6112/ 8083]
loss:        0.000473  [ 6272/ 8083]
loss:        0.000465  [ 6432/ 8083]
loss:        0.000364  [ 6592/ 8083]
loss:        0.000478  [ 6752/ 8083]
loss:        0.000454  [ 6912/ 8083]
loss:        0.000522  [ 7072/ 8083]
loss:        0.000526  [ 7232/ 8083]
loss:        0.000501  [ 7392/ 8083]
loss:        0.000590  [ 7552/ 8083]
loss:        0.000498  [ 7712/ 8083]
loss:        0.000528  [ 7872/ 8083]
loss:        0.000442  [ 8032/ 8083]

Avg validation loss:        0.000512
Learning rate: [0.001]
EarlyStopping counter: 2 out of 20


Epoch 21
-------------------------------
loss:        0.000412  [   32/ 8083]
loss:        0.000465  [  192/ 8083]
loss:        0.000383  [  352/ 8083]
loss:        0.000594  [  512/ 8083]
loss:        0.000563  [  672/ 8083]
loss:        0.000445  [  832/ 8083]
loss:        0.000524  [  992/ 8083]
loss:        0.000521  [ 1152/ 8083]
loss:        0.000487  [ 1312/ 8083]
loss:        0.000502  [ 1472/ 8083]
loss:        0.000632  [ 1632/ 8083]
loss:        0.000468  [ 1792/ 8083]
loss:        0.000633  [ 1952/ 8083]
loss:        0.000550  [ 2112/ 8083]
loss:        0.000477  [ 2272/ 8083]
loss:        0.000563  [ 2432/ 8083]
loss:        0.000516  [ 2592/ 8083]
loss:        0.000519  [ 2752/ 8083]
loss:        0.000641  [ 2912/ 8083]
loss:        0.000441  [ 3072/ 8083]
loss:        0.000582  [ 3232/ 8083]
loss:        0.000544  [ 3392/ 8083]
loss:        0.000494  [ 3552/ 8083]
loss:        0.000537  [ 3712/ 8083]
loss:        0.000556  [ 3872/ 8083]
loss:        0.000517  [ 4032/ 8083]
loss:        0.000488  [ 4192/ 8083]
loss:        0.000447  [ 4352/ 8083]
loss:        0.000488  [ 4512/ 8083]
loss:        0.000515  [ 4672/ 8083]
loss:        0.000537  [ 4832/ 8083]
loss:        0.000518  [ 4992/ 8083]
loss:        0.000595  [ 5152/ 8083]
loss:        0.000570  [ 5312/ 8083]
loss:        0.000583  [ 5472/ 8083]
loss:        0.000534  [ 5632/ 8083]
loss:        0.000474  [ 5792/ 8083]
loss:        0.000505  [ 5952/ 8083]
loss:        0.000654  [ 6112/ 8083]
loss:        0.000432  [ 6272/ 8083]
loss:        0.000467  [ 6432/ 8083]
loss:        0.000556  [ 6592/ 8083]
loss:        0.000527  [ 6752/ 8083]
loss:        0.000551  [ 6912/ 8083]
loss:        0.000443  [ 7072/ 8083]
loss:        0.000508  [ 7232/ 8083]
loss:        0.000527  [ 7392/ 8083]
loss:        0.000405  [ 7552/ 8083]
loss:        0.000347  [ 7712/ 8083]
loss:        0.000501  [ 7872/ 8083]
loss:        0.000489  [ 8032/ 8083]

Avg validation loss:        0.000504
Learning rate: [0.001]
EarlyStopping counter: 3 out of 20


Epoch 22
-------------------------------
loss:        0.000654  [   32/ 8083]
loss:        0.000526  [  192/ 8083]
loss:        0.000575  [  352/ 8083]
loss:        0.000539  [  512/ 8083]
loss:        0.000496  [  672/ 8083]
loss:        0.000471  [  832/ 8083]
loss:        0.000563  [  992/ 8083]
loss:        0.000521  [ 1152/ 8083]
loss:        0.000683  [ 1312/ 8083]
loss:        0.000515  [ 1472/ 8083]
loss:        0.000589  [ 1632/ 8083]
loss:        0.000548  [ 1792/ 8083]
loss:        0.000450  [ 1952/ 8083]
loss:        0.000544  [ 2112/ 8083]
loss:        0.000453  [ 2272/ 8083]
loss:        0.000477  [ 2432/ 8083]
loss:        0.000492  [ 2592/ 8083]
loss:        0.000576  [ 2752/ 8083]
loss:        0.000512  [ 2912/ 8083]
loss:        0.000521  [ 3072/ 8083]
loss:        0.000432  [ 3232/ 8083]
loss:        0.000499  [ 3392/ 8083]
loss:        0.000507  [ 3552/ 8083]
loss:        0.000550  [ 3712/ 8083]
loss:        0.000606  [ 3872/ 8083]
loss:        0.000575  [ 4032/ 8083]
loss:        0.000427  [ 4192/ 8083]
loss:        0.000588  [ 4352/ 8083]
loss:        0.000642  [ 4512/ 8083]
loss:        0.000388  [ 4672/ 8083]
loss:        0.000472  [ 4832/ 8083]
loss:        0.000474  [ 4992/ 8083]
loss:        0.000533  [ 5152/ 8083]
loss:        0.000495  [ 5312/ 8083]
loss:        0.000466  [ 5472/ 8083]
loss:        0.000458  [ 5632/ 8083]
loss:        0.000404  [ 5792/ 8083]
loss:        0.000463  [ 5952/ 8083]
loss:        0.000465  [ 6112/ 8083]
loss:        0.000524  [ 6272/ 8083]
loss:        0.000535  [ 6432/ 8083]
loss:        0.000544  [ 6592/ 8083]
loss:        0.000509  [ 6752/ 8083]
loss:        0.000545  [ 6912/ 8083]
loss:        0.000542  [ 7072/ 8083]
loss:        0.000468  [ 7232/ 8083]
loss:        0.000556  [ 7392/ 8083]
loss:        0.000548  [ 7552/ 8083]
loss:        0.000531  [ 7712/ 8083]
loss:        0.000613  [ 7872/ 8083]
loss:        0.000535  [ 8032/ 8083]

Avg validation loss:        0.000513
Learning rate: [0.001]
EarlyStopping counter: 4 out of 20


Epoch 23
-------------------------------
loss:        0.000478  [   32/ 8083]
loss:        0.000464  [  192/ 8083]
loss:        0.000514  [  352/ 8083]
loss:        0.000449  [  512/ 8083]
loss:        0.000460  [  672/ 8083]
loss:        0.000490  [  832/ 8083]
loss:        0.000509  [  992/ 8083]
loss:        0.000500  [ 1152/ 8083]
loss:        0.000508  [ 1312/ 8083]
loss:        0.000504  [ 1472/ 8083]
loss:        0.000505  [ 1632/ 8083]
loss:        0.000466  [ 1792/ 8083]
loss:        0.000485  [ 1952/ 8083]
loss:        0.000481  [ 2112/ 8083]
loss:        0.000507  [ 2272/ 8083]
loss:        0.000492  [ 2432/ 8083]
loss:        0.000508  [ 2592/ 8083]
loss:        0.000505  [ 2752/ 8083]
loss:        0.000564  [ 2912/ 8083]
loss:        0.000393  [ 3072/ 8083]
loss:        0.000419  [ 3232/ 8083]
loss:        0.000560  [ 3392/ 8083]
loss:        0.000444  [ 3552/ 8083]
loss:        0.000546  [ 3712/ 8083]
loss:        0.000507  [ 3872/ 8083]
loss:        0.000542  [ 4032/ 8083]
loss:        0.000514  [ 4192/ 8083]
loss:        0.000438  [ 4352/ 8083]
loss:        0.000517  [ 4512/ 8083]
loss:        0.000462  [ 4672/ 8083]
loss:        0.000491  [ 4832/ 8083]
loss:        0.000529  [ 4992/ 8083]
loss:        0.000493  [ 5152/ 8083]
loss:        0.000534  [ 5312/ 8083]
loss:        0.000561  [ 5472/ 8083]
loss:        0.000568  [ 5632/ 8083]
loss:        0.000513  [ 5792/ 8083]
loss:        0.000491  [ 5952/ 8083]
loss:        0.000603  [ 6112/ 8083]
loss:        0.000398  [ 6272/ 8083]
loss:        0.000505  [ 6432/ 8083]
loss:        0.000494  [ 6592/ 8083]
loss:        0.000531  [ 6752/ 8083]
loss:        0.000560  [ 6912/ 8083]
loss:        0.000517  [ 7072/ 8083]
loss:        0.000489  [ 7232/ 8083]
loss:        0.000532  [ 7392/ 8083]
loss:        0.000635  [ 7552/ 8083]
loss:        0.000480  [ 7712/ 8083]
loss:        0.000426  [ 7872/ 8083]
loss:        0.000497  [ 8032/ 8083]

Avg validation loss:        0.000504
Learning rate: [0.001]
EarlyStopping counter: 5 out of 20


Epoch 24
-------------------------------
loss:        0.000485  [   32/ 8083]
loss:        0.000520  [  192/ 8083]
loss:        0.000500  [  352/ 8083]
loss:        0.000447  [  512/ 8083]
loss:        0.000507  [  672/ 8083]
loss:        0.000558  [  832/ 8083]
loss:        0.000566  [  992/ 8083]
loss:        0.000605  [ 1152/ 8083]
loss:        0.000507  [ 1312/ 8083]
loss:        0.000501  [ 1472/ 8083]
loss:        0.000570  [ 1632/ 8083]
loss:        0.000536  [ 1792/ 8083]
loss:        0.000529  [ 1952/ 8083]
loss:        0.000490  [ 2112/ 8083]
loss:        0.000602  [ 2272/ 8083]
loss:        0.000480  [ 2432/ 8083]
loss:        0.000463  [ 2592/ 8083]
loss:        0.000521  [ 2752/ 8083]
loss:        0.000488  [ 2912/ 8083]
loss:        0.000546  [ 3072/ 8083]
loss:        0.000618  [ 3232/ 8083]
loss:        0.000542  [ 3392/ 8083]
loss:        0.000410  [ 3552/ 8083]
loss:        0.000456  [ 3712/ 8083]
loss:        0.000538  [ 3872/ 8083]
loss:        0.000530  [ 4032/ 8083]
loss:        0.000587  [ 4192/ 8083]
loss:        0.000526  [ 4352/ 8083]
loss:        0.000404  [ 4512/ 8083]
loss:        0.000535  [ 4672/ 8083]
loss:        0.000560  [ 4832/ 8083]
loss:        0.000496  [ 4992/ 8083]
loss:        0.000588  [ 5152/ 8083]
loss:        0.000540  [ 5312/ 8083]
loss:        0.000510  [ 5472/ 8083]
loss:        0.000545  [ 5632/ 8083]
loss:        0.000505  [ 5792/ 8083]
loss:        0.000565  [ 5952/ 8083]
loss:        0.000608  [ 6112/ 8083]
loss:        0.000542  [ 6272/ 8083]
loss:        0.000545  [ 6432/ 8083]
loss:        0.000540  [ 6592/ 8083]
loss:        0.000513  [ 6752/ 8083]
loss:        0.000490  [ 6912/ 8083]
loss:        0.000440  [ 7072/ 8083]
loss:        0.000621  [ 7232/ 8083]
loss:        0.000550  [ 7392/ 8083]
loss:        0.000503  [ 7552/ 8083]
loss:        0.000490  [ 7712/ 8083]
loss:        0.000528  [ 7872/ 8083]
loss:        0.000472  [ 8032/ 8083]

Avg validation loss:        0.000504
Learning rate: [0.0001]
EarlyStopping counter: 6 out of 20


Epoch 25
-------------------------------
loss:        0.000389  [   32/ 8083]
loss:        0.000435  [  192/ 8083]
loss:        0.000592  [  352/ 8083]
loss:        0.000564  [  512/ 8083]
loss:        0.000381  [  672/ 8083]
loss:        0.000538  [  832/ 8083]
loss:        0.000587  [  992/ 8083]
loss:        0.000545  [ 1152/ 8083]
loss:        0.000491  [ 1312/ 8083]
loss:        0.000495  [ 1472/ 8083]
loss:        0.000508  [ 1632/ 8083]
loss:        0.000591  [ 1792/ 8083]
loss:        0.000555  [ 1952/ 8083]
loss:        0.000452  [ 2112/ 8083]
loss:        0.000462  [ 2272/ 8083]
loss:        0.000516  [ 2432/ 8083]
loss:        0.000408  [ 2592/ 8083]
loss:        0.000517  [ 2752/ 8083]
loss:        0.000465  [ 2912/ 8083]
loss:        0.000453  [ 3072/ 8083]
loss:        0.000504  [ 3232/ 8083]
loss:        0.000518  [ 3392/ 8083]
loss:        0.000595  [ 3552/ 8083]
loss:        0.000458  [ 3712/ 8083]
loss:        0.000475  [ 3872/ 8083]
loss:        0.000600  [ 4032/ 8083]
loss:        0.000547  [ 4192/ 8083]
loss:        0.000495  [ 4352/ 8083]
loss:        0.000542  [ 4512/ 8083]
loss:        0.000529  [ 4672/ 8083]
loss:        0.000404  [ 4832/ 8083]
loss:        0.000358  [ 4992/ 8083]
loss:        0.000377  [ 5152/ 8083]
loss:        0.000461  [ 5312/ 8083]
loss:        0.000514  [ 5472/ 8083]
loss:        0.000449  [ 5632/ 8083]
loss:        0.000527  [ 5792/ 8083]
loss:        0.000439  [ 5952/ 8083]
loss:        0.000588  [ 6112/ 8083]
loss:        0.000485  [ 6272/ 8083]
loss:        0.000478  [ 6432/ 8083]
loss:        0.000486  [ 6592/ 8083]
loss:        0.000570  [ 6752/ 8083]
loss:        0.000517  [ 6912/ 8083]
loss:        0.000530  [ 7072/ 8083]
loss:        0.000512  [ 7232/ 8083]
loss:        0.000412  [ 7392/ 8083]
loss:        0.000474  [ 7552/ 8083]
loss:        0.000545  [ 7712/ 8083]
loss:        0.000436  [ 7872/ 8083]
loss:        0.000399  [ 8032/ 8083]

Avg validation loss:        0.000501
Learning rate: [0.0001]
Validation loss decreased (0.000503 --&gt; 0.000501).  Saving model ...


Epoch 26
-------------------------------
loss:        0.000563  [   32/ 8083]
loss:        0.000504  [  192/ 8083]
loss:        0.000499  [  352/ 8083]
loss:        0.000480  [  512/ 8083]
loss:        0.000427  [  672/ 8083]
loss:        0.000476  [  832/ 8083]
loss:        0.000364  [  992/ 8083]
loss:        0.000623  [ 1152/ 8083]
loss:        0.000505  [ 1312/ 8083]
loss:        0.000571  [ 1472/ 8083]
loss:        0.000587  [ 1632/ 8083]
loss:        0.000448  [ 1792/ 8083]
loss:        0.000495  [ 1952/ 8083]
loss:        0.000549  [ 2112/ 8083]
loss:        0.000470  [ 2272/ 8083]
loss:        0.000466  [ 2432/ 8083]
loss:        0.000477  [ 2592/ 8083]
loss:        0.000442  [ 2752/ 8083]
loss:        0.000550  [ 2912/ 8083]
loss:        0.000366  [ 3072/ 8083]
loss:        0.000469  [ 3232/ 8083]
loss:        0.000464  [ 3392/ 8083]
loss:        0.000403  [ 3552/ 8083]
loss:        0.000379  [ 3712/ 8083]
loss:        0.000542  [ 3872/ 8083]
loss:        0.000457  [ 4032/ 8083]
loss:        0.000443  [ 4192/ 8083]
loss:        0.000545  [ 4352/ 8083]
loss:        0.000444  [ 4512/ 8083]
loss:        0.000498  [ 4672/ 8083]
loss:        0.000372  [ 4832/ 8083]
loss:        0.000564  [ 4992/ 8083]
loss:        0.000546  [ 5152/ 8083]
loss:        0.000502  [ 5312/ 8083]
loss:        0.000457  [ 5472/ 8083]
loss:        0.000490  [ 5632/ 8083]
loss:        0.000548  [ 5792/ 8083]
loss:        0.000459  [ 5952/ 8083]
loss:        0.000531  [ 6112/ 8083]
loss:        0.000520  [ 6272/ 8083]
loss:        0.000463  [ 6432/ 8083]
loss:        0.000475  [ 6592/ 8083]
loss:        0.000517  [ 6752/ 8083]
loss:        0.000471  [ 6912/ 8083]
loss:        0.000532  [ 7072/ 8083]
loss:        0.000599  [ 7232/ 8083]
loss:        0.000569  [ 7392/ 8083]
loss:        0.000416  [ 7552/ 8083]
loss:        0.000499  [ 7712/ 8083]
loss:        0.000469  [ 7872/ 8083]
loss:        0.000443  [ 8032/ 8083]

Avg validation loss:        0.000506
Learning rate: [0.0001]
EarlyStopping counter: 1 out of 20


Epoch 27
-------------------------------
loss:        0.000596  [   32/ 8083]
loss:        0.000498  [  192/ 8083]
loss:        0.000490  [  352/ 8083]
loss:        0.000518  [  512/ 8083]
loss:        0.000527  [  672/ 8083]
loss:        0.000549  [  832/ 8083]
loss:        0.000459  [  992/ 8083]
loss:        0.000527  [ 1152/ 8083]
loss:        0.000567  [ 1312/ 8083]
loss:        0.000497  [ 1472/ 8083]
loss:        0.000507  [ 1632/ 8083]
loss:        0.000535  [ 1792/ 8083]
loss:        0.000534  [ 1952/ 8083]
loss:        0.000447  [ 2112/ 8083]
loss:        0.000494  [ 2272/ 8083]
loss:        0.000482  [ 2432/ 8083]
loss:        0.000427  [ 2592/ 8083]
loss:        0.000458  [ 2752/ 8083]
loss:        0.000482  [ 2912/ 8083]
loss:        0.000501  [ 3072/ 8083]
loss:        0.000442  [ 3232/ 8083]
loss:        0.000443  [ 3392/ 8083]
loss:        0.000423  [ 3552/ 8083]
loss:        0.000406  [ 3712/ 8083]
loss:        0.000383  [ 3872/ 8083]
loss:        0.000443  [ 4032/ 8083]
loss:        0.000453  [ 4192/ 8083]
loss:        0.000552  [ 4352/ 8083]
loss:        0.000438  [ 4512/ 8083]
loss:        0.000414  [ 4672/ 8083]
loss:        0.000529  [ 4832/ 8083]
loss:        0.000431  [ 4992/ 8083]
loss:        0.000449  [ 5152/ 8083]
loss:        0.000522  [ 5312/ 8083]
loss:        0.000526  [ 5472/ 8083]
loss:        0.000387  [ 5632/ 8083]
loss:        0.000546  [ 5792/ 8083]
loss:        0.000562  [ 5952/ 8083]
loss:        0.000425  [ 6112/ 8083]
loss:        0.000552  [ 6272/ 8083]
loss:        0.000506  [ 6432/ 8083]
loss:        0.000533  [ 6592/ 8083]
loss:        0.000555  [ 6752/ 8083]
loss:        0.000592  [ 6912/ 8083]
loss:        0.000422  [ 7072/ 8083]
loss:        0.000416  [ 7232/ 8083]
loss:        0.000487  [ 7392/ 8083]
loss:        0.000477  [ 7552/ 8083]
loss:        0.000485  [ 7712/ 8083]
loss:        0.000455  [ 7872/ 8083]
loss:        0.000623  [ 8032/ 8083]

Avg validation loss:        0.000501
Learning rate: [0.0001]
Validation loss decreased (0.000501 --&gt; 0.000501).  Saving model ...


Epoch 28
-------------------------------
loss:        0.000419  [   32/ 8083]
loss:        0.000552  [  192/ 8083]
loss:        0.000510  [  352/ 8083]
loss:        0.000601  [  512/ 8083]
loss:        0.000547  [  672/ 8083]
loss:        0.000640  [  832/ 8083]
loss:        0.000538  [  992/ 8083]
loss:        0.000507  [ 1152/ 8083]
loss:        0.000513  [ 1312/ 8083]
loss:        0.000562  [ 1472/ 8083]
loss:        0.000605  [ 1632/ 8083]
loss:        0.000583  [ 1792/ 8083]
loss:        0.000408  [ 1952/ 8083]
loss:        0.000479  [ 2112/ 8083]
loss:        0.000387  [ 2272/ 8083]
loss:        0.000536  [ 2432/ 8083]
loss:        0.000600  [ 2592/ 8083]
loss:        0.000390  [ 2752/ 8083]
loss:        0.000522  [ 2912/ 8083]
loss:        0.000423  [ 3072/ 8083]
loss:        0.000463  [ 3232/ 8083]
loss:        0.000491  [ 3392/ 8083]
loss:        0.000574  [ 3552/ 8083]
loss:        0.000448  [ 3712/ 8083]
loss:        0.000436  [ 3872/ 8083]
loss:        0.000569  [ 4032/ 8083]
loss:        0.000469  [ 4192/ 8083]
loss:        0.000418  [ 4352/ 8083]
loss:        0.000492  [ 4512/ 8083]
loss:        0.000528  [ 4672/ 8083]
loss:        0.000503  [ 4832/ 8083]
loss:        0.000496  [ 4992/ 8083]
loss:        0.000588  [ 5152/ 8083]
loss:        0.000442  [ 5312/ 8083]
loss:        0.000501  [ 5472/ 8083]
loss:        0.000489  [ 5632/ 8083]
loss:        0.000480  [ 5792/ 8083]
loss:        0.000513  [ 5952/ 8083]
loss:        0.000481  [ 6112/ 8083]
loss:        0.000430  [ 6272/ 8083]
loss:        0.000539  [ 6432/ 8083]
loss:        0.000480  [ 6592/ 8083]
loss:        0.000545  [ 6752/ 8083]
loss:        0.000551  [ 6912/ 8083]
loss:        0.000443  [ 7072/ 8083]
loss:        0.000479  [ 7232/ 8083]
loss:        0.000440  [ 7392/ 8083]
loss:        0.000447  [ 7552/ 8083]
loss:        0.000533  [ 7712/ 8083]
loss:        0.000543  [ 7872/ 8083]
loss:        0.000443  [ 8032/ 8083]

Avg validation loss:        0.000502
Learning rate: [0.0001]
EarlyStopping counter: 1 out of 20


Epoch 29
-------------------------------
loss:        0.000421  [   32/ 8083]
loss:        0.000490  [  192/ 8083]
loss:        0.000516  [  352/ 8083]
loss:        0.000493  [  512/ 8083]
loss:        0.000518  [  672/ 8083]
loss:        0.000418  [  832/ 8083]
loss:        0.000470  [  992/ 8083]
loss:        0.000502  [ 1152/ 8083]
loss:        0.000458  [ 1312/ 8083]
loss:        0.000440  [ 1472/ 8083]
loss:        0.000547  [ 1632/ 8083]
loss:        0.000528  [ 1792/ 8083]
loss:        0.000555  [ 1952/ 8083]
loss:        0.000478  [ 2112/ 8083]
loss:        0.000518  [ 2272/ 8083]
loss:        0.000404  [ 2432/ 8083]
loss:        0.000540  [ 2592/ 8083]
loss:        0.000522  [ 2752/ 8083]
loss:        0.000542  [ 2912/ 8083]
loss:        0.000523  [ 3072/ 8083]
loss:        0.000542  [ 3232/ 8083]
loss:        0.000485  [ 3392/ 8083]
loss:        0.000508  [ 3552/ 8083]
loss:        0.000490  [ 3712/ 8083]
loss:        0.000677  [ 3872/ 8083]
loss:        0.000507  [ 4032/ 8083]
loss:        0.000512  [ 4192/ 8083]
loss:        0.000534  [ 4352/ 8083]
loss:        0.000598  [ 4512/ 8083]
loss:        0.000385  [ 4672/ 8083]
loss:        0.000492  [ 4832/ 8083]
loss:        0.000519  [ 4992/ 8083]
loss:        0.000434  [ 5152/ 8083]
loss:        0.000608  [ 5312/ 8083]
loss:        0.000448  [ 5472/ 8083]
loss:        0.000580  [ 5632/ 8083]
loss:        0.000499  [ 5792/ 8083]
loss:        0.000394  [ 5952/ 8083]
loss:        0.000473  [ 6112/ 8083]
loss:        0.000498  [ 6272/ 8083]
loss:        0.000494  [ 6432/ 8083]
loss:        0.000493  [ 6592/ 8083]
loss:        0.000551  [ 6752/ 8083]
loss:        0.000472  [ 6912/ 8083]
loss:        0.000462  [ 7072/ 8083]
loss:        0.000566  [ 7232/ 8083]
loss:        0.000469  [ 7392/ 8083]
loss:        0.000463  [ 7552/ 8083]
loss:        0.000431  [ 7712/ 8083]
loss:        0.000561  [ 7872/ 8083]
loss:        0.000459  [ 8032/ 8083]

Avg validation loss:        0.000501
Learning rate: [0.0001]
EarlyStopping counter: 2 out of 20


Epoch 30
-------------------------------
loss:        0.000487  [   32/ 8083]
loss:        0.000511  [  192/ 8083]
loss:        0.000483  [  352/ 8083]
loss:        0.000465  [  512/ 8083]
loss:        0.000471  [  672/ 8083]
loss:        0.000494  [  832/ 8083]
loss:        0.000454  [  992/ 8083]
loss:        0.000468  [ 1152/ 8083]
loss:        0.000626  [ 1312/ 8083]
loss:        0.000490  [ 1472/ 8083]
loss:        0.000517  [ 1632/ 8083]
loss:        0.000510  [ 1792/ 8083]
loss:        0.000473  [ 1952/ 8083]
loss:        0.000489  [ 2112/ 8083]
loss:        0.000438  [ 2272/ 8083]
loss:        0.000479  [ 2432/ 8083]
loss:        0.000394  [ 2592/ 8083]
loss:        0.000546  [ 2752/ 8083]
loss:        0.000390  [ 2912/ 8083]
loss:        0.000501  [ 3072/ 8083]
loss:        0.000452  [ 3232/ 8083]
loss:        0.000559  [ 3392/ 8083]
loss:        0.000426  [ 3552/ 8083]
loss:        0.000393  [ 3712/ 8083]
loss:        0.000483  [ 3872/ 8083]
loss:        0.000564  [ 4032/ 8083]
loss:        0.000525  [ 4192/ 8083]
loss:        0.000501  [ 4352/ 8083]
loss:        0.000507  [ 4512/ 8083]
loss:        0.000503  [ 4672/ 8083]
loss:        0.000515  [ 4832/ 8083]
loss:        0.000504  [ 4992/ 8083]
loss:        0.000553  [ 5152/ 8083]
loss:        0.000478  [ 5312/ 8083]
loss:        0.000497  [ 5472/ 8083]
loss:        0.000469  [ 5632/ 8083]
loss:        0.000495  [ 5792/ 8083]
loss:        0.000537  [ 5952/ 8083]
loss:        0.000459  [ 6112/ 8083]
loss:        0.000478  [ 6272/ 8083]
loss:        0.000398  [ 6432/ 8083]
loss:        0.000546  [ 6592/ 8083]
loss:        0.000466  [ 6752/ 8083]
loss:        0.000532  [ 6912/ 8083]
loss:        0.000512  [ 7072/ 8083]
loss:        0.000454  [ 7232/ 8083]
loss:        0.000435  [ 7392/ 8083]
loss:        0.000540  [ 7552/ 8083]
loss:        0.000495  [ 7712/ 8083]
loss:        0.000540  [ 7872/ 8083]
loss:        0.000515  [ 8032/ 8083]

Avg validation loss:        0.000500
Learning rate: [0.0001]
Validation loss decreased (0.000501 --&gt; 0.000500).  Saving model ...


Epoch 31
-------------------------------
loss:        0.000470  [   32/ 8083]
loss:        0.000546  [  192/ 8083]
loss:        0.000557  [  352/ 8083]
loss:        0.000527  [  512/ 8083]
loss:        0.000589  [  672/ 8083]
loss:        0.000452  [  832/ 8083]
loss:        0.000511  [  992/ 8083]
loss:        0.000565  [ 1152/ 8083]
loss:        0.000472  [ 1312/ 8083]
loss:        0.000523  [ 1472/ 8083]
loss:        0.000463  [ 1632/ 8083]
loss:        0.000394  [ 1792/ 8083]
loss:        0.000469  [ 1952/ 8083]
loss:        0.000519  [ 2112/ 8083]
loss:        0.000386  [ 2272/ 8083]
loss:        0.000393  [ 2432/ 8083]
loss:        0.000464  [ 2592/ 8083]
loss:        0.000409  [ 2752/ 8083]
loss:        0.000445  [ 2912/ 8083]
loss:        0.000463  [ 3072/ 8083]
loss:        0.000503  [ 3232/ 8083]
loss:        0.000510  [ 3392/ 8083]
loss:        0.000527  [ 3552/ 8083]
loss:        0.000531  [ 3712/ 8083]
loss:        0.000449  [ 3872/ 8083]
loss:        0.000621  [ 4032/ 8083]
loss:        0.000460  [ 4192/ 8083]
loss:        0.000371  [ 4352/ 8083]
loss:        0.000573  [ 4512/ 8083]
loss:        0.000590  [ 4672/ 8083]
loss:        0.000449  [ 4832/ 8083]
loss:        0.000522  [ 4992/ 8083]
loss:        0.000584  [ 5152/ 8083]
loss:        0.000622  [ 5312/ 8083]
loss:        0.000504  [ 5472/ 8083]
loss:        0.000477  [ 5632/ 8083]
loss:        0.000484  [ 5792/ 8083]
loss:        0.000566  [ 5952/ 8083]
loss:        0.000488  [ 6112/ 8083]
loss:        0.000444  [ 6272/ 8083]
loss:        0.000560  [ 6432/ 8083]
loss:        0.000478  [ 6592/ 8083]
loss:        0.000422  [ 6752/ 8083]
loss:        0.000519  [ 6912/ 8083]
loss:        0.000472  [ 7072/ 8083]
loss:        0.000396  [ 7232/ 8083]
loss:        0.000553  [ 7392/ 8083]
loss:        0.000468  [ 7552/ 8083]
loss:        0.000421  [ 7712/ 8083]
loss:        0.000551  [ 7872/ 8083]
loss:        0.000445  [ 8032/ 8083]

Avg validation loss:        0.000502
Learning rate: [0.0001]
EarlyStopping counter: 1 out of 20


Epoch 32
-------------------------------
loss:        0.000510  [   32/ 8083]
loss:        0.000469  [  192/ 8083]
loss:        0.000485  [  352/ 8083]
loss:        0.000532  [  512/ 8083]
loss:        0.000480  [  672/ 8083]
loss:        0.000496  [  832/ 8083]
loss:        0.000527  [  992/ 8083]
loss:        0.000502  [ 1152/ 8083]
loss:        0.000551  [ 1312/ 8083]
loss:        0.000582  [ 1472/ 8083]
loss:        0.000563  [ 1632/ 8083]
loss:        0.000466  [ 1792/ 8083]
loss:        0.000466  [ 1952/ 8083]
loss:        0.000422  [ 2112/ 8083]
loss:        0.000564  [ 2272/ 8083]
loss:        0.000480  [ 2432/ 8083]
loss:        0.000563  [ 2592/ 8083]
loss:        0.000502  [ 2752/ 8083]
loss:        0.000537  [ 2912/ 8083]
loss:        0.000506  [ 3072/ 8083]
loss:        0.000486  [ 3232/ 8083]
loss:        0.000539  [ 3392/ 8083]
loss:        0.000507  [ 3552/ 8083]
loss:        0.000441  [ 3712/ 8083]
loss:        0.000504  [ 3872/ 8083]
loss:        0.000462  [ 4032/ 8083]
loss:        0.000522  [ 4192/ 8083]
loss:        0.000456  [ 4352/ 8083]
loss:        0.000425  [ 4512/ 8083]
loss:        0.000502  [ 4672/ 8083]
loss:        0.000452  [ 4832/ 8083]
loss:        0.000392  [ 4992/ 8083]
loss:        0.000401  [ 5152/ 8083]
loss:        0.000456  [ 5312/ 8083]
loss:        0.000480  [ 5472/ 8083]
loss:        0.000497  [ 5632/ 8083]
loss:        0.000468  [ 5792/ 8083]
loss:        0.000542  [ 5952/ 8083]
loss:        0.000434  [ 6112/ 8083]
loss:        0.000513  [ 6272/ 8083]
loss:        0.000537  [ 6432/ 8083]
loss:        0.000474  [ 6592/ 8083]
loss:        0.000450  [ 6752/ 8083]
loss:        0.000487  [ 6912/ 8083]
loss:        0.000522  [ 7072/ 8083]
loss:        0.000506  [ 7232/ 8083]
loss:        0.000486  [ 7392/ 8083]
loss:        0.000554  [ 7552/ 8083]
loss:        0.000521  [ 7712/ 8083]
loss:        0.000428  [ 7872/ 8083]
loss:        0.000514  [ 8032/ 8083]

Avg validation loss:        0.000502
Learning rate: [0.0001]
EarlyStopping counter: 2 out of 20


Epoch 33
-------------------------------
loss:        0.000541  [   32/ 8083]
loss:        0.000679  [  192/ 8083]
loss:        0.000508  [  352/ 8083]
loss:        0.000474  [  512/ 8083]
loss:        0.000498  [  672/ 8083]
loss:        0.000464  [  832/ 8083]
loss:        0.000549  [  992/ 8083]
loss:        0.000544  [ 1152/ 8083]
loss:        0.000452  [ 1312/ 8083]
loss:        0.000532  [ 1472/ 8083]
loss:        0.000430  [ 1632/ 8083]
loss:        0.000523  [ 1792/ 8083]
loss:        0.000487  [ 1952/ 8083]
loss:        0.000540  [ 2112/ 8083]
loss:        0.000437  [ 2272/ 8083]
loss:        0.000593  [ 2432/ 8083]
loss:        0.000483  [ 2592/ 8083]
loss:        0.000512  [ 2752/ 8083]
loss:        0.000442  [ 2912/ 8083]
loss:        0.000558  [ 3072/ 8083]
loss:        0.000447  [ 3232/ 8083]
loss:        0.000436  [ 3392/ 8083]
loss:        0.000433  [ 3552/ 8083]
loss:        0.000509  [ 3712/ 8083]
loss:        0.000511  [ 3872/ 8083]
loss:        0.000433  [ 4032/ 8083]
loss:        0.000543  [ 4192/ 8083]
loss:        0.000528  [ 4352/ 8083]
loss:        0.000557  [ 4512/ 8083]
loss:        0.000656  [ 4672/ 8083]
loss:        0.000546  [ 4832/ 8083]
loss:        0.000535  [ 4992/ 8083]
loss:        0.000504  [ 5152/ 8083]
loss:        0.000503  [ 5312/ 8083]
loss:        0.000514  [ 5472/ 8083]
loss:        0.000419  [ 5632/ 8083]
loss:        0.000458  [ 5792/ 8083]
loss:        0.000394  [ 5952/ 8083]
loss:        0.000459  [ 6112/ 8083]
loss:        0.000515  [ 6272/ 8083]
loss:        0.000497  [ 6432/ 8083]
loss:        0.000471  [ 6592/ 8083]
loss:        0.000486  [ 6752/ 8083]
loss:        0.000537  [ 6912/ 8083]
loss:        0.000373  [ 7072/ 8083]
loss:        0.000397  [ 7232/ 8083]
loss:        0.000442  [ 7392/ 8083]
loss:        0.000584  [ 7552/ 8083]
loss:        0.000599  [ 7712/ 8083]
loss:        0.000471  [ 7872/ 8083]
loss:        0.000582  [ 8032/ 8083]

Avg validation loss:        0.000502
Learning rate: [0.0001]
EarlyStopping counter: 3 out of 20


Epoch 34
-------------------------------
loss:        0.000439  [   32/ 8083]
loss:        0.000479  [  192/ 8083]
loss:        0.000538  [  352/ 8083]
loss:        0.000519  [  512/ 8083]
loss:        0.000468  [  672/ 8083]
loss:        0.000505  [  832/ 8083]
loss:        0.000497  [  992/ 8083]
loss:        0.000482  [ 1152/ 8083]
loss:        0.000589  [ 1312/ 8083]
loss:        0.000566  [ 1472/ 8083]
loss:        0.000444  [ 1632/ 8083]
loss:        0.000588  [ 1792/ 8083]
loss:        0.000519  [ 1952/ 8083]
loss:        0.000500  [ 2112/ 8083]
loss:        0.000460  [ 2272/ 8083]
loss:        0.000551  [ 2432/ 8083]
loss:        0.000468  [ 2592/ 8083]
loss:        0.000508  [ 2752/ 8083]
loss:        0.000424  [ 2912/ 8083]
loss:        0.000473  [ 3072/ 8083]
loss:        0.000512  [ 3232/ 8083]
loss:        0.000537  [ 3392/ 8083]
loss:        0.000443  [ 3552/ 8083]
loss:        0.000427  [ 3712/ 8083]
loss:        0.000627  [ 3872/ 8083]
loss:        0.000440  [ 4032/ 8083]
loss:        0.000464  [ 4192/ 8083]
loss:        0.000507  [ 4352/ 8083]
loss:        0.000378  [ 4512/ 8083]
loss:        0.000449  [ 4672/ 8083]
loss:        0.000551  [ 4832/ 8083]
loss:        0.000516  [ 4992/ 8083]
loss:        0.000514  [ 5152/ 8083]
loss:        0.000509  [ 5312/ 8083]
loss:        0.000359  [ 5472/ 8083]
loss:        0.000513  [ 5632/ 8083]
loss:        0.000443  [ 5792/ 8083]
loss:        0.000607  [ 5952/ 8083]
loss:        0.000393  [ 6112/ 8083]
loss:        0.000536  [ 6272/ 8083]
loss:        0.000447  [ 6432/ 8083]
loss:        0.000501  [ 6592/ 8083]
loss:        0.000526  [ 6752/ 8083]
loss:        0.000631  [ 6912/ 8083]
loss:        0.000607  [ 7072/ 8083]
loss:        0.000503  [ 7232/ 8083]
loss:        0.000428  [ 7392/ 8083]
loss:        0.000518  [ 7552/ 8083]
loss:        0.000453  [ 7712/ 8083]
loss:        0.000496  [ 7872/ 8083]
loss:        0.000475  [ 8032/ 8083]

Avg validation loss:        0.000502
Learning rate: [0.0001]
EarlyStopping counter: 4 out of 20


Epoch 35
-------------------------------
loss:        0.000385  [   32/ 8083]
loss:        0.000538  [  192/ 8083]
loss:        0.000432  [  352/ 8083]
loss:        0.000489  [  512/ 8083]
loss:        0.000589  [  672/ 8083]
loss:        0.000468  [  832/ 8083]
loss:        0.000489  [  992/ 8083]
loss:        0.000426  [ 1152/ 8083]
loss:        0.000485  [ 1312/ 8083]
loss:        0.000530  [ 1472/ 8083]
loss:        0.000493  [ 1632/ 8083]
loss:        0.000603  [ 1792/ 8083]
loss:        0.000446  [ 1952/ 8083]
loss:        0.000580  [ 2112/ 8083]
loss:        0.000484  [ 2272/ 8083]
loss:        0.000492  [ 2432/ 8083]
loss:        0.000597  [ 2592/ 8083]
loss:        0.000586  [ 2752/ 8083]
loss:        0.000542  [ 2912/ 8083]
loss:        0.000414  [ 3072/ 8083]
loss:        0.000544  [ 3232/ 8083]
loss:        0.000473  [ 3392/ 8083]
loss:        0.000491  [ 3552/ 8083]
loss:        0.000487  [ 3712/ 8083]
loss:        0.000428  [ 3872/ 8083]
loss:        0.000616  [ 4032/ 8083]
loss:        0.000533  [ 4192/ 8083]
loss:        0.000477  [ 4352/ 8083]
loss:        0.000476  [ 4512/ 8083]
loss:        0.000492  [ 4672/ 8083]
loss:        0.000512  [ 4832/ 8083]
loss:        0.000458  [ 4992/ 8083]
loss:        0.000531  [ 5152/ 8083]
loss:        0.000531  [ 5312/ 8083]
loss:        0.000456  [ 5472/ 8083]
loss:        0.000445  [ 5632/ 8083]
loss:        0.000466  [ 5792/ 8083]
loss:        0.000512  [ 5952/ 8083]
loss:        0.000552  [ 6112/ 8083]
loss:        0.000415  [ 6272/ 8083]
loss:        0.000467  [ 6432/ 8083]
loss:        0.000567  [ 6592/ 8083]
loss:        0.000523  [ 6752/ 8083]
loss:        0.000506  [ 6912/ 8083]
loss:        0.000447  [ 7072/ 8083]
loss:        0.000502  [ 7232/ 8083]
loss:        0.000534  [ 7392/ 8083]
loss:        0.000432  [ 7552/ 8083]
loss:        0.000578  [ 7712/ 8083]
loss:        0.000467  [ 7872/ 8083]
loss:        0.000628  [ 8032/ 8083]

Avg validation loss:        0.000500
Learning rate: [0.0001]
Validation loss decreased (0.000500 --&gt; 0.000500).  Saving model ...


Epoch 36
-------------------------------
loss:        0.000504  [   32/ 8083]
loss:        0.000439  [  192/ 8083]
loss:        0.000546  [  352/ 8083]
loss:        0.000497  [  512/ 8083]
loss:        0.000376  [  672/ 8083]
loss:        0.000528  [  832/ 8083]
loss:        0.000463  [  992/ 8083]
loss:        0.000541  [ 1152/ 8083]
loss:        0.000487  [ 1312/ 8083]
loss:        0.000558  [ 1472/ 8083]
loss:        0.000510  [ 1632/ 8083]
loss:        0.000503  [ 1792/ 8083]
loss:        0.000482  [ 1952/ 8083]
loss:        0.000491  [ 2112/ 8083]
loss:        0.000479  [ 2272/ 8083]
loss:        0.000537  [ 2432/ 8083]
loss:        0.000453  [ 2592/ 8083]
loss:        0.000530  [ 2752/ 8083]
loss:        0.000518  [ 2912/ 8083]
loss:        0.000533  [ 3072/ 8083]
loss:        0.000525  [ 3232/ 8083]
loss:        0.000485  [ 3392/ 8083]
loss:        0.000496  [ 3552/ 8083]
loss:        0.000520  [ 3712/ 8083]
loss:        0.000582  [ 3872/ 8083]
loss:        0.000481  [ 4032/ 8083]
loss:        0.000538  [ 4192/ 8083]
loss:        0.000412  [ 4352/ 8083]
loss:        0.000451  [ 4512/ 8083]
loss:        0.000555  [ 4672/ 8083]
loss:        0.000527  [ 4832/ 8083]
loss:        0.000620  [ 4992/ 8083]
loss:        0.000403  [ 5152/ 8083]
loss:        0.000460  [ 5312/ 8083]
loss:        0.000484  [ 5472/ 8083]
loss:        0.000421  [ 5632/ 8083]
loss:        0.000466  [ 5792/ 8083]
loss:        0.000531  [ 5952/ 8083]
loss:        0.000485  [ 6112/ 8083]
loss:        0.000463  [ 6272/ 8083]
loss:        0.000446  [ 6432/ 8083]
loss:        0.000466  [ 6592/ 8083]
loss:        0.000519  [ 6752/ 8083]
loss:        0.000479  [ 6912/ 8083]
loss:        0.000508  [ 7072/ 8083]
loss:        0.000473  [ 7232/ 8083]
loss:        0.000490  [ 7392/ 8083]
loss:        0.000464  [ 7552/ 8083]
loss:        0.000454  [ 7712/ 8083]
loss:        0.000491  [ 7872/ 8083]
loss:        0.000384  [ 8032/ 8083]

Avg validation loss:        0.000503
Learning rate: [0.0001]
EarlyStopping counter: 1 out of 20


Epoch 37
-------------------------------
loss:        0.000484  [   32/ 8083]
loss:        0.000521  [  192/ 8083]
loss:        0.000466  [  352/ 8083]
loss:        0.000493  [  512/ 8083]
loss:        0.000495  [  672/ 8083]
loss:        0.000473  [  832/ 8083]
loss:        0.000592  [  992/ 8083]
loss:        0.000489  [ 1152/ 8083]
loss:        0.000549  [ 1312/ 8083]
loss:        0.000507  [ 1472/ 8083]
loss:        0.000519  [ 1632/ 8083]
loss:        0.000433  [ 1792/ 8083]
loss:        0.000498  [ 1952/ 8083]
loss:        0.000547  [ 2112/ 8083]
loss:        0.000553  [ 2272/ 8083]
loss:        0.000464  [ 2432/ 8083]
loss:        0.000480  [ 2592/ 8083]
loss:        0.000599  [ 2752/ 8083]
loss:        0.000495  [ 2912/ 8083]
loss:        0.000547  [ 3072/ 8083]
loss:        0.000496  [ 3232/ 8083]
loss:        0.000526  [ 3392/ 8083]
loss:        0.000464  [ 3552/ 8083]
loss:        0.000521  [ 3712/ 8083]
loss:        0.000495  [ 3872/ 8083]
loss:        0.000535  [ 4032/ 8083]
loss:        0.000484  [ 4192/ 8083]
loss:        0.000463  [ 4352/ 8083]
loss:        0.000499  [ 4512/ 8083]
loss:        0.000580  [ 4672/ 8083]
loss:        0.000452  [ 4832/ 8083]
loss:        0.000451  [ 4992/ 8083]
loss:        0.000680  [ 5152/ 8083]
loss:        0.000477  [ 5312/ 8083]
loss:        0.000568  [ 5472/ 8083]
loss:        0.000552  [ 5632/ 8083]
loss:        0.000444  [ 5792/ 8083]
loss:        0.000446  [ 5952/ 8083]
loss:        0.000478  [ 6112/ 8083]
loss:        0.000390  [ 6272/ 8083]
loss:        0.000475  [ 6432/ 8083]
loss:        0.000496  [ 6592/ 8083]
loss:        0.000561  [ 6752/ 8083]
loss:        0.000462  [ 6912/ 8083]
loss:        0.000621  [ 7072/ 8083]
loss:        0.000505  [ 7232/ 8083]
loss:        0.000483  [ 7392/ 8083]
loss:        0.000485  [ 7552/ 8083]
loss:        0.000465  [ 7712/ 8083]
loss:        0.000478  [ 7872/ 8083]
loss:        0.000327  [ 8032/ 8083]

Avg validation loss:        0.000503
Learning rate: [0.0001]
EarlyStopping counter: 2 out of 20


Epoch 38
-------------------------------
loss:        0.000465  [   32/ 8083]
loss:        0.000555  [  192/ 8083]
loss:        0.000520  [  352/ 8083]
loss:        0.000462  [  512/ 8083]
loss:        0.000445  [  672/ 8083]
loss:        0.000381  [  832/ 8083]
loss:        0.000432  [  992/ 8083]
loss:        0.000481  [ 1152/ 8083]
loss:        0.000578  [ 1312/ 8083]
loss:        0.000580  [ 1472/ 8083]
loss:        0.000440  [ 1632/ 8083]
loss:        0.000515  [ 1792/ 8083]
loss:        0.000479  [ 1952/ 8083]
loss:        0.000472  [ 2112/ 8083]
loss:        0.000483  [ 2272/ 8083]
loss:        0.000530  [ 2432/ 8083]
loss:        0.000419  [ 2592/ 8083]
loss:        0.000534  [ 2752/ 8083]
loss:        0.000485  [ 2912/ 8083]
loss:        0.000535  [ 3072/ 8083]
loss:        0.000454  [ 3232/ 8083]
loss:        0.000441  [ 3392/ 8083]
loss:        0.000575  [ 3552/ 8083]
loss:        0.000544  [ 3712/ 8083]
loss:        0.000535  [ 3872/ 8083]
loss:        0.000409  [ 4032/ 8083]
loss:        0.000483  [ 4192/ 8083]
loss:        0.000502  [ 4352/ 8083]
loss:        0.000497  [ 4512/ 8083]
loss:        0.000483  [ 4672/ 8083]
loss:        0.000439  [ 4832/ 8083]
loss:        0.000530  [ 4992/ 8083]
loss:        0.000623  [ 5152/ 8083]
loss:        0.000512  [ 5312/ 8083]
loss:        0.000554  [ 5472/ 8083]
loss:        0.000437  [ 5632/ 8083]
loss:        0.000463  [ 5792/ 8083]
loss:        0.000505  [ 5952/ 8083]
loss:        0.000455  [ 6112/ 8083]
loss:        0.000452  [ 6272/ 8083]
loss:        0.000599  [ 6432/ 8083]
loss:        0.000491  [ 6592/ 8083]
loss:        0.000492  [ 6752/ 8083]
loss:        0.000459  [ 6912/ 8083]
loss:        0.000472  [ 7072/ 8083]
loss:        0.000510  [ 7232/ 8083]
loss:        0.000443  [ 7392/ 8083]
loss:        0.000488  [ 7552/ 8083]
loss:        0.000525  [ 7712/ 8083]
loss:        0.000571  [ 7872/ 8083]
loss:        0.000495  [ 8032/ 8083]

Avg validation loss:        0.000502
Learning rate: [0.0001]
EarlyStopping counter: 3 out of 20


Epoch 39
-------------------------------
loss:        0.000488  [   32/ 8083]
loss:        0.000507  [  192/ 8083]
loss:        0.000375  [  352/ 8083]
loss:        0.000587  [  512/ 8083]
loss:        0.000538  [  672/ 8083]
loss:        0.000479  [  832/ 8083]
loss:        0.000590  [  992/ 8083]
loss:        0.000441  [ 1152/ 8083]
loss:        0.000481  [ 1312/ 8083]
loss:        0.000467  [ 1472/ 8083]
loss:        0.000476  [ 1632/ 8083]
loss:        0.000401  [ 1792/ 8083]
loss:        0.000481  [ 1952/ 8083]
loss:        0.000530  [ 2112/ 8083]
loss:        0.000464  [ 2272/ 8083]
loss:        0.000509  [ 2432/ 8083]
loss:        0.000505  [ 2592/ 8083]
loss:        0.000545  [ 2752/ 8083]
loss:        0.000501  [ 2912/ 8083]
loss:        0.000535  [ 3072/ 8083]
loss:        0.000500  [ 3232/ 8083]
loss:        0.000486  [ 3392/ 8083]
loss:        0.000390  [ 3552/ 8083]
loss:        0.000542  [ 3712/ 8083]
loss:        0.000568  [ 3872/ 8083]
loss:        0.000575  [ 4032/ 8083]
loss:        0.000457  [ 4192/ 8083]
loss:        0.000451  [ 4352/ 8083]
loss:        0.000532  [ 4512/ 8083]
loss:        0.000580  [ 4672/ 8083]
loss:        0.000563  [ 4832/ 8083]
loss:        0.000494  [ 4992/ 8083]
loss:        0.000512  [ 5152/ 8083]
loss:        0.000606  [ 5312/ 8083]
loss:        0.000514  [ 5472/ 8083]
loss:        0.000466  [ 5632/ 8083]
loss:        0.000437  [ 5792/ 8083]
loss:        0.000441  [ 5952/ 8083]
loss:        0.000637  [ 6112/ 8083]
loss:        0.000435  [ 6272/ 8083]
loss:        0.000364  [ 6432/ 8083]
loss:        0.000543  [ 6592/ 8083]
loss:        0.000619  [ 6752/ 8083]
loss:        0.000484  [ 6912/ 8083]
loss:        0.000513  [ 7072/ 8083]
loss:        0.000590  [ 7232/ 8083]
loss:        0.000468  [ 7392/ 8083]
loss:        0.000492  [ 7552/ 8083]
loss:        0.000551  [ 7712/ 8083]
loss:        0.000431  [ 7872/ 8083]
loss:        0.000502  [ 8032/ 8083]

Avg validation loss:        0.000501
Learning rate: [0.0001]
EarlyStopping counter: 4 out of 20


Epoch 40
-------------------------------
loss:        0.000517  [   32/ 8083]
loss:        0.000537  [  192/ 8083]
loss:        0.000438  [  352/ 8083]
loss:        0.000525  [  512/ 8083]
loss:        0.000415  [  672/ 8083]
loss:        0.000468  [  832/ 8083]
loss:        0.000355  [  992/ 8083]
loss:        0.000451  [ 1152/ 8083]
loss:        0.000423  [ 1312/ 8083]
loss:        0.000488  [ 1472/ 8083]
loss:        0.000477  [ 1632/ 8083]
loss:        0.000528  [ 1792/ 8083]
loss:        0.000443  [ 1952/ 8083]
loss:        0.000496  [ 2112/ 8083]
loss:        0.000420  [ 2272/ 8083]
loss:        0.000604  [ 2432/ 8083]
loss:        0.000479  [ 2592/ 8083]
loss:        0.000517  [ 2752/ 8083]
loss:        0.000465  [ 2912/ 8083]
loss:        0.000415  [ 3072/ 8083]
loss:        0.000509  [ 3232/ 8083]
loss:        0.000389  [ 3392/ 8083]
loss:        0.000494  [ 3552/ 8083]
loss:        0.000546  [ 3712/ 8083]
loss:        0.000528  [ 3872/ 8083]
loss:        0.000465  [ 4032/ 8083]
loss:        0.000528  [ 4192/ 8083]
loss:        0.000524  [ 4352/ 8083]
loss:        0.000540  [ 4512/ 8083]
loss:        0.000412  [ 4672/ 8083]
loss:        0.000453  [ 4832/ 8083]
loss:        0.000512  [ 4992/ 8083]
loss:        0.000510  [ 5152/ 8083]
loss:        0.000489  [ 5312/ 8083]
loss:        0.000547  [ 5472/ 8083]
loss:        0.000504  [ 5632/ 8083]
loss:        0.000474  [ 5792/ 8083]
loss:        0.000511  [ 5952/ 8083]
loss:        0.000430  [ 6112/ 8083]
loss:        0.000536  [ 6272/ 8083]
loss:        0.000434  [ 6432/ 8083]
loss:        0.000457  [ 6592/ 8083]
loss:        0.000548  [ 6752/ 8083]
loss:        0.000374  [ 6912/ 8083]
loss:        0.000408  [ 7072/ 8083]
loss:        0.000499  [ 7232/ 8083]
loss:        0.000438  [ 7392/ 8083]
loss:        0.000526  [ 7552/ 8083]
loss:        0.000483  [ 7712/ 8083]
loss:        0.000642  [ 7872/ 8083]
loss:        0.000510  [ 8032/ 8083]

Avg validation loss:        0.000503
Learning rate: [0.0001]
EarlyStopping counter: 5 out of 20


Epoch 41
-------------------------------
loss:        0.000549  [   32/ 8083]
loss:        0.000363  [  192/ 8083]
loss:        0.000471  [  352/ 8083]
loss:        0.000519  [  512/ 8083]
loss:        0.000539  [  672/ 8083]
loss:        0.000484  [  832/ 8083]
loss:        0.000507  [  992/ 8083]
loss:        0.000585  [ 1152/ 8083]
loss:        0.000363  [ 1312/ 8083]
loss:        0.000447  [ 1472/ 8083]
loss:        0.000447  [ 1632/ 8083]
loss:        0.000385  [ 1792/ 8083]
loss:        0.000399  [ 1952/ 8083]
loss:        0.000615  [ 2112/ 8083]
loss:        0.000524  [ 2272/ 8083]
loss:        0.000506  [ 2432/ 8083]
loss:        0.000539  [ 2592/ 8083]
loss:        0.000602  [ 2752/ 8083]
loss:        0.000481  [ 2912/ 8083]
loss:        0.000438  [ 3072/ 8083]
loss:        0.000473  [ 3232/ 8083]
loss:        0.000450  [ 3392/ 8083]
loss:        0.000573  [ 3552/ 8083]
loss:        0.000481  [ 3712/ 8083]
loss:        0.000509  [ 3872/ 8083]
loss:        0.000496  [ 4032/ 8083]
loss:        0.000506  [ 4192/ 8083]
loss:        0.000462  [ 4352/ 8083]
loss:        0.000546  [ 4512/ 8083]
loss:        0.000499  [ 4672/ 8083]
loss:        0.000585  [ 4832/ 8083]
loss:        0.000581  [ 4992/ 8083]
loss:        0.000473  [ 5152/ 8083]
loss:        0.000513  [ 5312/ 8083]
loss:        0.000495  [ 5472/ 8083]
loss:        0.000513  [ 5632/ 8083]
loss:        0.000500  [ 5792/ 8083]
loss:        0.000486  [ 5952/ 8083]
loss:        0.000584  [ 6112/ 8083]
loss:        0.000540  [ 6272/ 8083]
loss:        0.000489  [ 6432/ 8083]
loss:        0.000491  [ 6592/ 8083]
loss:        0.000506  [ 6752/ 8083]
loss:        0.000452  [ 6912/ 8083]
loss:        0.000468  [ 7072/ 8083]
loss:        0.000447  [ 7232/ 8083]
loss:        0.000562  [ 7392/ 8083]
loss:        0.000448  [ 7552/ 8083]
loss:        0.000606  [ 7712/ 8083]
loss:        0.000537  [ 7872/ 8083]
loss:        0.000472  [ 8032/ 8083]

Avg validation loss:        0.000501
Learning rate: [1e-05]
EarlyStopping counter: 6 out of 20


Epoch 42
-------------------------------
loss:        0.000510  [   32/ 8083]
loss:        0.000587  [  192/ 8083]
loss:        0.000484  [  352/ 8083]
loss:        0.000426  [  512/ 8083]
loss:        0.000469  [  672/ 8083]
loss:        0.000466  [  832/ 8083]
loss:        0.000535  [  992/ 8083]
loss:        0.000438  [ 1152/ 8083]
loss:        0.000492  [ 1312/ 8083]
loss:        0.000436  [ 1472/ 8083]
loss:        0.000585  [ 1632/ 8083]
loss:        0.000547  [ 1792/ 8083]
loss:        0.000497  [ 1952/ 8083]
loss:        0.000458  [ 2112/ 8083]
loss:        0.000526  [ 2272/ 8083]
loss:        0.000514  [ 2432/ 8083]
loss:        0.000521  [ 2592/ 8083]
loss:        0.000530  [ 2752/ 8083]
loss:        0.000415  [ 2912/ 8083]
loss:        0.000578  [ 3072/ 8083]
loss:        0.000567  [ 3232/ 8083]
loss:        0.000624  [ 3392/ 8083]
loss:        0.000453  [ 3552/ 8083]
loss:        0.000484  [ 3712/ 8083]
loss:        0.000475  [ 3872/ 8083]
loss:        0.000541  [ 4032/ 8083]
loss:        0.000524  [ 4192/ 8083]
loss:        0.000530  [ 4352/ 8083]
loss:        0.000437  [ 4512/ 8083]
loss:        0.000431  [ 4672/ 8083]
loss:        0.000609  [ 4832/ 8083]
loss:        0.000593  [ 4992/ 8083]
loss:        0.000526  [ 5152/ 8083]
loss:        0.000584  [ 5312/ 8083]
loss:        0.000474  [ 5472/ 8083]
loss:        0.000541  [ 5632/ 8083]
loss:        0.000560  [ 5792/ 8083]
loss:        0.000452  [ 5952/ 8083]
loss:        0.000506  [ 6112/ 8083]
loss:        0.000541  [ 6272/ 8083]
loss:        0.000493  [ 6432/ 8083]
loss:        0.000501  [ 6592/ 8083]
loss:        0.000401  [ 6752/ 8083]
loss:        0.000491  [ 6912/ 8083]
loss:        0.000420  [ 7072/ 8083]
loss:        0.000524  [ 7232/ 8083]
loss:        0.000532  [ 7392/ 8083]
loss:        0.000466  [ 7552/ 8083]
loss:        0.000499  [ 7712/ 8083]
loss:        0.000477  [ 7872/ 8083]
loss:        0.000506  [ 8032/ 8083]

Avg validation loss:        0.000501
Learning rate: [1e-05]
EarlyStopping counter: 7 out of 20


Epoch 43
-------------------------------
loss:        0.000408  [   32/ 8083]
loss:        0.000487  [  192/ 8083]
loss:        0.000474  [  352/ 8083]
loss:        0.000431  [  512/ 8083]
loss:        0.000592  [  672/ 8083]
loss:        0.000523  [  832/ 8083]
loss:        0.000595  [  992/ 8083]
loss:        0.000410  [ 1152/ 8083]
loss:        0.000478  [ 1312/ 8083]
loss:        0.000566  [ 1472/ 8083]
loss:        0.000531  [ 1632/ 8083]
loss:        0.000427  [ 1792/ 8083]
loss:        0.000530  [ 1952/ 8083]
loss:        0.000473  [ 2112/ 8083]
loss:        0.000530  [ 2272/ 8083]
loss:        0.000461  [ 2432/ 8083]
loss:        0.000503  [ 2592/ 8083]
loss:        0.000493  [ 2752/ 8083]
loss:        0.000505  [ 2912/ 8083]
loss:        0.000609  [ 3072/ 8083]
loss:        0.000519  [ 3232/ 8083]
loss:        0.000468  [ 3392/ 8083]
loss:        0.000399  [ 3552/ 8083]
loss:        0.000513  [ 3712/ 8083]
loss:        0.000567  [ 3872/ 8083]
loss:        0.000511  [ 4032/ 8083]
loss:        0.000519  [ 4192/ 8083]
loss:        0.000512  [ 4352/ 8083]
loss:        0.000477  [ 4512/ 8083]
loss:        0.000478  [ 4672/ 8083]
loss:        0.000480  [ 4832/ 8083]
loss:        0.000483  [ 4992/ 8083]
loss:        0.000279  [ 5152/ 8083]
loss:        0.000455  [ 5312/ 8083]
loss:        0.000501  [ 5472/ 8083]
loss:        0.000477  [ 5632/ 8083]
loss:        0.000528  [ 5792/ 8083]
loss:        0.000409  [ 5952/ 8083]
loss:        0.000511  [ 6112/ 8083]
loss:        0.000514  [ 6272/ 8083]
loss:        0.000470  [ 6432/ 8083]
loss:        0.000480  [ 6592/ 8083]
loss:        0.000545  [ 6752/ 8083]
loss:        0.000442  [ 6912/ 8083]
loss:        0.000452  [ 7072/ 8083]
loss:        0.000431  [ 7232/ 8083]
loss:        0.000563  [ 7392/ 8083]
loss:        0.000545  [ 7552/ 8083]
loss:        0.000524  [ 7712/ 8083]
loss:        0.000351  [ 7872/ 8083]
loss:        0.000492  [ 8032/ 8083]

Avg validation loss:        0.000502
Learning rate: [1e-05]
EarlyStopping counter: 8 out of 20


Epoch 44
-------------------------------
loss:        0.000386  [   32/ 8083]
loss:        0.000627  [  192/ 8083]
loss:        0.000525  [  352/ 8083]
loss:        0.000544  [  512/ 8083]
loss:        0.000515  [  672/ 8083]
loss:        0.000491  [  832/ 8083]
loss:        0.000537  [  992/ 8083]
loss:        0.000493  [ 1152/ 8083]
loss:        0.000429  [ 1312/ 8083]
loss:        0.000544  [ 1472/ 8083]
loss:        0.000447  [ 1632/ 8083]
loss:        0.000486  [ 1792/ 8083]
loss:        0.000492  [ 1952/ 8083]
loss:        0.000511  [ 2112/ 8083]
loss:        0.000522  [ 2272/ 8083]
loss:        0.000334  [ 2432/ 8083]
loss:        0.000443  [ 2592/ 8083]
loss:        0.000518  [ 2752/ 8083]
loss:        0.000500  [ 2912/ 8083]
loss:        0.000510  [ 3072/ 8083]
loss:        0.000391  [ 3232/ 8083]
loss:        0.000512  [ 3392/ 8083]
loss:        0.000527  [ 3552/ 8083]
loss:        0.000487  [ 3712/ 8083]
loss:        0.000515  [ 3872/ 8083]
loss:        0.000532  [ 4032/ 8083]
loss:        0.000475  [ 4192/ 8083]
loss:        0.000435  [ 4352/ 8083]
loss:        0.000485  [ 4512/ 8083]
loss:        0.000499  [ 4672/ 8083]
loss:        0.000495  [ 4832/ 8083]
loss:        0.000575  [ 4992/ 8083]
loss:        0.000533  [ 5152/ 8083]
loss:        0.000537  [ 5312/ 8083]
loss:        0.000423  [ 5472/ 8083]
loss:        0.000477  [ 5632/ 8083]
loss:        0.000480  [ 5792/ 8083]
loss:        0.000384  [ 5952/ 8083]
loss:        0.000528  [ 6112/ 8083]
loss:        0.000500  [ 6272/ 8083]
loss:        0.000421  [ 6432/ 8083]
loss:        0.000526  [ 6592/ 8083]
loss:        0.000353  [ 6752/ 8083]
loss:        0.000462  [ 6912/ 8083]
loss:        0.000623  [ 7072/ 8083]
loss:        0.000496  [ 7232/ 8083]
loss:        0.000436  [ 7392/ 8083]
loss:        0.000551  [ 7552/ 8083]
loss:        0.000518  [ 7712/ 8083]
loss:        0.000527  [ 7872/ 8083]
loss:        0.000498  [ 8032/ 8083]

Avg validation loss:        0.000502
Learning rate: [1e-05]
EarlyStopping counter: 9 out of 20


Epoch 45
-------------------------------
loss:        0.000386  [   32/ 8083]
loss:        0.000383  [  192/ 8083]
loss:        0.000529  [  352/ 8083]
loss:        0.000536  [  512/ 8083]
loss:        0.000547  [  672/ 8083]
loss:        0.000535  [  832/ 8083]
loss:        0.000390  [  992/ 8083]
loss:        0.000550  [ 1152/ 8083]
loss:        0.000519  [ 1312/ 8083]
loss:        0.000513  [ 1472/ 8083]
loss:        0.000431  [ 1632/ 8083]
loss:        0.000498  [ 1792/ 8083]
loss:        0.000450  [ 1952/ 8083]
loss:        0.000509  [ 2112/ 8083]
loss:        0.000554  [ 2272/ 8083]
loss:        0.000386  [ 2432/ 8083]
loss:        0.000427  [ 2592/ 8083]
loss:        0.000561  [ 2752/ 8083]
loss:        0.000488  [ 2912/ 8083]
loss:        0.000507  [ 3072/ 8083]
loss:        0.000491  [ 3232/ 8083]
loss:        0.000511  [ 3392/ 8083]
loss:        0.000547  [ 3552/ 8083]
loss:        0.000471  [ 3712/ 8083]
loss:        0.000419  [ 3872/ 8083]
loss:        0.000527  [ 4032/ 8083]
loss:        0.000550  [ 4192/ 8083]
loss:        0.000538  [ 4352/ 8083]
loss:        0.000468  [ 4512/ 8083]
loss:        0.000631  [ 4672/ 8083]
loss:        0.000456  [ 4832/ 8083]
loss:        0.000522  [ 4992/ 8083]
loss:        0.000544  [ 5152/ 8083]
loss:        0.000492  [ 5312/ 8083]
loss:        0.000440  [ 5472/ 8083]
loss:        0.000550  [ 5632/ 8083]
loss:        0.000512  [ 5792/ 8083]
loss:        0.000491  [ 5952/ 8083]
loss:        0.000638  [ 6112/ 8083]
loss:        0.000566  [ 6272/ 8083]
loss:        0.000546  [ 6432/ 8083]
loss:        0.000467  [ 6592/ 8083]
loss:        0.000528  [ 6752/ 8083]
loss:        0.000481  [ 6912/ 8083]
loss:        0.000518  [ 7072/ 8083]
loss:        0.000477  [ 7232/ 8083]
loss:        0.000436  [ 7392/ 8083]
loss:        0.000416  [ 7552/ 8083]
loss:        0.000537  [ 7712/ 8083]
loss:        0.000587  [ 7872/ 8083]
loss:        0.000444  [ 8032/ 8083]

Avg validation loss:        0.000501
Learning rate: [1e-05]
EarlyStopping counter: 10 out of 20


Epoch 46
-------------------------------
loss:        0.000349  [   32/ 8083]
loss:        0.000427  [  192/ 8083]
loss:        0.000635  [  352/ 8083]
loss:        0.000672  [  512/ 8083]
loss:        0.000564  [  672/ 8083]
loss:        0.000503  [  832/ 8083]
loss:        0.000489  [  992/ 8083]
loss:        0.000553  [ 1152/ 8083]
loss:        0.000483  [ 1312/ 8083]
loss:        0.000419  [ 1472/ 8083]
loss:        0.000481  [ 1632/ 8083]
loss:        0.000445  [ 1792/ 8083]
loss:        0.000613  [ 1952/ 8083]
loss:        0.000492  [ 2112/ 8083]
loss:        0.000471  [ 2272/ 8083]
loss:        0.000547  [ 2432/ 8083]
loss:        0.000311  [ 2592/ 8083]
loss:        0.000400  [ 2752/ 8083]
loss:        0.000380  [ 2912/ 8083]
loss:        0.000515  [ 3072/ 8083]
loss:        0.000449  [ 3232/ 8083]
loss:        0.000560  [ 3392/ 8083]
loss:        0.000426  [ 3552/ 8083]
loss:        0.000559  [ 3712/ 8083]
loss:        0.000515  [ 3872/ 8083]
loss:        0.000530  [ 4032/ 8083]
loss:        0.000568  [ 4192/ 8083]
loss:        0.000439  [ 4352/ 8083]
loss:        0.000443  [ 4512/ 8083]
loss:        0.000549  [ 4672/ 8083]
loss:        0.000505  [ 4832/ 8083]
loss:        0.000470  [ 4992/ 8083]
loss:        0.000451  [ 5152/ 8083]
loss:        0.000555  [ 5312/ 8083]
loss:        0.000403  [ 5472/ 8083]
loss:        0.000538  [ 5632/ 8083]
loss:        0.000523  [ 5792/ 8083]
loss:        0.000483  [ 5952/ 8083]
loss:        0.000528  [ 6112/ 8083]
loss:        0.000444  [ 6272/ 8083]
loss:        0.000586  [ 6432/ 8083]
loss:        0.000407  [ 6592/ 8083]
loss:        0.000533  [ 6752/ 8083]
loss:        0.000538  [ 6912/ 8083]
loss:        0.000446  [ 7072/ 8083]
loss:        0.000402  [ 7232/ 8083]
loss:        0.000553  [ 7392/ 8083]
loss:        0.000538  [ 7552/ 8083]
loss:        0.000605  [ 7712/ 8083]
loss:        0.000528  [ 7872/ 8083]
loss:        0.000545  [ 8032/ 8083]

Avg validation loss:        0.000502
Learning rate: [1e-05]
EarlyStopping counter: 11 out of 20


Epoch 47
-------------------------------
loss:        0.000524  [   32/ 8083]
loss:        0.000559  [  192/ 8083]
loss:        0.000440  [  352/ 8083]
loss:        0.000498  [  512/ 8083]
loss:        0.000485  [  672/ 8083]
loss:        0.000472  [  832/ 8083]
loss:        0.000531  [  992/ 8083]
loss:        0.000488  [ 1152/ 8083]
loss:        0.000553  [ 1312/ 8083]
loss:        0.000431  [ 1472/ 8083]
loss:        0.000537  [ 1632/ 8083]
loss:        0.000469  [ 1792/ 8083]
loss:        0.000535  [ 1952/ 8083]
loss:        0.000439  [ 2112/ 8083]
loss:        0.000469  [ 2272/ 8083]
loss:        0.000585  [ 2432/ 8083]
loss:        0.000447  [ 2592/ 8083]
loss:        0.000547  [ 2752/ 8083]
loss:        0.000465  [ 2912/ 8083]
loss:        0.000487  [ 3072/ 8083]
loss:        0.000527  [ 3232/ 8083]
loss:        0.000478  [ 3392/ 8083]
loss:        0.000516  [ 3552/ 8083]
loss:        0.000370  [ 3712/ 8083]
loss:        0.000509  [ 3872/ 8083]
loss:        0.000445  [ 4032/ 8083]
loss:        0.000533  [ 4192/ 8083]
loss:        0.000543  [ 4352/ 8083]
loss:        0.000521  [ 4512/ 8083]
loss:        0.000517  [ 4672/ 8083]
loss:        0.000507  [ 4832/ 8083]
loss:        0.000557  [ 4992/ 8083]
loss:        0.000406  [ 5152/ 8083]
loss:        0.000420  [ 5312/ 8083]
loss:        0.000454  [ 5472/ 8083]
loss:        0.000601  [ 5632/ 8083]
loss:        0.000435  [ 5792/ 8083]
loss:        0.000488  [ 5952/ 8083]
loss:        0.000509  [ 6112/ 8083]
loss:        0.000493  [ 6272/ 8083]
loss:        0.000532  [ 6432/ 8083]
loss:        0.000514  [ 6592/ 8083]
loss:        0.000574  [ 6752/ 8083]
loss:        0.000469  [ 6912/ 8083]
loss:        0.000505  [ 7072/ 8083]
loss:        0.000478  [ 7232/ 8083]
loss:        0.000391  [ 7392/ 8083]
loss:        0.000502  [ 7552/ 8083]
loss:        0.000456  [ 7712/ 8083]
loss:        0.000396  [ 7872/ 8083]
loss:        0.000568  [ 8032/ 8083]

Avg validation loss:        0.000503
Learning rate: [1.0000000000000002e-06]
EarlyStopping counter: 12 out of 20


Epoch 48
-------------------------------
loss:        0.000442  [   32/ 8083]
loss:        0.000585  [  192/ 8083]
loss:        0.000536  [  352/ 8083]
loss:        0.000395  [  512/ 8083]
loss:        0.000489  [  672/ 8083]
loss:        0.000448  [  832/ 8083]
loss:        0.000501  [  992/ 8083]
loss:        0.000516  [ 1152/ 8083]
loss:        0.000482  [ 1312/ 8083]
loss:        0.000537  [ 1472/ 8083]
loss:        0.000536  [ 1632/ 8083]
loss:        0.000409  [ 1792/ 8083]
loss:        0.000480  [ 1952/ 8083]
loss:        0.000604  [ 2112/ 8083]
loss:        0.000476  [ 2272/ 8083]
loss:        0.000457  [ 2432/ 8083]
loss:        0.000447  [ 2592/ 8083]
loss:        0.000462  [ 2752/ 8083]
loss:        0.000391  [ 2912/ 8083]
loss:        0.000457  [ 3072/ 8083]
loss:        0.000528  [ 3232/ 8083]
loss:        0.000305  [ 3392/ 8083]
loss:        0.000582  [ 3552/ 8083]
loss:        0.000590  [ 3712/ 8083]
loss:        0.000401  [ 3872/ 8083]
loss:        0.000493  [ 4032/ 8083]
loss:        0.000405  [ 4192/ 8083]
loss:        0.000559  [ 4352/ 8083]
loss:        0.000423  [ 4512/ 8083]
loss:        0.000486  [ 4672/ 8083]
loss:        0.000457  [ 4832/ 8083]
loss:        0.000502  [ 4992/ 8083]
loss:        0.000402  [ 5152/ 8083]
loss:        0.000506  [ 5312/ 8083]
loss:        0.000509  [ 5472/ 8083]
loss:        0.000493  [ 5632/ 8083]
loss:        0.000554  [ 5792/ 8083]
loss:        0.000475  [ 5952/ 8083]
loss:        0.000620  [ 6112/ 8083]
loss:        0.000500  [ 6272/ 8083]
loss:        0.000476  [ 6432/ 8083]
loss:        0.000534  [ 6592/ 8083]
loss:        0.000491  [ 6752/ 8083]
loss:        0.000404  [ 6912/ 8083]
loss:        0.000521  [ 7072/ 8083]
loss:        0.000454  [ 7232/ 8083]
loss:        0.000414  [ 7392/ 8083]
loss:        0.000569  [ 7552/ 8083]
loss:        0.000441  [ 7712/ 8083]
loss:        0.000526  [ 7872/ 8083]
loss:        0.000417  [ 8032/ 8083]

Avg validation loss:        0.000503
Learning rate: [1.0000000000000002e-06]
EarlyStopping counter: 13 out of 20


Epoch 49
-------------------------------
loss:        0.000427  [   32/ 8083]
loss:        0.000498  [  192/ 8083]
loss:        0.000472  [  352/ 8083]
loss:        0.000413  [  512/ 8083]
loss:        0.000460  [  672/ 8083]
loss:        0.000525  [  832/ 8083]
loss:        0.000542  [  992/ 8083]
loss:        0.000465  [ 1152/ 8083]
loss:        0.000533  [ 1312/ 8083]
loss:        0.000464  [ 1472/ 8083]
loss:        0.000488  [ 1632/ 8083]
loss:        0.000557  [ 1792/ 8083]
loss:        0.000449  [ 1952/ 8083]
loss:        0.000430  [ 2112/ 8083]
loss:        0.000461  [ 2272/ 8083]
loss:        0.000463  [ 2432/ 8083]
loss:        0.000632  [ 2592/ 8083]
loss:        0.000498  [ 2752/ 8083]
loss:        0.000425  [ 2912/ 8083]
loss:        0.000399  [ 3072/ 8083]
loss:        0.000477  [ 3232/ 8083]
loss:        0.000496  [ 3392/ 8083]
loss:        0.000479  [ 3552/ 8083]
loss:        0.000517  [ 3712/ 8083]
loss:        0.000427  [ 3872/ 8083]
loss:        0.000481  [ 4032/ 8083]
loss:        0.000399  [ 4192/ 8083]
loss:        0.000552  [ 4352/ 8083]
loss:        0.000535  [ 4512/ 8083]
loss:        0.000565  [ 4672/ 8083]
loss:        0.000479  [ 4832/ 8083]
loss:        0.000520  [ 4992/ 8083]
loss:        0.000522  [ 5152/ 8083]
loss:        0.000596  [ 5312/ 8083]
loss:        0.000530  [ 5472/ 8083]
loss:        0.000424  [ 5632/ 8083]
loss:        0.000357  [ 5792/ 8083]
loss:        0.000525  [ 5952/ 8083]
loss:        0.000526  [ 6112/ 8083]
loss:        0.000454  [ 6272/ 8083]
loss:        0.000512  [ 6432/ 8083]
loss:        0.000522  [ 6592/ 8083]
loss:        0.000508  [ 6752/ 8083]
loss:        0.000505  [ 6912/ 8083]
loss:        0.000553  [ 7072/ 8083]
loss:        0.000426  [ 7232/ 8083]
loss:        0.000440  [ 7392/ 8083]
loss:        0.000427  [ 7552/ 8083]
loss:        0.000471  [ 7712/ 8083]
loss:        0.000476  [ 7872/ 8083]
loss:        0.000583  [ 8032/ 8083]

Avg validation loss:        0.000502
Learning rate: [1.0000000000000002e-06]
EarlyStopping counter: 14 out of 20


Epoch 50
-------------------------------
loss:        0.000517  [   32/ 8083]
loss:        0.000440  [  192/ 8083]
loss:        0.000465  [  352/ 8083]
loss:        0.000507  [  512/ 8083]
loss:        0.000544  [  672/ 8083]
loss:        0.000518  [  832/ 8083]
loss:        0.000531  [  992/ 8083]
loss:        0.000483  [ 1152/ 8083]
loss:        0.000564  [ 1312/ 8083]
loss:        0.000667  [ 1472/ 8083]
loss:        0.000515  [ 1632/ 8083]
loss:        0.000374  [ 1792/ 8083]
loss:        0.000433  [ 1952/ 8083]
loss:        0.000532  [ 2112/ 8083]
loss:        0.000595  [ 2272/ 8083]
loss:        0.000576  [ 2432/ 8083]
loss:        0.000492  [ 2592/ 8083]
loss:        0.000397  [ 2752/ 8083]
loss:        0.000499  [ 2912/ 8083]
loss:        0.000474  [ 3072/ 8083]
loss:        0.000465  [ 3232/ 8083]
loss:        0.000537  [ 3392/ 8083]
loss:        0.000486  [ 3552/ 8083]
loss:        0.000514  [ 3712/ 8083]
loss:        0.000521  [ 3872/ 8083]
loss:        0.000459  [ 4032/ 8083]
loss:        0.000474  [ 4192/ 8083]
loss:        0.000455  [ 4352/ 8083]
loss:        0.000477  [ 4512/ 8083]
loss:        0.000455  [ 4672/ 8083]
loss:        0.000526  [ 4832/ 8083]
loss:        0.000473  [ 4992/ 8083]
loss:        0.000410  [ 5152/ 8083]
loss:        0.000497  [ 5312/ 8083]
loss:        0.000477  [ 5472/ 8083]
loss:        0.000514  [ 5632/ 8083]
loss:        0.000445  [ 5792/ 8083]
loss:        0.000485  [ 5952/ 8083]
loss:        0.000575  [ 6112/ 8083]
loss:        0.000441  [ 6272/ 8083]
loss:        0.000496  [ 6432/ 8083]
loss:        0.000526  [ 6592/ 8083]
loss:        0.000541  [ 6752/ 8083]
loss:        0.000407  [ 6912/ 8083]
loss:        0.000449  [ 7072/ 8083]
loss:        0.000522  [ 7232/ 8083]
loss:        0.000464  [ 7392/ 8083]
loss:        0.000425  [ 7552/ 8083]
loss:        0.000537  [ 7712/ 8083]
loss:        0.000554  [ 7872/ 8083]
loss:        0.000503  [ 8032/ 8083]

Avg validation loss:        0.000503
Learning rate: [1.0000000000000002e-06]
EarlyStopping counter: 15 out of 20


Epoch 51
-------------------------------
loss:        0.000481  [   32/ 8083]
loss:        0.000491  [  192/ 8083]
loss:        0.000507  [  352/ 8083]
loss:        0.000411  [  512/ 8083]
loss:        0.000501  [  672/ 8083]
loss:        0.000372  [  832/ 8083]
loss:        0.000654  [  992/ 8083]
loss:        0.000435  [ 1152/ 8083]
loss:        0.000517  [ 1312/ 8083]
loss:        0.000457  [ 1472/ 8083]
loss:        0.000408  [ 1632/ 8083]
loss:        0.000503  [ 1792/ 8083]
loss:        0.000536  [ 1952/ 8083]
loss:        0.000522  [ 2112/ 8083]
loss:        0.000413  [ 2272/ 8083]
loss:        0.000440  [ 2432/ 8083]
loss:        0.000494  [ 2592/ 8083]
loss:        0.000512  [ 2752/ 8083]
loss:        0.000438  [ 2912/ 8083]
loss:        0.000382  [ 3072/ 8083]
loss:        0.000587  [ 3232/ 8083]
loss:        0.000429  [ 3392/ 8083]
loss:        0.000527  [ 3552/ 8083]
loss:        0.000581  [ 3712/ 8083]
loss:        0.000402  [ 3872/ 8083]
loss:        0.000470  [ 4032/ 8083]
loss:        0.000518  [ 4192/ 8083]
loss:        0.000510  [ 4352/ 8083]
loss:        0.000496  [ 4512/ 8083]
loss:        0.000516  [ 4672/ 8083]
loss:        0.000546  [ 4832/ 8083]
loss:        0.000510  [ 4992/ 8083]
loss:        0.000514  [ 5152/ 8083]
loss:        0.000558  [ 5312/ 8083]
loss:        0.000526  [ 5472/ 8083]
loss:        0.000446  [ 5632/ 8083]
loss:        0.000497  [ 5792/ 8083]
loss:        0.000379  [ 5952/ 8083]
loss:        0.000501  [ 6112/ 8083]
loss:        0.000460  [ 6272/ 8083]
loss:        0.000572  [ 6432/ 8083]
loss:        0.000532  [ 6592/ 8083]
loss:        0.000482  [ 6752/ 8083]
loss:        0.000511  [ 6912/ 8083]
loss:        0.000577  [ 7072/ 8083]
loss:        0.000528  [ 7232/ 8083]
loss:        0.000463  [ 7392/ 8083]
loss:        0.000596  [ 7552/ 8083]
loss:        0.000549  [ 7712/ 8083]
loss:        0.000531  [ 7872/ 8083]
loss:        0.000438  [ 8032/ 8083]

Avg validation loss:        0.000501
Learning rate: [1.0000000000000002e-06]
EarlyStopping counter: 16 out of 20


Epoch 52
-------------------------------
loss:        0.000570  [   32/ 8083]
loss:        0.000522  [  192/ 8083]
loss:        0.000404  [  352/ 8083]
loss:        0.000474  [  512/ 8083]
loss:        0.000569  [  672/ 8083]
loss:        0.000449  [  832/ 8083]
loss:        0.000533  [  992/ 8083]
loss:        0.000481  [ 1152/ 8083]
loss:        0.000534  [ 1312/ 8083]
loss:        0.000543  [ 1472/ 8083]
loss:        0.000560  [ 1632/ 8083]
loss:        0.000537  [ 1792/ 8083]
loss:        0.000510  [ 1952/ 8083]
loss:        0.000398  [ 2112/ 8083]
loss:        0.000592  [ 2272/ 8083]
loss:        0.000545  [ 2432/ 8083]
loss:        0.000491  [ 2592/ 8083]
loss:        0.000519  [ 2752/ 8083]
loss:        0.000399  [ 2912/ 8083]
loss:        0.000562  [ 3072/ 8083]
loss:        0.000439  [ 3232/ 8083]
loss:        0.000515  [ 3392/ 8083]
loss:        0.000518  [ 3552/ 8083]
loss:        0.000582  [ 3712/ 8083]
loss:        0.000493  [ 3872/ 8083]
loss:        0.000504  [ 4032/ 8083]
loss:        0.000522  [ 4192/ 8083]
loss:        0.000425  [ 4352/ 8083]
loss:        0.000490  [ 4512/ 8083]
loss:        0.000491  [ 4672/ 8083]
loss:        0.000566  [ 4832/ 8083]
loss:        0.000455  [ 4992/ 8083]
loss:        0.000549  [ 5152/ 8083]
loss:        0.000413  [ 5312/ 8083]
loss:        0.000556  [ 5472/ 8083]
loss:        0.000517  [ 5632/ 8083]
loss:        0.000481  [ 5792/ 8083]
loss:        0.000532  [ 5952/ 8083]
loss:        0.000591  [ 6112/ 8083]
loss:        0.000501  [ 6272/ 8083]
loss:        0.000522  [ 6432/ 8083]
loss:        0.000573  [ 6592/ 8083]
loss:        0.000539  [ 6752/ 8083]
loss:        0.000423  [ 6912/ 8083]
loss:        0.000373  [ 7072/ 8083]
loss:        0.000390  [ 7232/ 8083]
loss:        0.000431  [ 7392/ 8083]
loss:        0.000538  [ 7552/ 8083]
loss:        0.000390  [ 7712/ 8083]
loss:        0.000525  [ 7872/ 8083]
loss:        0.000631  [ 8032/ 8083]

Avg validation loss:        0.000502
Learning rate: [1.0000000000000002e-06]
EarlyStopping counter: 17 out of 20


Epoch 53
-------------------------------
loss:        0.000498  [   32/ 8083]
loss:        0.000528  [  192/ 8083]
loss:        0.000456  [  352/ 8083]
loss:        0.000444  [  512/ 8083]
loss:        0.000569  [  672/ 8083]
loss:        0.000513  [  832/ 8083]
loss:        0.000422  [  992/ 8083]
loss:        0.000493  [ 1152/ 8083]
loss:        0.000407  [ 1312/ 8083]
loss:        0.000455  [ 1472/ 8083]
loss:        0.000642  [ 1632/ 8083]
loss:        0.000433  [ 1792/ 8083]
loss:        0.000475  [ 1952/ 8083]
loss:        0.000506  [ 2112/ 8083]
loss:        0.000394  [ 2272/ 8083]
loss:        0.000499  [ 2432/ 8083]
loss:        0.000433  [ 2592/ 8083]
loss:        0.000454  [ 2752/ 8083]
loss:        0.000460  [ 2912/ 8083]
loss:        0.000429  [ 3072/ 8083]
loss:        0.000438  [ 3232/ 8083]
loss:        0.000475  [ 3392/ 8083]
loss:        0.000465  [ 3552/ 8083]
loss:        0.000571  [ 3712/ 8083]
loss:        0.000416  [ 3872/ 8083]
loss:        0.000570  [ 4032/ 8083]
loss:        0.000518  [ 4192/ 8083]
loss:        0.000528  [ 4352/ 8083]
loss:        0.000392  [ 4512/ 8083]
loss:        0.000529  [ 4672/ 8083]
loss:        0.000552  [ 4832/ 8083]
loss:        0.000557  [ 4992/ 8083]
loss:        0.000553  [ 5152/ 8083]
loss:        0.000538  [ 5312/ 8083]
loss:        0.000600  [ 5472/ 8083]
loss:        0.000503  [ 5632/ 8083]
loss:        0.000480  [ 5792/ 8083]
loss:        0.000424  [ 5952/ 8083]
loss:        0.000398  [ 6112/ 8083]
loss:        0.000490  [ 6272/ 8083]
loss:        0.000442  [ 6432/ 8083]
loss:        0.000488  [ 6592/ 8083]
loss:        0.000400  [ 6752/ 8083]
loss:        0.000537  [ 6912/ 8083]
loss:        0.000400  [ 7072/ 8083]
loss:        0.000501  [ 7232/ 8083]
loss:        0.000421  [ 7392/ 8083]
loss:        0.000468  [ 7552/ 8083]
loss:        0.000422  [ 7712/ 8083]
loss:        0.000522  [ 7872/ 8083]
loss:        0.000523  [ 8032/ 8083]

Avg validation loss:        0.000505
Learning rate: [1.0000000000000002e-07]
EarlyStopping counter: 18 out of 20


Epoch 54
-------------------------------
loss:        0.000504  [   32/ 8083]
loss:        0.000479  [  192/ 8083]
loss:        0.000500  [  352/ 8083]
loss:        0.000512  [  512/ 8083]
loss:        0.000498  [  672/ 8083]
loss:        0.000518  [  832/ 8083]
loss:        0.000556  [  992/ 8083]
loss:        0.000425  [ 1152/ 8083]
loss:        0.000444  [ 1312/ 8083]
loss:        0.000416  [ 1472/ 8083]
loss:        0.000489  [ 1632/ 8083]
loss:        0.000477  [ 1792/ 8083]
loss:        0.000499  [ 1952/ 8083]
loss:        0.000482  [ 2112/ 8083]
loss:        0.000449  [ 2272/ 8083]
loss:        0.000629  [ 2432/ 8083]
loss:        0.000500  [ 2592/ 8083]
loss:        0.000440  [ 2752/ 8083]
loss:        0.000474  [ 2912/ 8083]
loss:        0.000445  [ 3072/ 8083]
loss:        0.000480  [ 3232/ 8083]
loss:        0.000374  [ 3392/ 8083]
loss:        0.000455  [ 3552/ 8083]
loss:        0.000595  [ 3712/ 8083]
loss:        0.000521  [ 3872/ 8083]
loss:        0.000418  [ 4032/ 8083]
loss:        0.000484  [ 4192/ 8083]
loss:        0.000417  [ 4352/ 8083]
loss:        0.000490  [ 4512/ 8083]
loss:        0.000524  [ 4672/ 8083]
loss:        0.000474  [ 4832/ 8083]
loss:        0.000562  [ 4992/ 8083]
loss:        0.000536  [ 5152/ 8083]
loss:        0.000496  [ 5312/ 8083]
loss:        0.000488  [ 5472/ 8083]
loss:        0.000522  [ 5632/ 8083]
loss:        0.000478  [ 5792/ 8083]
loss:        0.000536  [ 5952/ 8083]
loss:        0.000438  [ 6112/ 8083]
loss:        0.000481  [ 6272/ 8083]
loss:        0.000465  [ 6432/ 8083]
loss:        0.000513  [ 6592/ 8083]
loss:        0.000557  [ 6752/ 8083]
loss:        0.000542  [ 6912/ 8083]
loss:        0.000432  [ 7072/ 8083]
loss:        0.000451  [ 7232/ 8083]
loss:        0.000502  [ 7392/ 8083]
loss:        0.000355  [ 7552/ 8083]
loss:        0.000487  [ 7712/ 8083]
loss:        0.000436  [ 7872/ 8083]
loss:        0.000514  [ 8032/ 8083]

Avg validation loss:        0.000504
Learning rate: [1.0000000000000002e-07]
EarlyStopping counter: 19 out of 20


Epoch 55
-------------------------------
loss:        0.000418  [   32/ 8083]
loss:        0.000527  [  192/ 8083]
loss:        0.000599  [  352/ 8083]
loss:        0.000568  [  512/ 8083]
loss:        0.000490  [  672/ 8083]
loss:        0.000577  [  832/ 8083]
loss:        0.000434  [  992/ 8083]
loss:        0.000611  [ 1152/ 8083]
loss:        0.000456  [ 1312/ 8083]
loss:        0.000504  [ 1472/ 8083]
loss:        0.000480  [ 1632/ 8083]
loss:        0.000411  [ 1792/ 8083]
loss:        0.000479  [ 1952/ 8083]
loss:        0.000486  [ 2112/ 8083]
loss:        0.000431  [ 2272/ 8083]
loss:        0.000537  [ 2432/ 8083]
loss:        0.000573  [ 2592/ 8083]
loss:        0.000457  [ 2752/ 8083]
loss:        0.000529  [ 2912/ 8083]
loss:        0.000508  [ 3072/ 8083]
loss:        0.000566  [ 3232/ 8083]
loss:        0.000409  [ 3392/ 8083]
loss:        0.000539  [ 3552/ 8083]
loss:        0.000481  [ 3712/ 8083]
loss:        0.000405  [ 3872/ 8083]
loss:        0.000393  [ 4032/ 8083]
loss:        0.000494  [ 4192/ 8083]
loss:        0.000469  [ 4352/ 8083]
loss:        0.000513  [ 4512/ 8083]
loss:        0.000477  [ 4672/ 8083]
loss:        0.000462  [ 4832/ 8083]
loss:        0.000474  [ 4992/ 8083]
loss:        0.000563  [ 5152/ 8083]
loss:        0.000480  [ 5312/ 8083]
loss:        0.000520  [ 5472/ 8083]
loss:        0.000550  [ 5632/ 8083]
loss:        0.000480  [ 5792/ 8083]
loss:        0.000483  [ 5952/ 8083]
loss:        0.000511  [ 6112/ 8083]
loss:        0.000470  [ 6272/ 8083]
loss:        0.000570  [ 6432/ 8083]
loss:        0.000446  [ 6592/ 8083]
loss:        0.000549  [ 6752/ 8083]
loss:        0.000507  [ 6912/ 8083]
loss:        0.000487  [ 7072/ 8083]
loss:        0.000502  [ 7232/ 8083]
loss:        0.000464  [ 7392/ 8083]
loss:        0.000537  [ 7552/ 8083]
loss:        0.000403  [ 7712/ 8083]
loss:        0.000463  [ 7872/ 8083]
loss:        0.000586  [ 8032/ 8083]

Avg validation loss:        0.000503
Learning rate: [1.0000000000000002e-07]
EarlyStopping counter: 20 out of 20
Early stopping
Avg test loss:        0.000520 

Done!</code></pre>
</div>
</div>
<section id="create-an-output-directory-to-save-the-outputs-and-plots." class="level3">
<h3 class="anchored" data-anchor-id="create-an-output-directory-to-save-the-outputs-and-plots.">Create an output directory to save the outputs and plots.</h3>
<div id="cell-97" class="cell" data-execution_count="61">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># To save the outputs of the model, create a directory</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>output_dir <span class="op">=</span> <span class="ss">f'outputs/model_outputs/deepSSF_S2/id</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>os.makedirs(output_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="save-the-validation-loss-as-a-dataframe" class="level3">
<h3 class="anchored" data-anchor-id="save-the-validation-loss-as-a-dataframe">Save the validation loss as a dataframe</h3>
<div id="cell-99" class="cell" data-execution_count="62">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Directory for saving the loss dataframe</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>filename_loss_csv <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>output_dir<span class="sc">}</span><span class="ss">/deepSSF_S2_val_loss_buffalo</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>today_date<span class="sc">}</span><span class="ss">.csv'</span></span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>val_losses_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epoch"</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(val_losses) <span class="op">+</span> <span class="dv">1</span>),</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"val_losses"</span>: val_losses</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the validation losses to a CSV file</span></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>val_losses_df.to_csv(filename_loss_csv, index<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="plot-the-validation-loss" class="level3">
<h3 class="anchored" data-anchor-id="plot-the-validation-loss">Plot the validation loss</h3>
<div id="cell-101" class="cell" data-execution_count="63">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Directory for saving the loss plots</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>filename_loss_png <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>output_dir<span class="sc">}</span><span class="ss">/deepSSF_S2_val_loss_buffalo</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>today_date<span class="sc">}</span><span class="ss">.png'</span></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the validation losses</span></span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>plt.plot(val_losses, label<span class="op">=</span><span class="st">'Validation Loss'</span>, color<span class="op">=</span><span class="st">'red'</span>)  <span class="co"># Plot validation loss in red</span></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Validation Losses'</span>)</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>plt.legend()  <span class="co"># Show legend to distinguish lines</span></span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>plt.savefig(filename_loss_png, dpi<span class="op">=</span><span class="dv">600</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>)</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-64-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="check-model-parameters" class="level3">
<h3 class="anchored" data-anchor-id="check-model-parameters">Check model parameters</h3>
<div id="cell-103" class="cell" data-execution_count="64">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to look at the parameters (weights and biases) of the model</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="co"># print(model.state_dict())</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
</section>
<section id="loading-in-previous-models" class="level1">
<h1>Loading in previous models</h1>
<p>As weâ€™ve trained the model, the model parameters are already stored in the <code>model</code> object. But as we were training the model, we were saving it to file, and that, and other trained models can be loaded.</p>
<p>The model parameters that are being loaded must match the model object that has been defined above. If the model object has changed, the model parameters will not be able to be loaded.</p>
<div id="cell-105" class="cell" data-execution_count="65">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>path_save_weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>'model_checkpoints/deepSSF_S2_slope_buffalo2005_2025-02-09.pt'</code></pre>
</div>
</div>
<section id="if-loading-a-previously-trained-model" class="level3">
<h3 class="anchored" data-anchor-id="if-loading-a-previously-trained-model">If loading a previously trained model</h3>
<div id="cell-107" class="cell" data-execution_count="66">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to load previously saved weights</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="co"># path_save_weights = f'model_checkpoints/deepSSF_S2_slope_buffalo2005_2025-02-09.pt'</span></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(torch.load(path_save_weights, </span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>                                 weights_only<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>                                 map_location<span class="op">=</span>torch.device(<span class="st">'cpu'</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>&lt;All keys matched successfully&gt;</code></pre>
</div>
</div>
</section>
</section>
<section id="test-model" class="level1">
<h1>Test model</h1>
<p>Take some random samples from the test dataset and generate predictions for them. We loop through the samples (which are shuffled randomly), make predictions, and plot the results.</p>
<section id="helper-functions" class="level3">
<h3 class="anchored" data-anchor-id="helper-functions">Helper functions</h3>
<p>To return the hour and day of the year to their original values, we can use the following functions.</p>
<div id="cell-110" class="cell" data-execution_count="67">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> recover_hour(sin_term, cos_term):</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the angle theta</span></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.arctan2(sin_term, cos_term)</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate hour_t2</span></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>    hour <span class="op">=</span> (<span class="dv">12</span> <span class="op">*</span> theta) <span class="op">/</span> np.pi <span class="op">%</span> <span class="dv">24</span></span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> hour</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> recover_yday(sin_term, cos_term):</span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the angle theta</span></span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.arctan2(sin_term, cos_term)</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate hour_t2</span></span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a>    yday <span class="op">=</span> (<span class="dv">365</span> <span class="op">*</span> theta) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi)  <span class="op">%</span> <span class="dv">365</span></span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> yday</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-111" class="cell" data-executioninfo="{&quot;elapsed&quot;:216119,&quot;status&quot;:&quot;error&quot;,&quot;timestamp&quot;:1731285474344,&quot;user&quot;:{&quot;displayName&quot;:&quot;Scott Forrest&quot;,&quot;userId&quot;:&quot;01745476916650130529&quot;},&quot;user_tz&quot;:-600}" data-outputid="7c83401e-7f3c-4eb2-a518-8ac326d265bb" data-execution_count="68">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Set the model in evaluation mode</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop over samples in the validation dataset</span></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">5</span>):</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Display image and label</span></span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>  x1, x2, x3, labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataloader_test))</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Pull out the scalars</span></span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>  hour_t2_sin <span class="op">=</span> x2.detach().numpy()[<span class="dv">0</span>,<span class="dv">0</span>]</span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>  hour_t2_cos <span class="op">=</span> x2.detach().numpy()[<span class="dv">0</span>,<span class="dv">1</span>]</span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>  yday_t2_sin <span class="op">=</span> x2.detach().numpy()[<span class="dv">0</span>,<span class="dv">2</span>]</span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a>  yday_t2_cos <span class="op">=</span> x2.detach().numpy()[<span class="dv">0</span>,<span class="dv">3</span>]</span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a>  bearing <span class="op">=</span> x3.detach().numpy()[<span class="dv">0</span>,<span class="dv">0</span>]</span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Recover the hour</span></span>
<span id="cb95-18"><a href="#cb95-18" aria-hidden="true" tabindex="-1"></a>  hour_t2 <span class="op">=</span> recover_hour(hour_t2_sin, hour_t2_cos)</span>
<span id="cb95-19"><a href="#cb95-19" aria-hidden="true" tabindex="-1"></a>  hour_t2_integer <span class="op">=</span> <span class="bu">int</span>(hour_t2)  <span class="co"># Convert to integer</span></span>
<span id="cb95-20"><a href="#cb95-20" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f'Hour:                        </span><span class="sc">{</span>hour_t2_integer<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb95-21"><a href="#cb95-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-22"><a href="#cb95-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Recover the day of the year</span></span>
<span id="cb95-23"><a href="#cb95-23" aria-hidden="true" tabindex="-1"></a>  yday_t2 <span class="op">=</span> recover_yday(yday_t2_sin, yday_t2_cos)</span>
<span id="cb95-24"><a href="#cb95-24" aria-hidden="true" tabindex="-1"></a>  yday_t2_integer <span class="op">=</span> <span class="bu">int</span>(yday_t2)  <span class="co"># Convert to integer</span></span>
<span id="cb95-25"><a href="#cb95-25" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f'Day of the year:             </span><span class="sc">{</span>yday_t2_integer<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb95-26"><a href="#cb95-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-27"><a href="#cb95-27" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Recover the bearing</span></span>
<span id="cb95-28"><a href="#cb95-28" aria-hidden="true" tabindex="-1"></a>  bearing_degrees <span class="op">=</span> np.degrees(bearing) <span class="op">%</span> <span class="dv">360</span></span>
<span id="cb95-29"><a href="#cb95-29" aria-hidden="true" tabindex="-1"></a>  bearing_degrees <span class="op">=</span> <span class="bu">round</span>(bearing_degrees, <span class="dv">1</span>)  <span class="co"># Round to 2 decimal places</span></span>
<span id="cb95-30"><a href="#cb95-30" aria-hidden="true" tabindex="-1"></a>  bearing_degrees <span class="op">=</span> <span class="bu">int</span>(bearing_degrees)  <span class="co"># Convert to integer</span></span>
<span id="cb95-31"><a href="#cb95-31" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f'Bearing (radians):           </span><span class="sc">{</span>bearing<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb95-32"><a href="#cb95-32" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f'Bearing (degrees):           </span><span class="sc">{</span>bearing_degrees<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb95-33"><a href="#cb95-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-34"><a href="#cb95-34" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Pull out the RGB layers for plotting</span></span>
<span id="cb95-35"><a href="#cb95-35" aria-hidden="true" tabindex="-1"></a>  blue_layer <span class="op">=</span> x1.detach().cpu().numpy()[<span class="dv">0</span>,<span class="dv">1</span>,:,:]</span>
<span id="cb95-36"><a href="#cb95-36" aria-hidden="true" tabindex="-1"></a>  green_layer <span class="op">=</span> x1.detach().cpu().numpy()[<span class="dv">0</span>,<span class="dv">2</span>,:,:]</span>
<span id="cb95-37"><a href="#cb95-37" aria-hidden="true" tabindex="-1"></a>  red_layer <span class="op">=</span> x1.detach().cpu().numpy()[<span class="dv">0</span>,<span class="dv">3</span>,:,:]</span>
<span id="cb95-38"><a href="#cb95-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-39"><a href="#cb95-39" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Stack the RGB layers</span></span>
<span id="cb95-40"><a href="#cb95-40" aria-hidden="true" tabindex="-1"></a>  rgb_image_np <span class="op">=</span> np.stack([red_layer, green_layer, blue_layer], axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb95-41"><a href="#cb95-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-42"><a href="#cb95-42" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Normalize to the range [0, 1] for display</span></span>
<span id="cb95-43"><a href="#cb95-43" aria-hidden="true" tabindex="-1"></a>  rgb_image_np <span class="op">=</span> (rgb_image_np <span class="op">-</span> rgb_image_np.<span class="bu">min</span>()) <span class="op">/</span> (rgb_image_np.<span class="bu">max</span>() <span class="op">-</span> rgb_image_np.<span class="bu">min</span>())</span>
<span id="cb95-44"><a href="#cb95-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-45"><a href="#cb95-45" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Find the coordinates of the element that is 1</span></span>
<span id="cb95-46"><a href="#cb95-46" aria-hidden="true" tabindex="-1"></a>  target <span class="op">=</span> labels.detach().cpu().numpy()[<span class="dv">0</span>,:,:]</span>
<span id="cb95-47"><a href="#cb95-47" aria-hidden="true" tabindex="-1"></a>  coordinates <span class="op">=</span> np.where(target <span class="op">==</span> <span class="dv">1</span>)</span>
<span id="cb95-48"><a href="#cb95-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-49"><a href="#cb95-49" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Extract the coordinates</span></span>
<span id="cb95-50"><a href="#cb95-50" aria-hidden="true" tabindex="-1"></a>  row, column <span class="op">=</span> coordinates[<span class="dv">0</span>][<span class="dv">0</span>], coordinates[<span class="dv">1</span>][<span class="dv">0</span>]</span>
<span id="cb95-51"><a href="#cb95-51" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"Next step is (row, column):  (</span><span class="sc">{</span>row<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>column<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb95-52"><a href="#cb95-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-53"><a href="#cb95-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-54"><a href="#cb95-54" aria-hidden="true" tabindex="-1"></a>  <span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb95-55"><a href="#cb95-55" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Run the model on the input data</span></span>
<span id="cb95-56"><a href="#cb95-56" aria-hidden="true" tabindex="-1"></a>  <span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb95-57"><a href="#cb95-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-58"><a href="#cb95-58" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Move input tensors to the GPU if available</span></span>
<span id="cb95-59"><a href="#cb95-59" aria-hidden="true" tabindex="-1"></a>  x1 <span class="op">=</span> x1.to(device)</span>
<span id="cb95-60"><a href="#cb95-60" aria-hidden="true" tabindex="-1"></a>  x2 <span class="op">=</span> x2.to(device)</span>
<span id="cb95-61"><a href="#cb95-61" aria-hidden="true" tabindex="-1"></a>  x3 <span class="op">=</span> x3.to(device)</span>
<span id="cb95-62"><a href="#cb95-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-63"><a href="#cb95-63" aria-hidden="true" tabindex="-1"></a>  test <span class="op">=</span> model((x1, x2, x3))</span>
<span id="cb95-64"><a href="#cb95-64" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print(test.shape)</span></span>
<span id="cb95-65"><a href="#cb95-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-66"><a href="#cb95-66" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Extract and exponentiate the habitat density channel</span></span>
<span id="cb95-67"><a href="#cb95-67" aria-hidden="true" tabindex="-1"></a>  hab_density <span class="op">=</span> test.detach().cpu().numpy()[<span class="dv">0</span>, :, :, <span class="dv">0</span>]</span>
<span id="cb95-68"><a href="#cb95-68" aria-hidden="true" tabindex="-1"></a>  hab_density_exp <span class="op">=</span> np.exp(hab_density)</span>
<span id="cb95-69"><a href="#cb95-69" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print(np.sum(hab_density_exp))  # Debug: check the sum of exponentiated values</span></span>
<span id="cb95-70"><a href="#cb95-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-71"><a href="#cb95-71" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Create masks to remove unwanted edge cells from visualization</span></span>
<span id="cb95-72"><a href="#cb95-72" aria-hidden="true" tabindex="-1"></a>  <span class="co">#    (setting them to -âˆž affects the color scale in plots)</span></span>
<span id="cb95-73"><a href="#cb95-73" aria-hidden="true" tabindex="-1"></a>  x_mask <span class="op">=</span> np.ones_like(hab_density)</span>
<span id="cb95-74"><a href="#cb95-74" aria-hidden="true" tabindex="-1"></a>  y_mask <span class="op">=</span> np.ones_like(hab_density)</span>
<span id="cb95-75"><a href="#cb95-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-76"><a href="#cb95-76" aria-hidden="true" tabindex="-1"></a>  <span class="co"># mask out cells on the edges that affect the colour scale</span></span>
<span id="cb95-77"><a href="#cb95-77" aria-hidden="true" tabindex="-1"></a>  x_mask[:, :<span class="dv">3</span>] <span class="op">=</span> <span class="op">-</span>np.inf</span>
<span id="cb95-78"><a href="#cb95-78" aria-hidden="true" tabindex="-1"></a>  x_mask[:, <span class="dv">98</span>:] <span class="op">=</span> <span class="op">-</span>np.inf</span>
<span id="cb95-79"><a href="#cb95-79" aria-hidden="true" tabindex="-1"></a>  y_mask[:<span class="dv">3</span>, :] <span class="op">=</span> <span class="op">-</span>np.inf</span>
<span id="cb95-80"><a href="#cb95-80" aria-hidden="true" tabindex="-1"></a>  y_mask[<span class="dv">98</span>:, :] <span class="op">=</span> <span class="op">-</span>np.inf</span>
<span id="cb95-81"><a href="#cb95-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-82"><a href="#cb95-82" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Apply the masks to the habitat density (log scale) and exponentiated version</span></span>
<span id="cb95-83"><a href="#cb95-83" aria-hidden="true" tabindex="-1"></a>  hab_density_mask <span class="op">=</span> hab_density <span class="op">*</span> x_mask <span class="op">*</span> y_mask</span>
<span id="cb95-84"><a href="#cb95-84" aria-hidden="true" tabindex="-1"></a>  hab_density_exp_mask <span class="op">=</span> hab_density_exp <span class="op">*</span> x_mask <span class="op">*</span> y_mask</span>
<span id="cb95-85"><a href="#cb95-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-86"><a href="#cb95-86" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Extract and exponentiate the movement density channel</span></span>
<span id="cb95-87"><a href="#cb95-87" aria-hidden="true" tabindex="-1"></a>  move_density <span class="op">=</span> test.detach().cpu().numpy()[<span class="dv">0</span>,:,:,<span class="dv">1</span>]</span>
<span id="cb95-88"><a href="#cb95-88" aria-hidden="true" tabindex="-1"></a>  move_density_exp <span class="op">=</span> np.exp(move_density)</span>
<span id="cb95-89"><a href="#cb95-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-90"><a href="#cb95-90" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Apply the same masking strategy to movement densities</span></span>
<span id="cb95-91"><a href="#cb95-91" aria-hidden="true" tabindex="-1"></a>  move_density_mask <span class="op">=</span> move_density <span class="op">*</span> x_mask <span class="op">*</span> y_mask</span>
<span id="cb95-92"><a href="#cb95-92" aria-hidden="true" tabindex="-1"></a>  move_density_exp_mask <span class="op">=</span> move_density_exp <span class="op">*</span> x_mask <span class="op">*</span> y_mask</span>
<span id="cb95-93"><a href="#cb95-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-94"><a href="#cb95-94" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute the next-step density by adding habitat + movement (log-space)</span></span>
<span id="cb95-95"><a href="#cb95-95" aria-hidden="true" tabindex="-1"></a>  step_density <span class="op">=</span> test[<span class="dv">0</span>, :, :, <span class="dv">0</span>] <span class="op">+</span> test[<span class="dv">0</span>, :, :, <span class="dv">1</span>]</span>
<span id="cb95-96"><a href="#cb95-96" aria-hidden="true" tabindex="-1"></a>  step_density <span class="op">=</span> step_density.detach().cpu().numpy()</span>
<span id="cb95-97"><a href="#cb95-97" aria-hidden="true" tabindex="-1"></a>  step_density_exp <span class="op">=</span> np.exp(step_density)</span>
<span id="cb95-98"><a href="#cb95-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-99"><a href="#cb95-99" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Apply masks to the step densities (log and exponentiated)</span></span>
<span id="cb95-100"><a href="#cb95-100" aria-hidden="true" tabindex="-1"></a>  step_density_mask <span class="op">=</span> step_density <span class="op">*</span> x_mask <span class="op">*</span> y_mask</span>
<span id="cb95-101"><a href="#cb95-101" aria-hidden="true" tabindex="-1"></a>  step_density_exp_mask <span class="op">=</span> step_density_exp <span class="op">*</span> x_mask <span class="op">*</span> y_mask</span>
<span id="cb95-102"><a href="#cb95-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-103"><a href="#cb95-103" aria-hidden="true" tabindex="-1"></a>  <span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb95-104"><a href="#cb95-104" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Plot the RGB image, slope, habitat selection, and movement density</span></span>
<span id="cb95-105"><a href="#cb95-105" aria-hidden="true" tabindex="-1"></a>  <span class="co">#   Change the panels to visualize different layers</span></span>
<span id="cb95-106"><a href="#cb95-106" aria-hidden="true" tabindex="-1"></a>  <span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb95-107"><a href="#cb95-107" aria-hidden="true" tabindex="-1"></a>  fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb95-108"><a href="#cb95-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-109"><a href="#cb95-109" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Plot RGB</span></span>
<span id="cb95-110"><a href="#cb95-110" aria-hidden="true" tabindex="-1"></a>  im1 <span class="op">=</span> axs[<span class="dv">0</span>, <span class="dv">0</span>].imshow(rgb_image_np)</span>
<span id="cb95-111"><a href="#cb95-111" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">'Sentinel-2 RGB'</span>)</span>
<span id="cb95-112"><a href="#cb95-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-113"><a href="#cb95-113" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Plot slope</span></span>
<span id="cb95-114"><a href="#cb95-114" aria-hidden="true" tabindex="-1"></a>  im2 <span class="op">=</span> axs[<span class="dv">0</span>, <span class="dv">1</span>].imshow(x1.detach().cpu().numpy()[<span class="dv">0</span>,<span class="dv">12</span>,:,:], cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb95-115"><a href="#cb95-115" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">'Slope'</span>)</span>
<span id="cb95-116"><a href="#cb95-116" aria-hidden="true" tabindex="-1"></a>  fig.colorbar(im2, ax<span class="op">=</span>axs[<span class="dv">0</span>, <span class="dv">1</span>], shrink<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb95-117"><a href="#cb95-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-118"><a href="#cb95-118" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Plot habitat selection</span></span>
<span id="cb95-119"><a href="#cb95-119" aria-hidden="true" tabindex="-1"></a>  im3 <span class="op">=</span> axs[<span class="dv">1</span>, <span class="dv">0</span>].imshow(hab_density_mask, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb95-120"><a href="#cb95-120" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">1</span>, <span class="dv">0</span>].set_title(<span class="st">'Habitat selection log-probability'</span>)</span>
<span id="cb95-121"><a href="#cb95-121" aria-hidden="true" tabindex="-1"></a>  fig.colorbar(im3, ax<span class="op">=</span>axs[<span class="dv">1</span>, <span class="dv">0</span>], shrink<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb95-122"><a href="#cb95-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-123"><a href="#cb95-123" aria-hidden="true" tabindex="-1"></a>  <span class="co"># # Movement density (change the axis and uncomment one of the other panels)</span></span>
<span id="cb95-124"><a href="#cb95-124" aria-hidden="true" tabindex="-1"></a>  <span class="co"># im3 = axs[1, 0].imshow(move_density_mask, cmap='viridis')</span></span>
<span id="cb95-125"><a href="#cb95-125" aria-hidden="true" tabindex="-1"></a>  <span class="co"># axs[1, 0].set_title('Movement log-probability')</span></span>
<span id="cb95-126"><a href="#cb95-126" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fig.colorbar(im3, ax=axs[0, 1], shrink=0.7)</span></span>
<span id="cb95-127"><a href="#cb95-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-128"><a href="#cb95-128" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Next-step probability</span></span>
<span id="cb95-129"><a href="#cb95-129" aria-hidden="true" tabindex="-1"></a>  im4 <span class="op">=</span> axs[<span class="dv">1</span>, <span class="dv">1</span>].imshow(step_density_mask, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb95-130"><a href="#cb95-130" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">1</span>, <span class="dv">1</span>].set_title(<span class="st">'Next-step log-probability'</span>)</span>
<span id="cb95-131"><a href="#cb95-131" aria-hidden="true" tabindex="-1"></a>  fig.colorbar(im4, ax<span class="op">=</span>axs[<span class="dv">1</span>, <span class="dv">1</span>], shrink<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb95-132"><a href="#cb95-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-133"><a href="#cb95-133" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Save the figure</span></span>
<span id="cb95-134"><a href="#cb95-134" aria-hidden="true" tabindex="-1"></a>  filename_covs <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>output_dir<span class="sc">}</span><span class="ss">/deepSSF_S2_slope_id</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_yday</span><span class="sc">{</span>yday_t2_integer<span class="sc">}</span><span class="ss">_hour</span><span class="sc">{</span>hour_t2_integer<span class="sc">}</span><span class="ss">_bearing</span><span class="sc">{</span>bearing_degrees<span class="sc">}</span><span class="ss">_next_r</span><span class="sc">{</span>row<span class="sc">}</span><span class="ss">_c</span><span class="sc">{</span>column<span class="sc">}</span><span class="ss">.png'</span></span>
<span id="cb95-135"><a href="#cb95-135" aria-hidden="true" tabindex="-1"></a>  plt.tight_layout()</span>
<span id="cb95-136"><a href="#cb95-136" aria-hidden="true" tabindex="-1"></a>  plt.savefig(filename_covs, dpi<span class="op">=</span><span class="dv">600</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>)</span>
<span id="cb95-137"><a href="#cb95-137" aria-hidden="true" tabindex="-1"></a>  plt.show()</span>
<span id="cb95-138"><a href="#cb95-138" aria-hidden="true" tabindex="-1"></a>  plt.close()  <span class="co"># Close the figure to free memory</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Hour:                        8
Day of the year:             175
Bearing (radians):           2.2515599727630615
Bearing (degrees):           129
Next step is (row, column):  (52, 45)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-69-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Hour:                        17
Day of the year:             217
Bearing (radians):           -0.1754153072834015
Bearing (degrees):           349
Next step is (row, column):  (48, 50)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-69-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Hour:                        3
Day of the year:             9
Bearing (radians):           -1.8308851718902588
Bearing (degrees):           255
Next step is (row, column):  (50, 50)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-69-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Hour:                        10
Day of the year:             224
Bearing (radians):           2.1292378902435303
Bearing (degrees):           122
Next step is (row, column):  (49, 62)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-69-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Hour:                        14
Day of the year:             7
Bearing (radians):           2.781324625015259
Bearing (degrees):           159
Next step is (row, column):  (49, 50)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-69-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="extracting-convolution-layer-outputs" class="level1">
<h1>Extracting convolution layer outputs</h1>
<p>In the convolutional blocks, each convolutional layer learns a set of <strong>filters</strong> (kernels) that extract different features from the input data. In the habitat selection subnetwork, the convolution filters (and their associated bias parameters - not shown below) are the only parameters that are trained, and it is the filters that transform the set of input covariates into the habitat selection probabilities. They do this by maximising features of the inputs that correlate with observed next-steps.</p>
<p>For each convolutional layer, there are typically a number of filters. For the habitat selection subnetwork, we used 4 filters in the first two layers, and a single filter in the last layer. Each of these filters has a number of <strong>channels</strong> which correspond one-to-one with the input layers. The outputs of the filter channels are then combined to produce a feature map, with a single feature map produced for each filter. In successive layers, the feature maps become the input layers, and the filters operate on these layers. Because there are multiple filters in ech layer, they can â€˜specialiseâ€™ in extracting different features from the input layers.</p>
<p>By visualizing and inspecting these filters, and the corresponding feature maps, we can:</p>
<ul>
<li>Gain interpretability: Understand what kind of features the network is detectingâ€”e.g., edges, shapes, or textures.</li>
<li>Debug: Check if the filters have meaningful patterns or if something went wrong (e.g., all zeros or random noise).</li>
<li>Compare layers: See how early layers often learn low-level patterns while deeper layers learn more abstract features.</li>
</ul>
<p>We will first set up some activation hooks for storing the feature maps. Activation hooks are placed at certain points within the modelâ€™s forward pass and store intermediate results. We will also extract the convolution filters (which are weights of the model and as such donâ€™t require hooks - we can access them directly).</p>
<p>We will then run the sample covariates through the model and extract the feature maps from the habitat selection convolutional block, and plot them along with the covariates and convolution filters.</p>
<p>Note that there are also ReLU activation functions in the convolutional blocks, which are not shown below. These are applied to the feature maps, and set all negative values to zero. They are not learned parameters, but are part of the forward pass of the model.</p>
<section id="create-scalar-grids-for-plotting" class="level3">
<h3 class="anchored" data-anchor-id="create-scalar-grids-for-plotting">Create scalar grids for plotting</h3>
<p>Using the <code>Scalar_to_Grid_Block</code> class from the <code>deepSSF_model</code> script, we can convert the scalar covariates into grids for plotting.</p>
<div id="cell-114" class="cell" data-execution_count="69">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an instance of the scalar-to-grid block using model parameters</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>scalar_to_grid_block <span class="op">=</span> deepSSF_model.Scalar_to_Grid_Block(params)</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert scalars into spatial grid representation</span></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>scalar_maps <span class="op">=</span> scalar_to_grid_block(x2)</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(scalar_maps.shape)  <span class="co"># Check the shape of the generated spatial maps</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([32, 4, 101, 101])</code></pre>
</div>
</div>
</section>
<section id="convolutional-layer-1" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-layer-1">Convolutional layer 1</h2>
<section id="activation-hook" class="level3">
<h3 class="anchored" data-anchor-id="activation-hook">Activation hook</h3>
<div id="cell-116" class="cell" data-execution_count="70">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dictionary to store activation outputs</span></span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a>activation <span class="op">=</span> {}</span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_activation(name):</span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb103-8"><a href="#cb103-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns a hook function that can be registered on a layer </span></span>
<span id="cb103-9"><a href="#cb103-9" aria-hidden="true" tabindex="-1"></a><span class="co">    to capture its output (i.e., feature maps) after the forward pass.</span></span>
<span id="cb103-10"><a href="#cb103-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-11"><a href="#cb103-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb103-12"><a href="#cb103-12" aria-hidden="true" tabindex="-1"></a><span class="co">        name (str): The key under which the activation is stored in the 'activation' dict.</span></span>
<span id="cb103-13"><a href="#cb103-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb103-14"><a href="#cb103-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hook(model, <span class="bu">input</span>, output):</span>
<span id="cb103-15"><a href="#cb103-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Detach and save the layer's output in the dictionary</span></span>
<span id="cb103-16"><a href="#cb103-16" aria-hidden="true" tabindex="-1"></a>        activation[name] <span class="op">=</span> output.detach()</span>
<span id="cb103-17"><a href="#cb103-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> hook</span>
<span id="cb103-18"><a href="#cb103-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-19"><a href="#cb103-19" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb103-20"><a href="#cb103-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Register a forward hook on the first convolution layer </span></span>
<span id="cb103-21"><a href="#cb103-21" aria-hidden="true" tabindex="-1"></a><span class="co">#    in the model's 'conv_habitat' block</span></span>
<span id="cb103-22"><a href="#cb103-22" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb103-23"><a href="#cb103-23" aria-hidden="true" tabindex="-1"></a>model.conv_habitat.conv2d[<span class="dv">0</span>].register_forward_hook(get_activation(<span class="st">"hab_conv1"</span>))</span>
<span id="cb103-24"><a href="#cb103-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-25"><a href="#cb103-25" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb103-26"><a href="#cb103-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform a forward pass through the model with the desired input</span></span>
<span id="cb103-27"><a href="#cb103-27" aria-hidden="true" tabindex="-1"></a><span class="co">#    The feature maps from the hooked layer will be stored in 'activation'</span></span>
<span id="cb103-28"><a href="#cb103-28" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb103-29"><a href="#cb103-29" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> model((x1, x2, x3))  <span class="co"># e.g., model((spatial_data_x, scalars_to_grid, bearing_x))</span></span>
<span id="cb103-30"><a href="#cb103-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-31"><a href="#cb103-31" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb103-32"><a href="#cb103-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve the captured feature maps from the dictionary</span></span>
<span id="cb103-33"><a href="#cb103-33" aria-hidden="true" tabindex="-1"></a><span class="co">#    and move them to the CPU for inspection</span></span>
<span id="cb103-34"><a href="#cb103-34" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb103-35"><a href="#cb103-35" aria-hidden="true" tabindex="-1"></a>feat_maps1 <span class="op">=</span> activation[<span class="st">"hab_conv1"</span>].cpu()</span>
<span id="cb103-36"><a href="#cb103-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Feature map shape:"</span>, feat_maps1.shape)</span>
<span id="cb103-37"><a href="#cb103-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Typically shape: (batch_size, out_channels, height, width)</span></span>
<span id="cb103-38"><a href="#cb103-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-39"><a href="#cb103-39" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb103-40"><a href="#cb103-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the feature maps for the first sample in the batch</span></span>
<span id="cb103-41"><a href="#cb103-41" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb103-42"><a href="#cb103-42" aria-hidden="true" tabindex="-1"></a>feat_maps1_sample <span class="op">=</span> feat_maps1[<span class="dv">0</span>]  <span class="co"># Shape: (out_channels, H, W)</span></span>
<span id="cb103-43"><a href="#cb103-43" aria-hidden="true" tabindex="-1"></a>num_maps1 <span class="op">=</span> feat_maps1_sample.shape[<span class="dv">0</span>]</span>
<span id="cb103-44"><a href="#cb103-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of feature maps:"</span>, num_maps1)</span>
<span id="cb103-45"><a href="#cb103-45" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Feature map shape: torch.Size([32, 4, 101, 101])
Number of feature maps: 4</code></pre>
</div>
</div>
</section>
<section id="stack-spatial-and-scalar-as-grid-covariates" class="level3">
<h3 class="anchored" data-anchor-id="stack-spatial-and-scalar-as-grid-covariates">Stack spatial and scalar (as grid) covariates</h3>
<p>For plotting. Also create a vector of names to index over.</p>
<div id="cell-118" class="cell" data-execution_count="71">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>covariate_stack <span class="op">=</span> torch.cat([x1, scalar_maps], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(covariate_stack.shape)</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>covariate_names <span class="op">=</span> [<span class="st">'S2 B1'</span>,</span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'S2 B2'</span>,</span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'S2 B3'</span>,</span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'S2 B4'</span>,</span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'S2 B5'</span>,</span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'S2 B6'</span>,</span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'S2 B7'</span>,</span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'S2 B8'</span>,</span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'S2 B8a'</span>,</span>
<span id="cb105-13"><a href="#cb105-13" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'S2 B9'</span>,</span>
<span id="cb105-14"><a href="#cb105-14" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'S2 B11'</span>,</span>
<span id="cb105-15"><a href="#cb105-15" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'S2 B12'</span>,</span>
<span id="cb105-16"><a href="#cb105-16" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'Slope'</span>, </span>
<span id="cb105-17"><a href="#cb105-17" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'Hour sin'</span>, </span>
<span id="cb105-18"><a href="#cb105-18" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'Hour cos'</span>, </span>
<span id="cb105-19"><a href="#cb105-19" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'yday sin'</span>, </span>
<span id="cb105-20"><a href="#cb105-20" aria-hidden="true" tabindex="-1"></a>                   <span class="st">'yday cos'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([32, 17, 101, 101])</code></pre>
</div>
</div>
</section>
<section id="extract-filters-and-plot" class="level3">
<h3 class="anchored" data-anchor-id="extract-filters-and-plot">Extract filters and plot</h3>
<div id="cell-120" class="cell" data-execution_count="72">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Check or print the convolution layer in conv_habitat (for debugging)</span></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.conv_habitat.conv2d)</span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the model to evaluation mode (disables dropout, etc.)</span></span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb107-12"><a href="#cb107-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the weights (filters) from the first convolution layer in conv_habitat</span></span>
<span id="cb107-13"><a href="#cb107-13" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb107-14"><a href="#cb107-14" aria-hidden="true" tabindex="-1"></a>filters_c1 <span class="op">=</span> model.conv_habitat.conv2d[<span class="dv">0</span>].weight.data.clone().cpu()</span>
<span id="cb107-15"><a href="#cb107-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Filters shape:"</span>, filters_c1.shape)</span>
<span id="cb107-16"><a href="#cb107-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Typically (out_channels, in_channels, kernel_height, kernel_width)</span></span>
<span id="cb107-17"><a href="#cb107-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-18"><a href="#cb107-18" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb107-19"><a href="#cb107-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize each filterâ€™s first channel in a grid of subplots</span></span>
<span id="cb107-20"><a href="#cb107-20" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb107-21"><a href="#cb107-21" aria-hidden="true" tabindex="-1"></a>num_filters_c1 <span class="op">=</span> filters_c1.shape[<span class="dv">1</span>]</span>
<span id="cb107-22"><a href="#cb107-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(num_filters_c1)</span>
<span id="cb107-23"><a href="#cb107-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-24"><a href="#cb107-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> z <span class="kw">in</span> <span class="bu">range</span>(num_maps1):</span>
<span id="cb107-25"><a href="#cb107-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-26"><a href="#cb107-26" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, num_filters_c1, figsize<span class="op">=</span>(<span class="dv">2</span><span class="op">*</span>num_filters_c1, <span class="dv">4</span>))</span>
<span id="cb107-27"><a href="#cb107-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_filters_c1):</span>
<span id="cb107-28"><a href="#cb107-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-29"><a href="#cb107-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add the covariates as the first row of subplots</span></span>
<span id="cb107-30"><a href="#cb107-30" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>,i].imshow(covariate_stack[<span class="dv">0</span>, i].detach().cpu().numpy(), cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb107-31"><a href="#cb107-31" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>,i].axis(<span class="st">'off'</span>)</span>
<span id="cb107-32"><a href="#cb107-32" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>,i].set_title(<span class="ss">f'</span><span class="sc">{</span>covariate_names[i]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb107-33"><a href="#cb107-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">&gt;</span> x1.shape[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb107-34"><a href="#cb107-34" aria-hidden="true" tabindex="-1"></a>            im1 <span class="op">=</span> axes[<span class="dv">0</span>,i].imshow(covariate_stack[<span class="dv">0</span>, i].detach().cpu().numpy(), cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb107-35"><a href="#cb107-35" aria-hidden="true" tabindex="-1"></a>            im1.set_clim(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb107-36"><a href="#cb107-36" aria-hidden="true" tabindex="-1"></a>            axes[<span class="dv">0</span>,i].text(scalar_maps.shape[<span class="dv">2</span>] <span class="op">//</span> <span class="dv">2</span>, scalar_maps.shape[<span class="dv">3</span>] <span class="op">//</span> <span class="dv">2</span>, </span>
<span id="cb107-37"><a href="#cb107-37" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f'Value: </span><span class="sc">{</span><span class="bu">round</span>(x2[<span class="dv">0</span>, i<span class="op">-</span>x1.shape[<span class="dv">1</span>]].item(), <span class="dv">2</span>)<span class="sc">}</span><span class="ss">'</span>, </span>
<span id="cb107-38"><a href="#cb107-38" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb107-39"><a href="#cb107-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-40"><a href="#cb107-40" aria-hidden="true" tabindex="-1"></a>        kernel <span class="op">=</span> filters_c1[z, i, :, :]  <span class="co"># Show the first input channel</span></span>
<span id="cb107-41"><a href="#cb107-41" aria-hidden="true" tabindex="-1"></a>        im <span class="op">=</span> axes[<span class="dv">1</span>,i].imshow(kernel, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb107-42"><a href="#cb107-42" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>,i].axis(<span class="st">'off'</span>)</span>
<span id="cb107-43"><a href="#cb107-43" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>,i].set_title(<span class="ss">f'Layer 1, Filter </span><span class="sc">{</span>z<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb107-44"><a href="#cb107-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Annotate each cell with the numeric value</span></span>
<span id="cb107-45"><a href="#cb107-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (j, k), val <span class="kw">in</span> np.ndenumerate(kernel):</span>
<span id="cb107-46"><a href="#cb107-46" aria-hidden="true" tabindex="-1"></a>            axes[<span class="dv">1</span>,i].text(k, j, <span class="ss">f'</span><span class="sc">{</span>val<span class="sc">:.2f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb107-47"><a href="#cb107-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-48"><a href="#cb107-48" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb107-49"><a href="#cb107-49" aria-hidden="true" tabindex="-1"></a>    plt.savefig(<span class="ss">f'</span><span class="sc">{</span>output_dir<span class="sc">}</span><span class="ss">/id</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_conv_layer1_filters</span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>today_date<span class="sc">}</span><span class="ss">.png'</span>, dpi<span class="op">=</span><span class="dv">600</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>)</span>
<span id="cb107-50"><a href="#cb107-50" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb107-51"><a href="#cb107-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-52"><a href="#cb107-52" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb107-53"><a href="#cb107-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -----------------------------------------------------------</span></span>
<span id="cb107-54"><a href="#cb107-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop over each feature map channel and save them as images.</span></span>
<span id="cb107-55"><a href="#cb107-55" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    Multiply by x_mask * y_mask if you need to mask out edges.</span></span>
<span id="cb107-56"><a href="#cb107-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -----------------------------------------------------------</span></span>
<span id="cb107-57"><a href="#cb107-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-58"><a href="#cb107-58" aria-hidden="true" tabindex="-1"></a>    plt.figure()</span>
<span id="cb107-59"><a href="#cb107-59" aria-hidden="true" tabindex="-1"></a>    plt.imshow(feat_maps1_sample[z].numpy() <span class="op">*</span> x_mask <span class="op">*</span> y_mask, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb107-60"><a href="#cb107-60" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"Layer 1, Feature Map </span><span class="sc">{</span>z<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb107-61"><a href="#cb107-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hide axis if you prefer: plt.axis('off')</span></span>
<span id="cb107-62"><a href="#cb107-62" aria-hidden="true" tabindex="-1"></a>    plt.savefig(<span class="ss">f'</span><span class="sc">{</span>output_dir<span class="sc">}</span><span class="ss">/id</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_conv_layer1_feature_map</span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>today_date<span class="sc">}</span><span class="ss">.png'</span>, dpi<span class="op">=</span><span class="dv">600</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>)</span>
<span id="cb107-63"><a href="#cb107-63" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb107-64"><a href="#cb107-64" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Sequential(
  (0): Conv2d(17, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
  (4): Conv2d(4, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
Filters shape: torch.Size([4, 17, 3, 3])
17</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-73-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-73-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-73-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-73-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-73-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-73-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-73-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-73-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="convolutional-layer-2" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-layer-2">Convolutional layer 2</h2>
<section id="activation-hook-1" class="level3">
<h3 class="anchored" data-anchor-id="activation-hook-1">Activation hook</h3>
<div id="cell-122" class="cell" data-execution_count="73">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Register a forward hook on the second convolution layer </span></span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a><span class="co">#    in the model's 'conv_habitat' block</span></span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a>model.conv_habitat.conv2d[<span class="dv">2</span>].register_forward_hook(get_activation(<span class="st">"hab_conv2"</span>))</span>
<span id="cb109-6"><a href="#cb109-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-7"><a href="#cb109-7" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb109-8"><a href="#cb109-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform a forward pass through the model with the desired input</span></span>
<span id="cb109-9"><a href="#cb109-9" aria-hidden="true" tabindex="-1"></a><span class="co">#    The feature maps from the hooked layer will be stored in 'activation'</span></span>
<span id="cb109-10"><a href="#cb109-10" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb109-11"><a href="#cb109-11" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> model((x1, x2, x3))  <span class="co"># e.g., model((spatial_data_x, scalars_to_grid, bearing_x))</span></span>
<span id="cb109-12"><a href="#cb109-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-13"><a href="#cb109-13" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb109-14"><a href="#cb109-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve the captured feature maps from the dictionary</span></span>
<span id="cb109-15"><a href="#cb109-15" aria-hidden="true" tabindex="-1"></a><span class="co">#    and move them to the CPU for inspection</span></span>
<span id="cb109-16"><a href="#cb109-16" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb109-17"><a href="#cb109-17" aria-hidden="true" tabindex="-1"></a>feat_maps2 <span class="op">=</span> activation[<span class="st">"hab_conv2"</span>].cpu()</span>
<span id="cb109-18"><a href="#cb109-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Feature map shape:"</span>, feat_maps2.shape)</span>
<span id="cb109-19"><a href="#cb109-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Typically shape: (batch_size, out_channels, height, width)</span></span>
<span id="cb109-20"><a href="#cb109-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-21"><a href="#cb109-21" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb109-22"><a href="#cb109-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the feature maps for the first sample in the batch</span></span>
<span id="cb109-23"><a href="#cb109-23" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb109-24"><a href="#cb109-24" aria-hidden="true" tabindex="-1"></a>feat_maps2_sample <span class="op">=</span> feat_maps2[<span class="dv">0</span>]  <span class="co"># Shape: (out_channels, H, W)</span></span>
<span id="cb109-25"><a href="#cb109-25" aria-hidden="true" tabindex="-1"></a>num_maps2 <span class="op">=</span> feat_maps2_sample.shape[<span class="dv">0</span>]</span>
<span id="cb109-26"><a href="#cb109-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of feature maps:"</span>, num_maps2)</span>
<span id="cb109-27"><a href="#cb109-27" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Feature map shape: torch.Size([32, 4, 101, 101])
Number of feature maps: 4</code></pre>
</div>
</div>
</section>
<section id="extract-filters-and-plot-1" class="level3">
<h3 class="anchored" data-anchor-id="extract-filters-and-plot-1">Extract filters and plot</h3>
<div id="cell-124" class="cell" data-execution_count="74">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the weights (filters) from the second convolution layer in conv_habitat</span></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>filters_c2 <span class="op">=</span> model.conv_habitat.conv2d[<span class="dv">2</span>].weight.data.clone().cpu()</span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Filters shape:"</span>, filters_c2.shape)</span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Typically (out_channels, in_channels, kernel_height, kernel_width)</span></span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize each filterâ€™s first channel in a grid of subplots</span></span>
<span id="cb111-10"><a href="#cb111-10" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb111-11"><a href="#cb111-11" aria-hidden="true" tabindex="-1"></a>num_filters_c2 <span class="op">=</span> filters_c2.shape[<span class="dv">1</span>]</span>
<span id="cb111-12"><a href="#cb111-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(num_filters_c2)</span>
<span id="cb111-13"><a href="#cb111-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-14"><a href="#cb111-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> z <span class="kw">in</span> <span class="bu">range</span>(num_maps2):</span>
<span id="cb111-15"><a href="#cb111-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-16"><a href="#cb111-16" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, num_filters_c2, figsize<span class="op">=</span>(<span class="dv">2</span><span class="op">*</span>num_filters_c2, <span class="dv">4</span>))</span>
<span id="cb111-17"><a href="#cb111-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_filters_c2):</span>
<span id="cb111-18"><a href="#cb111-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-19"><a href="#cb111-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add the covariates as the first row of subplots</span></span>
<span id="cb111-20"><a href="#cb111-20" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>,i].imshow(feat_maps1_sample[i].numpy() <span class="op">*</span> x_mask <span class="op">*</span> y_mask, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb111-21"><a href="#cb111-21" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>,i].axis(<span class="st">'off'</span>)</span>
<span id="cb111-22"><a href="#cb111-22" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>,i].set_title(<span class="ss">f"Layer 1, Map </span><span class="sc">{</span>z<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb111-23"><a href="#cb111-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-24"><a href="#cb111-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if i &gt; 3:</span></span>
<span id="cb111-25"><a href="#cb111-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     im1 = axes[0,i].imshow(covariate_stack[0, i].detach().cpu().numpy(), cmap='viridis')</span></span>
<span id="cb111-26"><a href="#cb111-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     im1.set_clim(-1, 1)</span></span>
<span id="cb111-27"><a href="#cb111-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     axes[0,i].text(scalar_maps.shape[2] // 2, scalar_maps.shape[3] // 2, </span></span>
<span id="cb111-28"><a href="#cb111-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">#         f'Value: {round(x2[0, i-4].item(), 2)}', </span></span>
<span id="cb111-29"><a href="#cb111-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">#         ha='center', va='center', color='white', fontsize=12)</span></span>
<span id="cb111-30"><a href="#cb111-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-31"><a href="#cb111-31" aria-hidden="true" tabindex="-1"></a>        kernel <span class="op">=</span> filters_c2[z, i, :, :]  <span class="co"># Show the first input channel</span></span>
<span id="cb111-32"><a href="#cb111-32" aria-hidden="true" tabindex="-1"></a>        im <span class="op">=</span> axes[<span class="dv">1</span>,i].imshow(kernel, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb111-33"><a href="#cb111-33" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>,i].axis(<span class="st">'off'</span>)</span>
<span id="cb111-34"><a href="#cb111-34" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>,i].set_title(<span class="ss">f'Layer 2, Filter </span><span class="sc">{</span>z<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb111-35"><a href="#cb111-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Annotate each cell with the numeric value</span></span>
<span id="cb111-36"><a href="#cb111-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (j, k), val <span class="kw">in</span> np.ndenumerate(kernel):</span>
<span id="cb111-37"><a href="#cb111-37" aria-hidden="true" tabindex="-1"></a>            axes[<span class="dv">1</span>,i].text(k, j, <span class="ss">f'</span><span class="sc">{</span>val<span class="sc">:.2f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb111-38"><a href="#cb111-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-39"><a href="#cb111-39" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb111-40"><a href="#cb111-40" aria-hidden="true" tabindex="-1"></a>    plt.savefig(<span class="ss">f'</span><span class="sc">{</span>output_dir<span class="sc">}</span><span class="ss">/id</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_conv_layer2_filters</span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>today_date<span class="sc">}</span><span class="ss">.png'</span>, dpi<span class="op">=</span><span class="dv">600</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>)</span>
<span id="cb111-41"><a href="#cb111-41" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb111-42"><a href="#cb111-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-43"><a href="#cb111-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb111-44"><a href="#cb111-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -----------------------------------------------------------</span></span>
<span id="cb111-45"><a href="#cb111-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6. Loop over each feature map channel and save them as images.</span></span>
<span id="cb111-46"><a href="#cb111-46" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    Multiply by x_mask * y_mask if you need to mask out edges.</span></span>
<span id="cb111-47"><a href="#cb111-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -----------------------------------------------------------</span></span>
<span id="cb111-48"><a href="#cb111-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-49"><a href="#cb111-49" aria-hidden="true" tabindex="-1"></a>    plt.figure()</span>
<span id="cb111-50"><a href="#cb111-50" aria-hidden="true" tabindex="-1"></a>    plt.imshow(feat_maps2_sample[z].numpy() <span class="op">*</span> x_mask <span class="op">*</span> y_mask, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb111-51"><a href="#cb111-51" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"Layer 2, Feature Map </span><span class="sc">{</span>z<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb111-52"><a href="#cb111-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hide axis if you prefer: plt.axis('off')</span></span>
<span id="cb111-53"><a href="#cb111-53" aria-hidden="true" tabindex="-1"></a>    plt.savefig(<span class="ss">f'</span><span class="sc">{</span>output_dir<span class="sc">}</span><span class="ss">/id</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_conv_layer2_feature_map</span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>today_date<span class="sc">}</span><span class="ss">.png'</span>, dpi<span class="op">=</span><span class="dv">600</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>)</span>
<span id="cb111-54"><a href="#cb111-54" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb111-55"><a href="#cb111-55" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Filters shape: torch.Size([4, 4, 3, 3])
4</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-75-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-75-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-75-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-75-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-75-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-75-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-75-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-75-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="convolutional-layer-3" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-layer-3">Convolutional layer 3</h2>
<section id="activation-hook-2" class="level3">
<h3 class="anchored" data-anchor-id="activation-hook-2">Activation hook</h3>
<div id="cell-126" class="cell" data-execution_count="75">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Register a forward hook on the third convolution layer </span></span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a><span class="co">#    in the model's 'conv_habitat' block</span></span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a>model.conv_habitat.conv2d[<span class="dv">4</span>].register_forward_hook(get_activation(<span class="st">"hab_conv3"</span>))</span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-7"><a href="#cb113-7" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb113-8"><a href="#cb113-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform a forward pass through the model with the desired input</span></span>
<span id="cb113-9"><a href="#cb113-9" aria-hidden="true" tabindex="-1"></a><span class="co">#    The feature maps from the hooked layer will be stored in 'activation'</span></span>
<span id="cb113-10"><a href="#cb113-10" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb113-11"><a href="#cb113-11" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> model((x1, x2, x3))  <span class="co"># e.g., model((spatial_data_x, scalars_to_grid, bearing_x))</span></span>
<span id="cb113-12"><a href="#cb113-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-13"><a href="#cb113-13" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb113-14"><a href="#cb113-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve the captured feature maps from the dictionary</span></span>
<span id="cb113-15"><a href="#cb113-15" aria-hidden="true" tabindex="-1"></a><span class="co">#    and move them to the CPU for inspection</span></span>
<span id="cb113-16"><a href="#cb113-16" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb113-17"><a href="#cb113-17" aria-hidden="true" tabindex="-1"></a>feat_maps3 <span class="op">=</span> activation[<span class="st">"hab_conv3"</span>].cpu()</span>
<span id="cb113-18"><a href="#cb113-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Feature map shape:"</span>, feat_maps3.shape)</span>
<span id="cb113-19"><a href="#cb113-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Typically shape: (batch_size, out_channels, height, width)</span></span>
<span id="cb113-20"><a href="#cb113-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-21"><a href="#cb113-21" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb113-22"><a href="#cb113-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the feature maps for the first sample in the batch</span></span>
<span id="cb113-23"><a href="#cb113-23" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb113-24"><a href="#cb113-24" aria-hidden="true" tabindex="-1"></a>feat_maps3_sample <span class="op">=</span> feat_maps3[<span class="dv">0</span>]  <span class="co"># Shape: (out_channels, H, W)</span></span>
<span id="cb113-25"><a href="#cb113-25" aria-hidden="true" tabindex="-1"></a>num_maps3 <span class="op">=</span> feat_maps3_sample.shape[<span class="dv">0</span>]</span>
<span id="cb113-26"><a href="#cb113-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of feature maps:"</span>, num_maps3)</span>
<span id="cb113-27"><a href="#cb113-27" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Feature map shape: torch.Size([32, 1, 101, 101])
Number of feature maps: 1</code></pre>
</div>
</div>
</section>
<section id="extract-filters-and-plot-2" class="level3">
<h3 class="anchored" data-anchor-id="extract-filters-and-plot-2">Extract filters and plot</h3>
<div id="cell-128" class="cell" data-execution_count="76">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the weights (filters) from the second convolution layer in conv_habitat</span></span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>filters_c3 <span class="op">=</span> model.conv_habitat.conv2d[<span class="dv">4</span>].weight.data.clone().cpu()</span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Filters shape:"</span>, filters_c3.shape)</span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Typically (out_channels, in_channels, kernel_height, kernel_width)</span></span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb115-9"><a href="#cb115-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize each filterâ€™s first channel in a grid of subplots</span></span>
<span id="cb115-10"><a href="#cb115-10" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb115-11"><a href="#cb115-11" aria-hidden="true" tabindex="-1"></a>num_filters_c3 <span class="op">=</span> filters_c3.shape[<span class="dv">1</span>]</span>
<span id="cb115-12"><a href="#cb115-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(num_filters_c3)</span>
<span id="cb115-13"><a href="#cb115-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-14"><a href="#cb115-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> z <span class="kw">in</span> <span class="bu">range</span>(num_maps3):</span>
<span id="cb115-15"><a href="#cb115-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-16"><a href="#cb115-16" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, num_filters_c3, figsize<span class="op">=</span>(<span class="dv">2</span><span class="op">*</span>num_filters_c3, <span class="dv">4</span>))</span>
<span id="cb115-17"><a href="#cb115-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_filters_c3):</span>
<span id="cb115-18"><a href="#cb115-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-19"><a href="#cb115-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add the covariates as the first row of subplots</span></span>
<span id="cb115-20"><a href="#cb115-20" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>,i].imshow(feat_maps2_sample[i].numpy() <span class="op">*</span> x_mask <span class="op">*</span> y_mask, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb115-21"><a href="#cb115-21" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>,i].axis(<span class="st">'off'</span>)</span>
<span id="cb115-22"><a href="#cb115-22" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>,i].set_title(<span class="ss">f"Layer 2, Map </span><span class="sc">{</span>z<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb115-23"><a href="#cb115-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-24"><a href="#cb115-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-25"><a href="#cb115-25" aria-hidden="true" tabindex="-1"></a>        kernel <span class="op">=</span> filters_c3[z, i, :, :]  <span class="co"># Show the first input channel</span></span>
<span id="cb115-26"><a href="#cb115-26" aria-hidden="true" tabindex="-1"></a>        im <span class="op">=</span> axes[<span class="dv">1</span>,i].imshow(kernel, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb115-27"><a href="#cb115-27" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>,i].axis(<span class="st">'off'</span>)</span>
<span id="cb115-28"><a href="#cb115-28" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>,i].set_title(<span class="ss">f'Layer 3, Filter </span><span class="sc">{</span>z<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb115-29"><a href="#cb115-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Annotate each cell with the numeric value</span></span>
<span id="cb115-30"><a href="#cb115-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (j, k), val <span class="kw">in</span> np.ndenumerate(kernel):</span>
<span id="cb115-31"><a href="#cb115-31" aria-hidden="true" tabindex="-1"></a>            axes[<span class="dv">1</span>,i].text(k, j, <span class="ss">f'</span><span class="sc">{</span>val<span class="sc">:.2f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb115-32"><a href="#cb115-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-33"><a href="#cb115-33" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb115-34"><a href="#cb115-34" aria-hidden="true" tabindex="-1"></a>    plt.savefig(<span class="ss">f'</span><span class="sc">{</span>output_dir<span class="sc">}</span><span class="ss">/id</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_conv_layer3_filters</span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>today_date<span class="sc">}</span><span class="ss">.png'</span>, dpi<span class="op">=</span><span class="dv">600</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>)</span>
<span id="cb115-35"><a href="#cb115-35" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb115-36"><a href="#cb115-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-37"><a href="#cb115-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb115-38"><a href="#cb115-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -----------------------------------------------------------</span></span>
<span id="cb115-39"><a href="#cb115-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6. Loop over each feature map channel and save them as images.</span></span>
<span id="cb115-40"><a href="#cb115-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    Multiply by x_mask * y_mask if you need to mask out edges.</span></span>
<span id="cb115-41"><a href="#cb115-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -----------------------------------------------------------</span></span>
<span id="cb115-42"><a href="#cb115-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-43"><a href="#cb115-43" aria-hidden="true" tabindex="-1"></a>    plt.figure()</span>
<span id="cb115-44"><a href="#cb115-44" aria-hidden="true" tabindex="-1"></a>    plt.imshow(feat_maps3_sample[z].numpy() <span class="op">*</span> x_mask <span class="op">*</span> y_mask, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb115-45"><a href="#cb115-45" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"Habitat selection log probability"</span>)</span>
<span id="cb115-46"><a href="#cb115-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hide axis if you prefer: plt.axis('off')</span></span>
<span id="cb115-47"><a href="#cb115-47" aria-hidden="true" tabindex="-1"></a>    plt.savefig(<span class="ss">f'</span><span class="sc">{</span>output_dir<span class="sc">}</span><span class="ss">/id</span><span class="sc">{</span>buffalo_id<span class="sc">}</span><span class="ss">_conv_layer3_feature_map</span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>today_date<span class="sc">}</span><span class="ss">.png'</span>, dpi<span class="op">=</span><span class="dv">600</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>)</span>
<span id="cb115-48"><a href="#cb115-48" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb115-49"><a href="#cb115-49" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Filters shape: torch.Size([1, 4, 3, 3])
4</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-77-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-77-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="checking-estimated-movement-parameters" class="level1">
<h1>Checking estimated movement parameters</h1>
<p>Similarly to the convolutional layers, we can set hooks to extract the predicted movement parameters from the model, and assess how variable that is across samples.</p>
<div id="cell-130" class="cell" data-execution_count="77">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list to store the intermediate output from the fully connected</span></span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a><span class="co">#    movement sub-network (fcn_movement_all)</span></span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a>intermediate_output <span class="op">=</span> []</span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> hook(module, <span class="bu">input</span>, output):</span>
<span id="cb117-8"><a href="#cb117-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb117-9"><a href="#cb117-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Hook function that captures the output of the specified layer</span></span>
<span id="cb117-10"><a href="#cb117-10" aria-hidden="true" tabindex="-1"></a><span class="co">    (fcn_movement_all) during the forward pass.</span></span>
<span id="cb117-11"><a href="#cb117-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb117-12"><a href="#cb117-12" aria-hidden="true" tabindex="-1"></a>    intermediate_output.append(output)</span>
<span id="cb117-13"><a href="#cb117-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-14"><a href="#cb117-14" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb117-15"><a href="#cb117-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Register the forward hook on 'fcn_movement_all', so its outputs</span></span>
<span id="cb117-16"><a href="#cb117-16" aria-hidden="true" tabindex="-1"></a><span class="co">#    are recorded every time the model does a forward pass.</span></span>
<span id="cb117-17"><a href="#cb117-17" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb117-18"><a href="#cb117-18" aria-hidden="true" tabindex="-1"></a>hook_handle <span class="op">=</span> model.fcn_movement_all.register_forward_hook(hook)</span>
<span id="cb117-19"><a href="#cb117-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-20"><a href="#cb117-20" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb117-21"><a href="#cb117-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform a forward pass with the model in evaluation mode, </span></span>
<span id="cb117-22"><a href="#cb117-22" aria-hidden="true" tabindex="-1"></a><span class="co">#    disabling gradient computation.</span></span>
<span id="cb117-23"><a href="#cb117-23" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb117-24"><a href="#cb117-24" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb117-25"><a href="#cb117-25" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb117-26"><a href="#cb117-26" aria-hidden="true" tabindex="-1"></a>    final_output <span class="op">=</span> model((x1, x2, x3))</span>
<span id="cb117-27"><a href="#cb117-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-28"><a href="#cb117-28" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb117-29"><a href="#cb117-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect the captured intermediate output</span></span>
<span id="cb117-30"><a href="#cb117-30" aria-hidden="true" tabindex="-1"></a><span class="co">#    'intermediate_output[0]' corresponds to the first (and only) forward pass.</span></span>
<span id="cb117-31"><a href="#cb117-31" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb117-32"><a href="#cb117-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Intermediate output shape:"</span>, intermediate_output[<span class="dv">0</span>].shape)</span>
<span id="cb117-33"><a href="#cb117-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Intermediate output values:"</span>, intermediate_output[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb117-34"><a href="#cb117-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-35"><a href="#cb117-35" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb117-36"><a href="#cb117-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove the hook to avoid repeated capturing in subsequent passes</span></span>
<span id="cb117-37"><a href="#cb117-37" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb117-38"><a href="#cb117-38" aria-hidden="true" tabindex="-1"></a>hook_handle.remove()</span>
<span id="cb117-39"><a href="#cb117-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-40"><a href="#cb117-40" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb117-41"><a href="#cb117-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Unpack the parameters from the FCN output (assumes a specific ordering)</span></span>
<span id="cb117-42"><a href="#cb117-42" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb117-43"><a href="#cb117-43" aria-hidden="true" tabindex="-1"></a>gamma_shape1, gamma_scale1, gamma_weight1, <span class="op">\</span></span>
<span id="cb117-44"><a href="#cb117-44" aria-hidden="true" tabindex="-1"></a>gamma_shape2, gamma_scale2, gamma_weight2, <span class="op">\</span></span>
<span id="cb117-45"><a href="#cb117-45" aria-hidden="true" tabindex="-1"></a>vonmises_mu1, vonmises_kappa1, vonmises_weight1, <span class="op">\</span></span>
<span id="cb117-46"><a href="#cb117-46" aria-hidden="true" tabindex="-1"></a>vonmises_mu2, vonmises_kappa2, vonmises_weight2 <span class="op">=</span> intermediate_output[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb117-47"><a href="#cb117-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-48"><a href="#cb117-48" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb117-49"><a href="#cb117-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert parameters from log-space (if applicable) and print them</span></span>
<span id="cb117-50"><a href="#cb117-50" aria-hidden="true" tabindex="-1"></a><span class="co">#    Gamma and von Mises parameters</span></span>
<span id="cb117-51"><a href="#cb117-51" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb117-52"><a href="#cb117-52" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Gamma #1 ---</span></span>
<span id="cb117-53"><a href="#cb117-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Gamma shape 1:"</span>, torch.exp(gamma_shape1))</span>
<span id="cb117-54"><a href="#cb117-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Gamma scale 1:"</span>, torch.exp(gamma_scale1))</span>
<span id="cb117-55"><a href="#cb117-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Gamma weight 1:"</span>,</span>
<span id="cb117-56"><a href="#cb117-56" aria-hidden="true" tabindex="-1"></a>      torch.exp(gamma_weight1) <span class="op">/</span> (torch.exp(gamma_weight1) <span class="op">+</span> torch.exp(gamma_weight2)))</span>
<span id="cb117-57"><a href="#cb117-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-58"><a href="#cb117-58" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Gamma #2 ---</span></span>
<span id="cb117-59"><a href="#cb117-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Gamma shape 2:"</span>, torch.exp(gamma_shape2))</span>
<span id="cb117-60"><a href="#cb117-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Gamma scale 2:"</span>, torch.exp(gamma_scale2) <span class="op">*</span> <span class="dv">500</span>)  <span class="co"># scale factor 500</span></span>
<span id="cb117-61"><a href="#cb117-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Gamma weight 2:"</span>,</span>
<span id="cb117-62"><a href="#cb117-62" aria-hidden="true" tabindex="-1"></a>      torch.exp(gamma_weight2) <span class="op">/</span> (torch.exp(gamma_weight1) <span class="op">+</span> torch.exp(gamma_weight2)))</span>
<span id="cb117-63"><a href="#cb117-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-64"><a href="#cb117-64" aria-hidden="true" tabindex="-1"></a><span class="co"># --- von Mises #1 ---</span></span>
<span id="cb117-65"><a href="#cb117-65" aria-hidden="true" tabindex="-1"></a><span class="co"># % (2*np.pi) ensures the mu (angle) is wrapped within [0, 2Ï€)</span></span>
<span id="cb117-66"><a href="#cb117-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Von Mises mu 1:"</span>, vonmises_mu1 <span class="op">%</span> (<span class="dv">2</span><span class="op">*</span>np.pi))</span>
<span id="cb117-67"><a href="#cb117-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Von Mises kappa 1:"</span>, torch.exp(vonmises_kappa1))</span>
<span id="cb117-68"><a href="#cb117-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Von Mises weight 1:"</span>,</span>
<span id="cb117-69"><a href="#cb117-69" aria-hidden="true" tabindex="-1"></a>      torch.exp(vonmises_weight1) <span class="op">/</span> (torch.exp(vonmises_weight1) <span class="op">+</span> torch.exp(vonmises_weight2)))</span>
<span id="cb117-70"><a href="#cb117-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-71"><a href="#cb117-71" aria-hidden="true" tabindex="-1"></a><span class="co"># --- von Mises #2 ---</span></span>
<span id="cb117-72"><a href="#cb117-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Von Mises mu 2:"</span>, vonmises_mu2 <span class="op">%</span> (<span class="dv">2</span><span class="op">*</span>np.pi))</span>
<span id="cb117-73"><a href="#cb117-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Von Mises kappa 2:"</span>, torch.exp(vonmises_kappa2))</span>
<span id="cb117-74"><a href="#cb117-74" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Von Mises weight 2:"</span>,</span>
<span id="cb117-75"><a href="#cb117-75" aria-hidden="true" tabindex="-1"></a>      torch.exp(vonmises_weight2) <span class="op">/</span> (torch.exp(vonmises_weight1) <span class="op">+</span> torch.exp(vonmises_weight2)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Intermediate output shape: torch.Size([32, 12])
Intermediate output values: tensor([ 0.6753,  0.7900,  0.5984, -0.4723, -0.6171, -3.2411,  0.0966, -1.1834,
         0.4872,  0.0490,  1.1332,  0.0900])
Gamma shape 1: tensor(1.9647)
Gamma scale 1: tensor(2.2034)
Gamma weight 1: tensor(0.9789)
Gamma shape 2: tensor(0.6236)
Gamma scale 2: tensor(269.7635)
Gamma weight 2: tensor(0.0211)
Von Mises mu 1: tensor(0.0966)
Von Mises kappa 1: tensor(0.3062)
Von Mises weight 1: tensor(0.5980)
Von Mises mu 2: tensor(0.0490)
Von Mises kappa 2: tensor(3.1056)
Von Mises weight 2: tensor(0.4020)</code></pre>
</div>
</div>
<section id="plot-the-movement-distributions" class="level2">
<h2 class="anchored" data-anchor-id="plot-the-movement-distributions">Plot the movement distributions</h2>
<p>We can use the movement parameters to plot the step length and turning angle distributions for the sample covariates.</p>
<div id="cell-132" class="cell" data-execution_count="78">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb119"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Define helper functions for calculating Gamma and von Mises log-densities</span></span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gamma_density(x, shape, scale):</span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Computes the log of the Gamma density for each value in x.</span></span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-8"><a href="#cb119-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb119-9"><a href="#cb119-9" aria-hidden="true" tabindex="-1"></a><span class="co">      x (Tensor): Input values for which to compute the density.</span></span>
<span id="cb119-10"><a href="#cb119-10" aria-hidden="true" tabindex="-1"></a><span class="co">      shape (float): Gamma shape parameter</span></span>
<span id="cb119-11"><a href="#cb119-11" aria-hidden="true" tabindex="-1"></a><span class="co">      scale (float): Gamma scale parameter</span></span>
<span id="cb119-12"><a href="#cb119-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-13"><a href="#cb119-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb119-14"><a href="#cb119-14" aria-hidden="true" tabindex="-1"></a><span class="co">      Tensor: The log of the Gamma probability density at each x.</span></span>
<span id="cb119-15"><a href="#cb119-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb119-16"><a href="#cb119-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="dv">1</span><span class="op">*</span>torch.lgamma(shape) <span class="op">-</span> shape<span class="op">*</span>torch.log(scale) <span class="op">\</span></span>
<span id="cb119-17"><a href="#cb119-17" aria-hidden="true" tabindex="-1"></a>           <span class="op">+</span> (shape <span class="op">-</span> <span class="dv">1</span>)<span class="op">*</span>torch.log(x) <span class="op">-</span> x<span class="op">/</span>scale</span>
<span id="cb119-18"><a href="#cb119-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-19"><a href="#cb119-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> vonmises_density(x, kappa, vm_mu):</span>
<span id="cb119-20"><a href="#cb119-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb119-21"><a href="#cb119-21" aria-hidden="true" tabindex="-1"></a><span class="co">    Computes the log of the von Mises density for each value in x.</span></span>
<span id="cb119-22"><a href="#cb119-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-23"><a href="#cb119-23" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb119-24"><a href="#cb119-24" aria-hidden="true" tabindex="-1"></a><span class="co">      x (Tensor): Input angles in radians.</span></span>
<span id="cb119-25"><a href="#cb119-25" aria-hidden="true" tabindex="-1"></a><span class="co">      kappa (float): Concentration parameter (kappa)</span></span>
<span id="cb119-26"><a href="#cb119-26" aria-hidden="true" tabindex="-1"></a><span class="co">      vm_mu (float): Mean direction parameter (mu)</span></span>
<span id="cb119-27"><a href="#cb119-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-28"><a href="#cb119-28" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb119-29"><a href="#cb119-29" aria-hidden="true" tabindex="-1"></a><span class="co">      Tensor: The log of the von Mises probability density at each x.</span></span>
<span id="cb119-30"><a href="#cb119-30" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb119-31"><a href="#cb119-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> kappa<span class="op">*</span>torch.cos(x <span class="op">-</span> vm_mu) <span class="op">-</span> <span class="dv">1</span><span class="op">*</span>(np.log(<span class="dv">2</span><span class="op">*</span>torch.pi) <span class="op">+</span> torch.log(torch.special.i0(kappa)))</span>
<span id="cb119-32"><a href="#cb119-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-33"><a href="#cb119-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-34"><a href="#cb119-34" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb119-35"><a href="#cb119-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Round and display the mixture weights for the Gamma distributions</span></span>
<span id="cb119-36"><a href="#cb119-36" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb119-37"><a href="#cb119-37" aria-hidden="true" tabindex="-1"></a>gamma_weight1_recovered <span class="op">=</span> torch.exp(gamma_weight1)<span class="op">/</span>(torch.exp(gamma_weight1) <span class="op">+</span> torch.exp(gamma_weight2))</span>
<span id="cb119-38"><a href="#cb119-38" aria-hidden="true" tabindex="-1"></a>rounded_gamma_weight1 <span class="op">=</span> <span class="bu">round</span>(gamma_weight1_recovered.item(), <span class="dv">2</span>)</span>
<span id="cb119-39"><a href="#cb119-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-40"><a href="#cb119-40" aria-hidden="true" tabindex="-1"></a>gamma_weight2_recovered <span class="op">=</span> torch.exp(gamma_weight2)<span class="op">/</span>(torch.exp(gamma_weight1) <span class="op">+</span> torch.exp(gamma_weight2))</span>
<span id="cb119-41"><a href="#cb119-41" aria-hidden="true" tabindex="-1"></a>rounded_gamma_weight2 <span class="op">=</span> <span class="bu">round</span>(gamma_weight2_recovered.item(), <span class="dv">2</span>)</span>
<span id="cb119-42"><a href="#cb119-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-43"><a href="#cb119-43" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb119-44"><a href="#cb119-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Round and display the mixture weights for the von Mises distributions</span></span>
<span id="cb119-45"><a href="#cb119-45" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb119-46"><a href="#cb119-46" aria-hidden="true" tabindex="-1"></a>vonmises_weight1_recovered <span class="op">=</span> torch.exp(vonmises_weight1)<span class="op">/</span>(torch.exp(vonmises_weight1) <span class="op">+</span> torch.exp(vonmises_weight2))</span>
<span id="cb119-47"><a href="#cb119-47" aria-hidden="true" tabindex="-1"></a>rounded_vm_weight1 <span class="op">=</span> <span class="bu">round</span>(vonmises_weight1_recovered.item(), <span class="dv">2</span>)</span>
<span id="cb119-48"><a href="#cb119-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-49"><a href="#cb119-49" aria-hidden="true" tabindex="-1"></a>vonmises_weight2_recovered <span class="op">=</span> torch.exp(vonmises_weight2)<span class="op">/</span>(torch.exp(vonmises_weight1) <span class="op">+</span> torch.exp(vonmises_weight2))</span>
<span id="cb119-50"><a href="#cb119-50" aria-hidden="true" tabindex="-1"></a>rounded_vm_weight2 <span class="op">=</span> <span class="bu">round</span>(vonmises_weight2_recovered.item(), <span class="dv">2</span>)</span>
<span id="cb119-51"><a href="#cb119-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-52"><a href="#cb119-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-53"><a href="#cb119-53" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb119-54"><a href="#cb119-54" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Plotting the Gamma mixture distribution</span></span>
<span id="cb119-55"><a href="#cb119-55" aria-hidden="true" tabindex="-1"></a><span class="co">#    a) Generate x values</span></span>
<span id="cb119-56"><a href="#cb119-56" aria-hidden="true" tabindex="-1"></a><span class="co">#    b) Compute individual Gamma log densities</span></span>
<span id="cb119-57"><a href="#cb119-57" aria-hidden="true" tabindex="-1"></a><span class="co">#    c) Exponentiate and combine using recovered weights</span></span>
<span id="cb119-58"><a href="#cb119-58" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb119-59"><a href="#cb119-59" aria-hidden="true" tabindex="-1"></a>x_values <span class="op">=</span> torch.linspace(<span class="dv">1</span>, <span class="dv">101</span>, <span class="dv">1000</span>).to(device)</span>
<span id="cb119-60"><a href="#cb119-60" aria-hidden="true" tabindex="-1"></a>gamma1_density <span class="op">=</span> gamma_density(x_values, torch.exp(gamma_shape1), torch.exp(gamma_scale1))</span>
<span id="cb119-61"><a href="#cb119-61" aria-hidden="true" tabindex="-1"></a>gamma2_density <span class="op">=</span> gamma_density(x_values, torch.exp(gamma_shape2), torch.exp(gamma_scale2)<span class="op">*</span><span class="dv">500</span>)</span>
<span id="cb119-62"><a href="#cb119-62" aria-hidden="true" tabindex="-1"></a>gamma_mixture_density <span class="op">=</span> gamma_weight1_recovered<span class="op">*</span>torch.exp(gamma1_density) <span class="op">\</span></span>
<span id="cb119-63"><a href="#cb119-63" aria-hidden="true" tabindex="-1"></a>                        <span class="op">+</span> gamma_weight2_recovered<span class="op">*</span>torch.exp(gamma2_density)</span>
<span id="cb119-64"><a href="#cb119-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-65"><a href="#cb119-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Move results to CPU and convert to NumPy for plotting</span></span>
<span id="cb119-66"><a href="#cb119-66" aria-hidden="true" tabindex="-1"></a>x_values_np <span class="op">=</span> x_values.cpu().numpy()</span>
<span id="cb119-67"><a href="#cb119-67" aria-hidden="true" tabindex="-1"></a>gamma1_density_np <span class="op">=</span> np.exp(gamma1_density.cpu().numpy())</span>
<span id="cb119-68"><a href="#cb119-68" aria-hidden="true" tabindex="-1"></a>gamma2_density_np <span class="op">=</span> np.exp(gamma2_density.cpu().numpy())</span>
<span id="cb119-69"><a href="#cb119-69" aria-hidden="true" tabindex="-1"></a>gamma_mixture_density_np <span class="op">=</span> gamma_mixture_density.cpu().numpy()</span>
<span id="cb119-70"><a href="#cb119-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-71"><a href="#cb119-71" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb119-72"><a href="#cb119-72" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Plot the Gamma distributions and their mixture</span></span>
<span id="cb119-73"><a href="#cb119-73" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb119-74"><a href="#cb119-74" aria-hidden="true" tabindex="-1"></a>plt.plot(x_values_np, gamma1_density_np, label<span class="op">=</span><span class="ss">f'Gamma 1 Density: weight = </span><span class="sc">{</span>rounded_gamma_weight1<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb119-75"><a href="#cb119-75" aria-hidden="true" tabindex="-1"></a>plt.plot(x_values_np, gamma2_density_np, label<span class="op">=</span><span class="ss">f'Gamma 2 Density: weight = </span><span class="sc">{</span>rounded_gamma_weight2<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb119-76"><a href="#cb119-76" aria-hidden="true" tabindex="-1"></a>plt.plot(x_values_np, gamma_mixture_density_np, label<span class="op">=</span><span class="st">'Gamma Mixture Density'</span>)</span>
<span id="cb119-77"><a href="#cb119-77" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb119-78"><a href="#cb119-78" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb119-79"><a href="#cb119-79" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Gamma Density Function'</span>)</span>
<span id="cb119-80"><a href="#cb119-80" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb119-81"><a href="#cb119-81" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb119-82"><a href="#cb119-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-83"><a href="#cb119-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-84"><a href="#cb119-84" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb119-85"><a href="#cb119-85" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Plotting the von Mises mixture distribution</span></span>
<span id="cb119-86"><a href="#cb119-86" aria-hidden="true" tabindex="-1"></a><span class="co">#    a) Generate x values from -Ï€ to Ï€</span></span>
<span id="cb119-87"><a href="#cb119-87" aria-hidden="true" tabindex="-1"></a><span class="co">#    b) Compute individual von Mises log densities</span></span>
<span id="cb119-88"><a href="#cb119-88" aria-hidden="true" tabindex="-1"></a><span class="co">#    c) Exponentiate and combine using recovered weights</span></span>
<span id="cb119-89"><a href="#cb119-89" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb119-90"><a href="#cb119-90" aria-hidden="true" tabindex="-1"></a>x_values <span class="op">=</span> torch.linspace(<span class="op">-</span>np.pi, np.pi, <span class="dv">1000</span>).to(device)</span>
<span id="cb119-91"><a href="#cb119-91" aria-hidden="true" tabindex="-1"></a>vonmises1_density <span class="op">=</span> vonmises_density(x_values, torch.exp(vonmises_kappa1), vonmises_mu1)</span>
<span id="cb119-92"><a href="#cb119-92" aria-hidden="true" tabindex="-1"></a>vonmises2_density <span class="op">=</span> vonmises_density(x_values, torch.exp(vonmises_kappa2), vonmises_mu2)</span>
<span id="cb119-93"><a href="#cb119-93" aria-hidden="true" tabindex="-1"></a>vonmises_mixture_density <span class="op">=</span> vonmises_weight1_recovered<span class="op">*</span>torch.exp(vonmises1_density) <span class="op">\</span></span>
<span id="cb119-94"><a href="#cb119-94" aria-hidden="true" tabindex="-1"></a>                           <span class="op">+</span> vonmises_weight2_recovered<span class="op">*</span>torch.exp(vonmises2_density)</span>
<span id="cb119-95"><a href="#cb119-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-96"><a href="#cb119-96" aria-hidden="true" tabindex="-1"></a><span class="co"># Move results to CPU and convert to NumPy for plotting</span></span>
<span id="cb119-97"><a href="#cb119-97" aria-hidden="true" tabindex="-1"></a>x_values_np <span class="op">=</span> x_values.cpu().numpy()</span>
<span id="cb119-98"><a href="#cb119-98" aria-hidden="true" tabindex="-1"></a>vonmises1_density_np <span class="op">=</span> np.exp(vonmises1_density.cpu().numpy())</span>
<span id="cb119-99"><a href="#cb119-99" aria-hidden="true" tabindex="-1"></a>vonmises2_density_np <span class="op">=</span> np.exp(vonmises2_density.cpu().numpy())</span>
<span id="cb119-100"><a href="#cb119-100" aria-hidden="true" tabindex="-1"></a>vonmises_mixture_density_np <span class="op">=</span> vonmises_mixture_density.cpu().numpy()</span>
<span id="cb119-101"><a href="#cb119-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-102"><a href="#cb119-102" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb119-103"><a href="#cb119-103" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Plot the von Mises distributions and their mixture</span></span>
<span id="cb119-104"><a href="#cb119-104" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------------------------</span></span>
<span id="cb119-105"><a href="#cb119-105" aria-hidden="true" tabindex="-1"></a>plt.plot(x_values_np, vonmises1_density_np, label<span class="op">=</span><span class="ss">f'Von Mises 1 Density: weight = </span><span class="sc">{</span>rounded_vm_weight1<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb119-106"><a href="#cb119-106" aria-hidden="true" tabindex="-1"></a>plt.plot(x_values_np, vonmises2_density_np, label<span class="op">=</span><span class="ss">f'Von Mises 2 Density: weight = </span><span class="sc">{</span>rounded_vm_weight2<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb119-107"><a href="#cb119-107" aria-hidden="true" tabindex="-1"></a>plt.plot(x_values_np, vonmises_mixture_density_np, label<span class="op">=</span><span class="st">'Von Mises Mixture Density'</span>)</span>
<span id="cb119-108"><a href="#cb119-108" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x (radians)'</span>)</span>
<span id="cb119-109"><a href="#cb119-109" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb119-110"><a href="#cb119-110" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Von Mises Density Function'</span>)</span>
<span id="cb119-111"><a href="#cb119-111" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="fl">1.2</span>)  <span class="co"># Set a limit for the y-axis</span></span>
<span id="cb119-112"><a href="#cb119-112" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb119-113"><a href="#cb119-113" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-79-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-79-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="generate-a-distribution-of-movement-parameters" class="level2">
<h2 class="anchored" data-anchor-id="generate-a-distribution-of-movement-parameters">Generate a distribution of movement parameters</h2>
<p>To see how variable the movement parameters are across samples, we can generate a distribution of movement parameters from a batch of samples.</p>
<p>We take the code from above that we used to create the DataLoader for the test data and increase the batch size (to get more samples to create the distribution from).</p>
<p>As weâ€™re not using the test dataset any more, weâ€™ll just put all of the samples in the same batch, and generate movement parameters for all of them.</p>
<div id="cell-134" class="cell" data-execution_count="79">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'There are </span><span class="sc">{</span><span class="bu">len</span>(dataset_test)<span class="sc">}</span><span class="ss"> samples in the test dataset'</span>)</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>bs <span class="op">=</span> <span class="bu">len</span>(dataset_test) <span class="co"># batch size</span></span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a>dataloader_test <span class="op">=</span> DataLoader(dataset<span class="op">=</span>dataset_test, batch_size<span class="op">=</span>bs, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>There are 1010 samples in the test dataset</code></pre>
</div>
</div>
<p>Take all of the samples from the test dataset and put them in a single batch.</p>
<div id="cell-136" class="cell" data-execution_count="80">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Fetch a batch of data from the training dataloader</span></span>
<span id="cb122-3"><a href="#cb122-3" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb122-4"><a href="#cb122-4" aria-hidden="true" tabindex="-1"></a>x1_batch, x2_batch, x3_batch, labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataloader_test))</span>
<span id="cb122-5"><a href="#cb122-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-6"><a href="#cb122-6" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb122-7"><a href="#cb122-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Register a forward hook to capture the outputs </span></span>
<span id="cb122-8"><a href="#cb122-8" aria-hidden="true" tabindex="-1"></a><span class="co">#    from 'fcn_movement_all' during the forward pass</span></span>
<span id="cb122-9"><a href="#cb122-9" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb122-10"><a href="#cb122-10" aria-hidden="true" tabindex="-1"></a>hook_handle <span class="op">=</span> model.fcn_movement_all.register_forward_hook(hook)</span>
<span id="cb122-11"><a href="#cb122-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-12"><a href="#cb122-12" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb122-13"><a href="#cb122-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform a forward pass in evaluation mode to generate </span></span>
<span id="cb122-14"><a href="#cb122-14" aria-hidden="true" tabindex="-1"></a><span class="co">#    and capture the sub-network's outputs in 'intermediate_output'</span></span>
<span id="cb122-15"><a href="#cb122-15" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb122-16"><a href="#cb122-16" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()  <span class="co"># Disables certain layers like dropout</span></span>
<span id="cb122-17"><a href="#cb122-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-18"><a href="#cb122-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass the batch through the model</span></span>
<span id="cb122-19"><a href="#cb122-19" aria-hidden="true" tabindex="-1"></a>final_output <span class="op">=</span> model((x1_batch, x2_batch, x3_batch))</span>
<span id="cb122-20"><a href="#cb122-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-21"><a href="#cb122-21" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb122-22"><a href="#cb122-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare lists to store the distribution parameters </span></span>
<span id="cb122-23"><a href="#cb122-23" aria-hidden="true" tabindex="-1"></a><span class="co">#    for each sample in the batch</span></span>
<span id="cb122-24"><a href="#cb122-24" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb122-25"><a href="#cb122-25" aria-hidden="true" tabindex="-1"></a>gamma_shape1_list <span class="op">=</span> []</span>
<span id="cb122-26"><a href="#cb122-26" aria-hidden="true" tabindex="-1"></a>gamma_scale1_list <span class="op">=</span> []</span>
<span id="cb122-27"><a href="#cb122-27" aria-hidden="true" tabindex="-1"></a>gamma_weight1_list <span class="op">=</span> []</span>
<span id="cb122-28"><a href="#cb122-28" aria-hidden="true" tabindex="-1"></a>gamma_shape2_list <span class="op">=</span> []</span>
<span id="cb122-29"><a href="#cb122-29" aria-hidden="true" tabindex="-1"></a>gamma_scale2_list <span class="op">=</span> []</span>
<span id="cb122-30"><a href="#cb122-30" aria-hidden="true" tabindex="-1"></a>gamma_weight2_list <span class="op">=</span> []</span>
<span id="cb122-31"><a href="#cb122-31" aria-hidden="true" tabindex="-1"></a>vonmises_mu1_list <span class="op">=</span> []</span>
<span id="cb122-32"><a href="#cb122-32" aria-hidden="true" tabindex="-1"></a>vonmises_kappa1_list <span class="op">=</span> []</span>
<span id="cb122-33"><a href="#cb122-33" aria-hidden="true" tabindex="-1"></a>vonmises_weight1_list <span class="op">=</span> []</span>
<span id="cb122-34"><a href="#cb122-34" aria-hidden="true" tabindex="-1"></a>vonmises_mu2_list <span class="op">=</span> []</span>
<span id="cb122-35"><a href="#cb122-35" aria-hidden="true" tabindex="-1"></a>vonmises_kappa2_list <span class="op">=</span> []</span>
<span id="cb122-36"><a href="#cb122-36" aria-hidden="true" tabindex="-1"></a>vonmises_weight2_list <span class="op">=</span> []</span>
<span id="cb122-37"><a href="#cb122-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-38"><a href="#cb122-38" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb122-39"><a href="#cb122-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract parameters from 'intermediate_output' </span></span>
<span id="cb122-40"><a href="#cb122-40" aria-hidden="true" tabindex="-1"></a><span class="co">#    for every sample in the batch</span></span>
<span id="cb122-41"><a href="#cb122-41" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb122-42"><a href="#cb122-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch_output <span class="kw">in</span> intermediate_output:</span>
<span id="cb122-43"><a href="#cb122-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each 'batch_output' corresponds to one forward pass;</span></span>
<span id="cb122-44"><a href="#cb122-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># it might contain multiple samples if the batch size &gt; 1</span></span>
<span id="cb122-45"><a href="#cb122-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sample_output <span class="kw">in</span> batch_output:</span>
<span id="cb122-46"><a href="#cb122-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Unpack the 12 parameters of the Gamma and von Mises mixtures</span></span>
<span id="cb122-47"><a href="#cb122-47" aria-hidden="true" tabindex="-1"></a>        gamma_shape1, gamma_scale1, gamma_weight1, <span class="op">\</span></span>
<span id="cb122-48"><a href="#cb122-48" aria-hidden="true" tabindex="-1"></a>        gamma_shape2, gamma_scale2, gamma_weight2, <span class="op">\</span></span>
<span id="cb122-49"><a href="#cb122-49" aria-hidden="true" tabindex="-1"></a>        vonmises_mu1, vonmises_kappa1, vonmises_weight1, <span class="op">\</span></span>
<span id="cb122-50"><a href="#cb122-50" aria-hidden="true" tabindex="-1"></a>        vonmises_mu2, vonmises_kappa2, vonmises_weight2 <span class="op">=</span> sample_output</span>
<span id="cb122-51"><a href="#cb122-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-52"><a href="#cb122-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert log-space parameters to real space, then store</span></span>
<span id="cb122-53"><a href="#cb122-53" aria-hidden="true" tabindex="-1"></a>        gamma_shape1_list.append(torch.exp(gamma_shape1).item())</span>
<span id="cb122-54"><a href="#cb122-54" aria-hidden="true" tabindex="-1"></a>        gamma_scale1_list.append(torch.exp(gamma_scale1).item())</span>
<span id="cb122-55"><a href="#cb122-55" aria-hidden="true" tabindex="-1"></a>        gamma_weight1_list.append(</span>
<span id="cb122-56"><a href="#cb122-56" aria-hidden="true" tabindex="-1"></a>            (torch.exp(gamma_weight1)<span class="op">/</span>(torch.exp(gamma_weight1) <span class="op">+</span> torch.exp(gamma_weight2))).item()</span>
<span id="cb122-57"><a href="#cb122-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb122-58"><a href="#cb122-58" aria-hidden="true" tabindex="-1"></a>        gamma_shape2_list.append(torch.exp(gamma_shape2).item())</span>
<span id="cb122-59"><a href="#cb122-59" aria-hidden="true" tabindex="-1"></a>        gamma_scale2_list.append((torch.exp(gamma_scale2)<span class="op">*</span><span class="dv">500</span>).item())  <span class="co"># scale factor 500</span></span>
<span id="cb122-60"><a href="#cb122-60" aria-hidden="true" tabindex="-1"></a>        gamma_weight2_list.append(</span>
<span id="cb122-61"><a href="#cb122-61" aria-hidden="true" tabindex="-1"></a>            (torch.exp(gamma_weight2)<span class="op">/</span>(torch.exp(gamma_weight1) <span class="op">+</span> torch.exp(gamma_weight2))).item()</span>
<span id="cb122-62"><a href="#cb122-62" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb122-63"><a href="#cb122-63" aria-hidden="true" tabindex="-1"></a>        vonmises_mu1_list.append((vonmises_mu1 <span class="op">%</span> (<span class="dv">2</span><span class="op">*</span>np.pi)).item())</span>
<span id="cb122-64"><a href="#cb122-64" aria-hidden="true" tabindex="-1"></a>        vonmises_kappa1_list.append(torch.exp(vonmises_kappa1).item())</span>
<span id="cb122-65"><a href="#cb122-65" aria-hidden="true" tabindex="-1"></a>        vonmises_weight1_list.append(</span>
<span id="cb122-66"><a href="#cb122-66" aria-hidden="true" tabindex="-1"></a>            (torch.exp(vonmises_weight1)<span class="op">/</span>(torch.exp(vonmises_weight1) <span class="op">+</span> torch.exp(vonmises_weight2))).item()</span>
<span id="cb122-67"><a href="#cb122-67" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb122-68"><a href="#cb122-68" aria-hidden="true" tabindex="-1"></a>        vonmises_mu2_list.append((vonmises_mu2 <span class="op">%</span> (<span class="dv">2</span><span class="op">*</span>np.pi)).item())</span>
<span id="cb122-69"><a href="#cb122-69" aria-hidden="true" tabindex="-1"></a>        vonmises_kappa2_list.append(torch.exp(vonmises_kappa2).item())</span>
<span id="cb122-70"><a href="#cb122-70" aria-hidden="true" tabindex="-1"></a>        vonmises_weight2_list.append(</span>
<span id="cb122-71"><a href="#cb122-71" aria-hidden="true" tabindex="-1"></a>            (torch.exp(vonmises_weight2)<span class="op">/</span>(torch.exp(vonmises_weight1) <span class="op">+</span> torch.exp(vonmises_weight2))).item()</span>
<span id="cb122-72"><a href="#cb122-72" aria-hidden="true" tabindex="-1"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="plot-the-distribution-of-movement-parameters" class="level3">
<h3 class="anchored" data-anchor-id="plot-the-distribution-of-movement-parameters">Plot the distribution of movement parameters</h3>
<div id="cell-138" class="cell" data-execution_count="81">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a helper function to plot histograms </span></span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a><span class="co">#    for the collected parameters</span></span>
<span id="cb123-4"><a href="#cb123-4" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb123-5"><a href="#cb123-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_histogram(data, title, xlabel):</span>
<span id="cb123-6"><a href="#cb123-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb123-7"><a href="#cb123-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Plots a histogram of the provided data.</span></span>
<span id="cb123-8"><a href="#cb123-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-9"><a href="#cb123-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb123-10"><a href="#cb123-10" aria-hidden="true" tabindex="-1"></a><span class="co">        data (list): Data points to plot in a histogram.</span></span>
<span id="cb123-11"><a href="#cb123-11" aria-hidden="true" tabindex="-1"></a><span class="co">        title (str): Title of the histogram plot.</span></span>
<span id="cb123-12"><a href="#cb123-12" aria-hidden="true" tabindex="-1"></a><span class="co">        xlabel (str): X-axis label.</span></span>
<span id="cb123-13"><a href="#cb123-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb123-14"><a href="#cb123-14" aria-hidden="true" tabindex="-1"></a>    plt.figure()</span>
<span id="cb123-15"><a href="#cb123-15" aria-hidden="true" tabindex="-1"></a>    plt.hist(data, bins<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.75</span>)</span>
<span id="cb123-16"><a href="#cb123-16" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb123-17"><a href="#cb123-17" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(xlabel)</span>
<span id="cb123-18"><a href="#cb123-18" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb123-19"><a href="#cb123-19" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb123-20"><a href="#cb123-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-21"><a href="#cb123-21" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb123-22"><a href="#cb123-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot histograms for each parameter distribution</span></span>
<span id="cb123-23"><a href="#cb123-23" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb123-24"><a href="#cb123-24" aria-hidden="true" tabindex="-1"></a>plot_histogram(gamma_shape1_list, <span class="st">'Gamma Shape 1 Distribution'</span>, <span class="st">'Shape 1'</span>)</span>
<span id="cb123-25"><a href="#cb123-25" aria-hidden="true" tabindex="-1"></a>plot_histogram(gamma_scale1_list, <span class="st">'Gamma Scale 1 Distribution'</span>, <span class="st">'Scale 1'</span>)</span>
<span id="cb123-26"><a href="#cb123-26" aria-hidden="true" tabindex="-1"></a>plot_histogram(gamma_weight1_list, <span class="st">'Gamma Weight 1 Distribution'</span>, <span class="st">'Weight 1'</span>)</span>
<span id="cb123-27"><a href="#cb123-27" aria-hidden="true" tabindex="-1"></a>plot_histogram(gamma_shape2_list, <span class="st">'Gamma Shape 2 Distribution'</span>, <span class="st">'Shape 2'</span>)</span>
<span id="cb123-28"><a href="#cb123-28" aria-hidden="true" tabindex="-1"></a>plot_histogram(gamma_scale2_list, <span class="st">'Gamma Scale 2 Distribution'</span>, <span class="st">'Scale 2'</span>)</span>
<span id="cb123-29"><a href="#cb123-29" aria-hidden="true" tabindex="-1"></a>plot_histogram(gamma_weight2_list, <span class="st">'Gamma Weight 2 Distribution'</span>, <span class="st">'Weight 2'</span>)</span>
<span id="cb123-30"><a href="#cb123-30" aria-hidden="true" tabindex="-1"></a>plot_histogram(vonmises_mu1_list, <span class="st">'Von Mises Mu 1 Distribution'</span>, <span class="st">'Mu 1'</span>)</span>
<span id="cb123-31"><a href="#cb123-31" aria-hidden="true" tabindex="-1"></a>plot_histogram(vonmises_kappa1_list, <span class="st">'Von Mises Kappa 1 Distribution'</span>, <span class="st">'Kappa 1'</span>)</span>
<span id="cb123-32"><a href="#cb123-32" aria-hidden="true" tabindex="-1"></a>plot_histogram(vonmises_weight1_list, <span class="st">'Von Mises Weight 1 Distribution'</span>, <span class="st">'Weight 1'</span>)</span>
<span id="cb123-33"><a href="#cb123-33" aria-hidden="true" tabindex="-1"></a>plot_histogram(vonmises_mu2_list, <span class="st">'Von Mises Mu 2 Distribution'</span>, <span class="st">'Mu 2'</span>)</span>
<span id="cb123-34"><a href="#cb123-34" aria-hidden="true" tabindex="-1"></a>plot_histogram(vonmises_kappa2_list, <span class="st">'Von Mises Kappa 2 Distribution'</span>, <span class="st">'Kappa 2'</span>)</span>
<span id="cb123-35"><a href="#cb123-35" aria-hidden="true" tabindex="-1"></a>plot_histogram(vonmises_weight2_list, <span class="st">'Von Mises Weight 2 Distribution'</span>, <span class="st">'Weight 2'</span>)</span>
<span id="cb123-36"><a href="#cb123-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-37"><a href="#cb123-37" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb123-38"><a href="#cb123-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove the hook to stop capturing outputs </span></span>
<span id="cb123-39"><a href="#cb123-39" aria-hidden="true" tabindex="-1"></a><span class="co">#    in subsequent forward passes</span></span>
<span id="cb123-40"><a href="#cb123-40" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb123-41"><a href="#cb123-41" aria-hidden="true" tabindex="-1"></a>hook_handle.remove()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-82-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-82-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-82-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-82-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-82-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-82-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-82-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-82-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-82-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-82-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-82-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="deepSSF_train_S2_files/figure-html/cell-82-output-12.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../Python/deepSSF_train.html" class="pagination-link" aria-label="deepSSF Training">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">deepSSF Training</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../Python/deepSSF_simulations.html" class="pagination-link" aria-label="deepSSF Simulations">
        <span class="nav-page-text">deepSSF Simulations</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>